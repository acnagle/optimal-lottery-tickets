Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=5, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='fc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.5, use_relu=False, wd=0.0005) 

Training a Four-Layer Fully Connected Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.328382
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.619794
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.189643
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.033750
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.339701
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.196007
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.209934
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.289669
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.106114

Test set: Average loss: 0.2285, Accuracy: 9399/10000 (94%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.198546
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.195219
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.060343
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.074906
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.088116
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.126151
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.036116
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.323596
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.163084

Test set: Average loss: 0.1871, Accuracy: 9481/10000 (95%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.103447
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.153099
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.221948
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.028946
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.068526
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.088935
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.051008
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.048789
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.082618

Test set: Average loss: 0.0997, Accuracy: 9706/10000 (97%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.027142
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.039902
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.008259
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.084742
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.022148
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.062421
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.067895
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.009130
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.003877

Test set: Average loss: 0.0739, Accuracy: 9796/10000 (98%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.059171
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.001504
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.003274
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.038294
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.002548
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.002837
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.019921
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.002819
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.001881

Test set: Average loss: 0.0540, Accuracy: 9859/10000 (99%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=5, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='fc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.5, use_relu=False, wd=0.0005)


Total time spent pruning/training: 0.59 minutes
Total number of parameters in model: 897000
