Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 6.319695
Train Epoch: 1 [12736/60000 (21%)]	Loss: 7.282774
Train Epoch: 1 [19136/60000 (32%)]	Loss: 11.757396
Train Epoch: 1 [25536/60000 (43%)]	Loss: 11.550618
Train Epoch: 1 [31936/60000 (53%)]	Loss: 12.442515
Train Epoch: 1 [38336/60000 (64%)]	Loss: 10.038383
Train Epoch: 1 [44736/60000 (75%)]	Loss: 14.018641
Train Epoch: 1 [51136/60000 (85%)]	Loss: 13.866883
Train Epoch: 1 [57536/60000 (96%)]	Loss: 13.986225

Test set: Average loss: 14.9508, Accuracy: 1055/10000 (11%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 13.610775
Train Epoch: 2 [12736/60000 (21%)]	Loss: 14.560392
Train Epoch: 2 [19136/60000 (32%)]	Loss: 16.752373
Train Epoch: 2 [25536/60000 (43%)]	Loss: 17.032230
Train Epoch: 2 [31936/60000 (53%)]	Loss: 16.917376
Train Epoch: 2 [38336/60000 (64%)]	Loss: 16.769985
Train Epoch: 2 [44736/60000 (75%)]	Loss: 15.311489
Train Epoch: 2 [51136/60000 (85%)]	Loss: 16.768078
Train Epoch: 2 [57536/60000 (96%)]	Loss: 16.722181

Test set: Average loss: 16.5239, Accuracy: 982/10000 (10%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 17.210333
Train Epoch: 3 [12736/60000 (21%)]	Loss: 18.193634
Train Epoch: 3 [19136/60000 (32%)]	Loss: 17.442352
Train Epoch: 3 [25536/60000 (43%)]	Loss: 16.156483
Train Epoch: 3 [31936/60000 (53%)]	Loss: 17.299906
Train Epoch: 3 [38336/60000 (64%)]	Loss: 16.770706
Train Epoch: 3 [44736/60000 (75%)]	Loss: 17.090904
Train Epoch: 3 [51136/60000 (85%)]	Loss: 17.793713
Train Epoch: 3 [57536/60000 (96%)]	Loss: 16.193354

Test set: Average loss: 16.9767, Accuracy: 963/10000 (10%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 17.088436
Train Epoch: 4 [12736/60000 (21%)]	Loss: 17.253128
Train Epoch: 4 [19136/60000 (32%)]	Loss: 16.021154
Train Epoch: 4 [25536/60000 (43%)]	Loss: 19.548166
Train Epoch: 4 [31936/60000 (53%)]	Loss: 15.312771
Train Epoch: 4 [38336/60000 (64%)]	Loss: 15.210284
Train Epoch: 4 [44736/60000 (75%)]	Loss: 16.710247
Train Epoch: 4 [51136/60000 (85%)]	Loss: 14.873194
Train Epoch: 4 [57536/60000 (96%)]	Loss: 16.503651

Test set: Average loss: 17.4068, Accuracy: 1003/10000 (10%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 16.607714
Train Epoch: 5 [12736/60000 (21%)]	Loss: 18.402468
Train Epoch: 5 [19136/60000 (32%)]	Loss: 17.500174
Train Epoch: 5 [25536/60000 (43%)]	Loss: 17.584150
Train Epoch: 5 [31936/60000 (53%)]	Loss: 15.406679
Train Epoch: 5 [38336/60000 (64%)]	Loss: 16.387190
Train Epoch: 5 [44736/60000 (75%)]	Loss: 20.903791
Train Epoch: 5 [51136/60000 (85%)]	Loss: 19.918488
Train Epoch: 5 [57536/60000 (96%)]	Loss: 16.916809

Test set: Average loss: 17.4591, Accuracy: 916/10000 (9%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 15.784688
Train Epoch: 6 [12736/60000 (21%)]	Loss: 14.690059
Train Epoch: 6 [19136/60000 (32%)]	Loss: 16.413050
Train Epoch: 6 [25536/60000 (43%)]	Loss: 16.741631
Train Epoch: 6 [31936/60000 (53%)]	Loss: 17.085642
Train Epoch: 6 [38336/60000 (64%)]	Loss: 19.821922
Train Epoch: 6 [44736/60000 (75%)]	Loss: 18.020828
Train Epoch: 6 [51136/60000 (85%)]	Loss: 18.407381
Train Epoch: 6 [57536/60000 (96%)]	Loss: 18.561102

Test set: Average loss: 17.6408, Accuracy: 954/10000 (10%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 17.155205
Train Epoch: 7 [12736/60000 (21%)]	Loss: 17.211571
Train Epoch: 7 [19136/60000 (32%)]	Loss: 15.531838
Train Epoch: 7 [25536/60000 (43%)]	Loss: 15.827355
Train Epoch: 7 [31936/60000 (53%)]	Loss: 13.787913
Train Epoch: 7 [38336/60000 (64%)]	Loss: 18.406002
Train Epoch: 7 [44736/60000 (75%)]	Loss: 17.081308
Train Epoch: 7 [51136/60000 (85%)]	Loss: 16.155027
Train Epoch: 7 [57536/60000 (96%)]	Loss: 16.395306

Test set: Average loss: 17.3116, Accuracy: 927/10000 (9%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 18.698147
Train Epoch: 8 [12736/60000 (21%)]	Loss: 18.907629
Train Epoch: 8 [19136/60000 (32%)]	Loss: 15.047804
Train Epoch: 8 [25536/60000 (43%)]	Loss: 17.707792
Train Epoch: 8 [31936/60000 (53%)]	Loss: 17.356142
Train Epoch: 8 [38336/60000 (64%)]	Loss: 16.491285
Train Epoch: 8 [44736/60000 (75%)]	Loss: 17.366856
Train Epoch: 8 [51136/60000 (85%)]	Loss: 16.128893
Train Epoch: 8 [57536/60000 (96%)]	Loss: 16.719034

Test set: Average loss: 17.8281, Accuracy: 938/10000 (9%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 16.325144
Train Epoch: 9 [12736/60000 (21%)]	Loss: 15.518991
Train Epoch: 9 [19136/60000 (32%)]	Loss: 16.627520
Train Epoch: 9 [25536/60000 (43%)]	Loss: 17.758448
Train Epoch: 9 [31936/60000 (53%)]	Loss: 16.005838
Train Epoch: 9 [38336/60000 (64%)]	Loss: 17.971022
Train Epoch: 9 [44736/60000 (75%)]	Loss: 18.460684
Train Epoch: 9 [51136/60000 (85%)]	Loss: 18.495274
Train Epoch: 9 [57536/60000 (96%)]	Loss: 16.469559

Test set: Average loss: 17.8146, Accuracy: 948/10000 (9%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 0.99 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1971505
