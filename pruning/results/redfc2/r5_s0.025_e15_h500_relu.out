Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.551369
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.987207
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.831800
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.670769
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.599564
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.678785
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.786752
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.504479
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.486332

Test set: Average loss: 0.7313, Accuracy: 7783/10000 (78%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.385820
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.040872
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.417663
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.990932
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.929419
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.975844
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.221665
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.563823
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.286138

Test set: Average loss: 1.8736, Accuracy: 4957/10000 (50%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.999467
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.661822
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.291057
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.986924
Train Epoch: 3 [31936/60000 (53%)]	Loss: 3.085907
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.862238
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.900683
Train Epoch: 3 [51136/60000 (85%)]	Loss: 3.598107
Train Epoch: 3 [57536/60000 (96%)]	Loss: 3.125768

Test set: Average loss: 3.3403, Accuracy: 3124/10000 (31%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 4.494068
Train Epoch: 4 [12736/60000 (21%)]	Loss: 3.240808
Train Epoch: 4 [19136/60000 (32%)]	Loss: 4.211637
Train Epoch: 4 [25536/60000 (43%)]	Loss: 4.473254
Train Epoch: 4 [31936/60000 (53%)]	Loss: 3.754317
Train Epoch: 4 [38336/60000 (64%)]	Loss: 4.302860
Train Epoch: 4 [44736/60000 (75%)]	Loss: 4.222750
Train Epoch: 4 [51136/60000 (85%)]	Loss: 4.069790
Train Epoch: 4 [57536/60000 (96%)]	Loss: 5.030897

Test set: Average loss: 4.8001, Accuracy: 1880/10000 (19%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 5.257179
Train Epoch: 5 [12736/60000 (21%)]	Loss: 4.766456
Train Epoch: 5 [19136/60000 (32%)]	Loss: 5.827570
Train Epoch: 5 [25536/60000 (43%)]	Loss: 5.947697
Train Epoch: 5 [31936/60000 (53%)]	Loss: 5.347125
Train Epoch: 5 [38336/60000 (64%)]	Loss: 5.623834
Train Epoch: 5 [44736/60000 (75%)]	Loss: 5.649929
Train Epoch: 5 [51136/60000 (85%)]	Loss: 5.743345
Train Epoch: 5 [57536/60000 (96%)]	Loss: 5.471478

Test set: Average loss: 5.6161, Accuracy: 1413/10000 (14%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 5.202709
Train Epoch: 6 [12736/60000 (21%)]	Loss: 5.922386
Train Epoch: 6 [19136/60000 (32%)]	Loss: 5.497190
Train Epoch: 6 [25536/60000 (43%)]	Loss: 6.305374
Train Epoch: 6 [31936/60000 (53%)]	Loss: 5.445346
Train Epoch: 6 [38336/60000 (64%)]	Loss: 6.332977
Train Epoch: 6 [44736/60000 (75%)]	Loss: 6.438006
Train Epoch: 6 [51136/60000 (85%)]	Loss: 6.444272
Train Epoch: 6 [57536/60000 (96%)]	Loss: 5.957911

Test set: Average loss: 6.2741, Accuracy: 1086/10000 (11%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 5.940541
Train Epoch: 7 [12736/60000 (21%)]	Loss: 6.078161
Train Epoch: 7 [19136/60000 (32%)]	Loss: 6.154092
Train Epoch: 7 [25536/60000 (43%)]	Loss: 5.721634
Train Epoch: 7 [31936/60000 (53%)]	Loss: 5.929419
Train Epoch: 7 [38336/60000 (64%)]	Loss: 6.349126
Train Epoch: 7 [44736/60000 (75%)]	Loss: 6.334907
Train Epoch: 7 [51136/60000 (85%)]	Loss: 7.115890
Train Epoch: 7 [57536/60000 (96%)]	Loss: 7.132361

Test set: Average loss: 6.4558, Accuracy: 1060/10000 (11%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 6.893667
Train Epoch: 8 [12736/60000 (21%)]	Loss: 7.200001
Train Epoch: 8 [19136/60000 (32%)]	Loss: 5.387436
Train Epoch: 8 [25536/60000 (43%)]	Loss: 6.535975
Train Epoch: 8 [31936/60000 (53%)]	Loss: 5.829750
Train Epoch: 8 [38336/60000 (64%)]	Loss: 6.369075
Train Epoch: 8 [44736/60000 (75%)]	Loss: 7.113522
Train Epoch: 8 [51136/60000 (85%)]	Loss: 6.448440
Train Epoch: 8 [57536/60000 (96%)]	Loss: 7.243201

Test set: Average loss: 6.6341, Accuracy: 1065/10000 (11%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 6.018006
Train Epoch: 9 [12736/60000 (21%)]	Loss: 6.485150
Train Epoch: 9 [19136/60000 (32%)]	Loss: 6.556951
Train Epoch: 9 [25536/60000 (43%)]	Loss: 7.257690
Train Epoch: 9 [31936/60000 (53%)]	Loss: 6.074377
Train Epoch: 9 [38336/60000 (64%)]	Loss: 7.220844
Train Epoch: 9 [44736/60000 (75%)]	Loss: 7.623805
Train Epoch: 9 [51136/60000 (85%)]	Loss: 7.225723
Train Epoch: 9 [57536/60000 (96%)]	Loss: 6.968336

Test set: Average loss: 6.8522, Accuracy: 1066/10000 (11%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 6.993502
Train Epoch: 10 [12736/60000 (21%)]	Loss: 6.556063
Train Epoch: 10 [19136/60000 (32%)]	Loss: 6.674399
Train Epoch: 10 [25536/60000 (43%)]	Loss: 7.388960
Train Epoch: 10 [31936/60000 (53%)]	Loss: 6.907541
Train Epoch: 10 [38336/60000 (64%)]	Loss: 6.642307
Train Epoch: 10 [44736/60000 (75%)]	Loss: 6.670266
Train Epoch: 10 [51136/60000 (85%)]	Loss: 7.167453
Train Epoch: 10 [57536/60000 (96%)]	Loss: 6.318546

Test set: Average loss: 6.9245, Accuracy: 1075/10000 (11%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 6.467494
Train Epoch: 11 [12736/60000 (21%)]	Loss: 5.901332
Train Epoch: 11 [19136/60000 (32%)]	Loss: 6.709872
Train Epoch: 11 [25536/60000 (43%)]	Loss: 7.386029
Train Epoch: 11 [31936/60000 (53%)]	Loss: 6.625070
Train Epoch: 11 [38336/60000 (64%)]	Loss: 6.809181
Train Epoch: 11 [44736/60000 (75%)]	Loss: 8.616716
Train Epoch: 11 [51136/60000 (85%)]	Loss: 6.258862
Train Epoch: 11 [57536/60000 (96%)]	Loss: 7.065689

Test set: Average loss: 7.1414, Accuracy: 1056/10000 (11%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 6.506044
Train Epoch: 12 [12736/60000 (21%)]	Loss: 7.768455
Train Epoch: 12 [19136/60000 (32%)]	Loss: 7.629451
Train Epoch: 12 [25536/60000 (43%)]	Loss: 7.016508
Train Epoch: 12 [31936/60000 (53%)]	Loss: 8.311812
Train Epoch: 12 [38336/60000 (64%)]	Loss: 8.222385
Train Epoch: 12 [44736/60000 (75%)]	Loss: 8.407451
Train Epoch: 12 [51136/60000 (85%)]	Loss: 7.504750
Train Epoch: 12 [57536/60000 (96%)]	Loss: 7.234002

Test set: Average loss: 7.5050, Accuracy: 882/10000 (9%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 6.389965
Train Epoch: 13 [12736/60000 (21%)]	Loss: 8.090731
Train Epoch: 13 [19136/60000 (32%)]	Loss: 7.462853
Train Epoch: 13 [25536/60000 (43%)]	Loss: 7.978033
Train Epoch: 13 [31936/60000 (53%)]	Loss: 6.918202
Train Epoch: 13 [38336/60000 (64%)]	Loss: 7.790039
Train Epoch: 13 [44736/60000 (75%)]	Loss: 6.908340
Train Epoch: 13 [51136/60000 (85%)]	Loss: 7.421018
Train Epoch: 13 [57536/60000 (96%)]	Loss: 7.152701

Test set: Average loss: 7.4262, Accuracy: 933/10000 (9%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 7.780590
Train Epoch: 14 [12736/60000 (21%)]	Loss: 7.345346
Train Epoch: 14 [19136/60000 (32%)]	Loss: 7.629421
Train Epoch: 14 [25536/60000 (43%)]	Loss: 7.740607
Train Epoch: 14 [31936/60000 (53%)]	Loss: 7.466313
Train Epoch: 14 [38336/60000 (64%)]	Loss: 8.609636
Train Epoch: 14 [44736/60000 (75%)]	Loss: 8.259099
Train Epoch: 14 [51136/60000 (85%)]	Loss: 7.701560
Train Epoch: 14 [57536/60000 (96%)]	Loss: 7.494790

Test set: Average loss: 7.5637, Accuracy: 894/10000 (9%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 8.018709
Train Epoch: 15 [12736/60000 (21%)]	Loss: 6.492795
Train Epoch: 15 [19136/60000 (32%)]	Loss: 6.895154
Train Epoch: 15 [25536/60000 (43%)]	Loss: 7.364134
Train Epoch: 15 [31936/60000 (53%)]	Loss: 8.739020
Train Epoch: 15 [38336/60000 (64%)]	Loss: 7.585768
Train Epoch: 15 [44736/60000 (75%)]	Loss: 6.530951
Train Epoch: 15 [51136/60000 (85%)]	Loss: 7.907834
Train Epoch: 15 [57536/60000 (96%)]	Loss: 7.919422

Test set: Average loss: 7.7847, Accuracy: 853/10000 (9%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.65 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1941634
