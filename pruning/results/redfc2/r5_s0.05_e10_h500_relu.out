Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.517663
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.645766
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.763680
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.387334
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.408293
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.336996
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.442142
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.232538
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.307562

Test set: Average loss: 0.3110, Accuracy: 8997/10000 (90%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.204073
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.411402
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.181211
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.466699
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.312664
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.238574
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.348498
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.531309
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.429273

Test set: Average loss: 0.3066, Accuracy: 9094/10000 (91%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.376107
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.146807
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.370351
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.276115
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.341062
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.393734
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.383810
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.415384
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.546049

Test set: Average loss: 0.3786, Accuracy: 8794/10000 (88%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.248609
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.706852
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.283919
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.450697
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.714651
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.472296
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.384480
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.289504
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.567165

Test set: Average loss: 0.6495, Accuracy: 8013/10000 (80%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.682518
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.489996
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.492089
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.539945
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.750968
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.652565
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.452118
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.768173
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.724890

Test set: Average loss: 0.9478, Accuracy: 7087/10000 (71%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.414490
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.621639
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.795506
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.690037
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.765003
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.556383
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.640529
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.551471
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.084707

Test set: Average loss: 0.8569, Accuracy: 7379/10000 (74%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.784938
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.075489
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.763239
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.412601
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.641763
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.067105
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.228697
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.944883
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.944885

Test set: Average loss: 1.7263, Accuracy: 5945/10000 (59%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.292960
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.858657
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.482813
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.890821
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.531524
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.737679
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.126400
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.438438
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.502422

Test set: Average loss: 1.2298, Accuracy: 6703/10000 (67%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.820751
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.075013
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.272754
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.121158
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.071355
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.029505
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.163775
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.551738
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.344737

Test set: Average loss: 1.3644, Accuracy: 6462/10000 (65%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.933339
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.045637
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.189235
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.573273
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.968411
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.330869
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.128101
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.295590
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.757803

Test set: Average loss: 1.2124, Accuracy: 6638/10000 (66%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.11 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1891849
