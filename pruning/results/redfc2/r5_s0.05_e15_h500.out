Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=False, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.241253
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.863802
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.203591
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.565550
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.517444
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.452271
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.543688
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.464651
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.347895

Test set: Average loss: 0.4721, Accuracy: 8679/10000 (87%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.208271
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.000170
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.463517
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.010070
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.635307
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.402595
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.554032
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.032433
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.974299

Test set: Average loss: 0.7908, Accuracy: 8200/10000 (82%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.071000
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.707783
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.469140
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.949406
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.475559
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.191601
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.544350
Train Epoch: 3 [51136/60000 (85%)]	Loss: 3.322844
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.809651

Test set: Average loss: 3.3705, Accuracy: 5800/10000 (58%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.941182
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.954998
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.488663
Train Epoch: 4 [25536/60000 (43%)]	Loss: 4.398871
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.776430
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.796955
Train Epoch: 4 [44736/60000 (75%)]	Loss: 3.816918
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.507029
Train Epoch: 4 [57536/60000 (96%)]	Loss: 4.222891

Test set: Average loss: 3.6428, Accuracy: 5197/10000 (52%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 5.564394
Train Epoch: 5 [12736/60000 (21%)]	Loss: 4.149686
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.785864
Train Epoch: 5 [25536/60000 (43%)]	Loss: 3.456054
Train Epoch: 5 [31936/60000 (53%)]	Loss: 3.763621
Train Epoch: 5 [38336/60000 (64%)]	Loss: 3.996161
Train Epoch: 5 [44736/60000 (75%)]	Loss: 4.634315
Train Epoch: 5 [51136/60000 (85%)]	Loss: 5.422228
Train Epoch: 5 [57536/60000 (96%)]	Loss: 5.137480

Test set: Average loss: 5.8610, Accuracy: 3851/10000 (39%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 5.202604
Train Epoch: 6 [12736/60000 (21%)]	Loss: 5.295065
Train Epoch: 6 [19136/60000 (32%)]	Loss: 4.079014
Train Epoch: 6 [25536/60000 (43%)]	Loss: 3.946122
Train Epoch: 6 [31936/60000 (53%)]	Loss: 5.286009
Train Epoch: 6 [38336/60000 (64%)]	Loss: 6.452506
Train Epoch: 6 [44736/60000 (75%)]	Loss: 6.054419
Train Epoch: 6 [51136/60000 (85%)]	Loss: 4.492054
Train Epoch: 6 [57536/60000 (96%)]	Loss: 6.574259

Test set: Average loss: 5.8451, Accuracy: 4063/10000 (41%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 6.557770
Train Epoch: 7 [12736/60000 (21%)]	Loss: 5.480793
Train Epoch: 7 [19136/60000 (32%)]	Loss: 5.544541
Train Epoch: 7 [25536/60000 (43%)]	Loss: 5.888265
Train Epoch: 7 [31936/60000 (53%)]	Loss: 6.226995
Train Epoch: 7 [38336/60000 (64%)]	Loss: 7.072828
Train Epoch: 7 [44736/60000 (75%)]	Loss: 6.039291
Train Epoch: 7 [51136/60000 (85%)]	Loss: 6.515223
Train Epoch: 7 [57536/60000 (96%)]	Loss: 6.357225

Test set: Average loss: 6.7584, Accuracy: 3185/10000 (32%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 6.728938
Train Epoch: 8 [12736/60000 (21%)]	Loss: 6.737028
Train Epoch: 8 [19136/60000 (32%)]	Loss: 6.732836
Train Epoch: 8 [25536/60000 (43%)]	Loss: 5.590326
Train Epoch: 8 [31936/60000 (53%)]	Loss: 8.956385
Train Epoch: 8 [38336/60000 (64%)]	Loss: 5.663625
Train Epoch: 8 [44736/60000 (75%)]	Loss: 7.982680
Train Epoch: 8 [51136/60000 (85%)]	Loss: 7.350182
Train Epoch: 8 [57536/60000 (96%)]	Loss: 7.473775

Test set: Average loss: 6.2491, Accuracy: 3019/10000 (30%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 5.349160
Train Epoch: 9 [12736/60000 (21%)]	Loss: 5.587822
Train Epoch: 9 [19136/60000 (32%)]	Loss: 5.971251
Train Epoch: 9 [25536/60000 (43%)]	Loss: 5.896922
Train Epoch: 9 [31936/60000 (53%)]	Loss: 6.423129
Train Epoch: 9 [38336/60000 (64%)]	Loss: 7.291149
Train Epoch: 9 [44736/60000 (75%)]	Loss: 7.591383
Train Epoch: 9 [51136/60000 (85%)]	Loss: 7.882729
Train Epoch: 9 [57536/60000 (96%)]	Loss: 6.740273

Test set: Average loss: 7.5218, Accuracy: 3143/10000 (31%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 7.584571
Train Epoch: 10 [12736/60000 (21%)]	Loss: 6.611238
Train Epoch: 10 [19136/60000 (32%)]	Loss: 7.732160
Train Epoch: 10 [25536/60000 (43%)]	Loss: 7.898793
Train Epoch: 10 [31936/60000 (53%)]	Loss: 8.137131
Train Epoch: 10 [38336/60000 (64%)]	Loss: 8.567568
Train Epoch: 10 [44736/60000 (75%)]	Loss: 6.460538
Train Epoch: 10 [51136/60000 (85%)]	Loss: 8.374610
Train Epoch: 10 [57536/60000 (96%)]	Loss: 8.577874

Test set: Average loss: 6.6723, Accuracy: 3086/10000 (31%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 6.839162
Train Epoch: 11 [12736/60000 (21%)]	Loss: 6.366449
Train Epoch: 11 [19136/60000 (32%)]	Loss: 6.363215
Train Epoch: 11 [25536/60000 (43%)]	Loss: 6.831911
Train Epoch: 11 [31936/60000 (53%)]	Loss: 6.868745
Train Epoch: 11 [38336/60000 (64%)]	Loss: 6.970669
Train Epoch: 11 [44736/60000 (75%)]	Loss: 7.079581
Train Epoch: 11 [51136/60000 (85%)]	Loss: 6.375555
Train Epoch: 11 [57536/60000 (96%)]	Loss: 5.941002

Test set: Average loss: 6.7349, Accuracy: 3317/10000 (33%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 8.086025
Train Epoch: 12 [12736/60000 (21%)]	Loss: 8.605015
Train Epoch: 12 [19136/60000 (32%)]	Loss: 6.680978
Train Epoch: 12 [25536/60000 (43%)]	Loss: 6.435647
Train Epoch: 12 [31936/60000 (53%)]	Loss: 6.391268
Train Epoch: 12 [38336/60000 (64%)]	Loss: 7.882425
Train Epoch: 12 [44736/60000 (75%)]	Loss: 7.525322
Train Epoch: 12 [51136/60000 (85%)]	Loss: 7.828427
Train Epoch: 12 [57536/60000 (96%)]	Loss: 6.380836

Test set: Average loss: 8.6070, Accuracy: 2514/10000 (25%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 7.926137
Train Epoch: 13 [12736/60000 (21%)]	Loss: 8.632690
Train Epoch: 13 [19136/60000 (32%)]	Loss: 6.874290
Train Epoch: 13 [25536/60000 (43%)]	Loss: 6.878411
Train Epoch: 13 [31936/60000 (53%)]	Loss: 7.466611
Train Epoch: 13 [38336/60000 (64%)]	Loss: 7.425361
Train Epoch: 13 [44736/60000 (75%)]	Loss: 4.440674
Train Epoch: 13 [51136/60000 (85%)]	Loss: 8.328104
Train Epoch: 13 [57536/60000 (96%)]	Loss: 8.047633

Test set: Average loss: 8.0967, Accuracy: 2547/10000 (25%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 5.026831
Train Epoch: 14 [12736/60000 (21%)]	Loss: 6.371750
Train Epoch: 14 [19136/60000 (32%)]	Loss: 8.804009
Train Epoch: 14 [25536/60000 (43%)]	Loss: 5.459943
Train Epoch: 14 [31936/60000 (53%)]	Loss: 6.075182
Train Epoch: 14 [38336/60000 (64%)]	Loss: 9.136762
Train Epoch: 14 [44736/60000 (75%)]	Loss: 6.884577
Train Epoch: 14 [51136/60000 (85%)]	Loss: 7.702065
Train Epoch: 14 [57536/60000 (96%)]	Loss: 6.558061

Test set: Average loss: 8.1066, Accuracy: 2530/10000 (25%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 6.186416
Train Epoch: 15 [12736/60000 (21%)]	Loss: 6.228956
Train Epoch: 15 [19136/60000 (32%)]	Loss: 7.182458
Train Epoch: 15 [25536/60000 (43%)]	Loss: 5.565490
Train Epoch: 15 [31936/60000 (53%)]	Loss: 6.939454
Train Epoch: 15 [38336/60000 (64%)]	Loss: 8.504680
Train Epoch: 15 [44736/60000 (75%)]	Loss: 5.581717
Train Epoch: 15 [51136/60000 (85%)]	Loss: 7.366594
Train Epoch: 15 [57536/60000 (96%)]	Loss: 7.439806

Test set: Average loss: 8.0591, Accuracy: 2803/10000 (28%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.67 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1891849
