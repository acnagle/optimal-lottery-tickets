Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.1, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.466836
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.572155
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.525000
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.386608
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.340176
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.254994
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.222446
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.171231
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.297344

Test set: Average loss: 0.2921, Accuracy: 9085/10000 (91%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.161389
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.280986
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.120772
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.193935
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.184607
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.294415
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.307925
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.261129
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.295116

Test set: Average loss: 0.2366, Accuracy: 9266/10000 (93%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.131555
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.107050
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.172492
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.252718
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.265677
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.231237
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.257992
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.258837
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.594504

Test set: Average loss: 0.2264, Accuracy: 9290/10000 (93%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.192127
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.450036
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.168194
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.247733
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.399768
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.479989
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.247213
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.239981
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.311620

Test set: Average loss: 0.2503, Accuracy: 9209/10000 (92%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.319880
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.241622
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.232790
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.185739
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.249898
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.351081
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.138881
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.269867
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.200781

Test set: Average loss: 0.2695, Accuracy: 9169/10000 (92%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.183942
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.176211
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.203859
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.173131
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.180226
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.202653
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.151966
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.167556
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.326492

Test set: Average loss: 0.2428, Accuracy: 9261/10000 (93%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.122864
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.222299
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.148657
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.151885
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.169329
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.159064
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.295742
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.197582
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.247599

Test set: Average loss: 0.2350, Accuracy: 9252/10000 (93%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.377481
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.150504
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.134474
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.275090
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.230945
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.138317
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.470484
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.172366
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.191405

Test set: Average loss: 0.2527, Accuracy: 9232/10000 (92%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.147761
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.064363
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.106749
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.200672
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.144994
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.180892
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.108061
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.342712
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.048973

Test set: Average loss: 0.1528, Accuracy: 9522/10000 (95%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.099604
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.168162
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.126193
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.056535
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.107737
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.158277
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.088951
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.104623
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.061918

Test set: Average loss: 0.1039, Accuracy: 9671/10000 (97%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.1, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.08 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1792278
