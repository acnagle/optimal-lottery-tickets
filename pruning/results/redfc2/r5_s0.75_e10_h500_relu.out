Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.75, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.743024
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.527251
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.715980
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.490911
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.442560
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.425960
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.415416
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.299683
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.384081

Test set: Average loss: 0.3648, Accuracy: 8973/10000 (90%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.352964
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.451833
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.340816
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.449622
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.283034
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.303853
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.403125
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.447030
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.377598

Test set: Average loss: 0.3353, Accuracy: 9032/10000 (90%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.242138
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.272484
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.365706
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.395886
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.387277
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.400192
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.402951
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.330024
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.561002

Test set: Average loss: 0.3158, Accuracy: 9111/10000 (91%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.246576
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.543365
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.322644
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.367124
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.476283
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.484987
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.343808
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.229635
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.435130

Test set: Average loss: 0.3430, Accuracy: 9047/10000 (90%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.406479
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.311085
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.261688
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.424438
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.261892
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.365155
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.251799
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.425016
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.253565

Test set: Average loss: 0.2995, Accuracy: 9164/10000 (92%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.262702
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.319092
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.290125
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.255688
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.230820
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.241942
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.265023
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.278475
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.413560

Test set: Average loss: 0.3060, Accuracy: 9140/10000 (91%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.199949
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.212570
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.286134
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.228551
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.193036
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.264060
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.340916
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.282641
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.224611

Test set: Average loss: 0.2943, Accuracy: 9180/10000 (92%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.487594
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.314120
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.327444
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.344282
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.348870
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.181156
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.483496
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.290017
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.239663

Test set: Average loss: 0.2687, Accuracy: 9252/10000 (93%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.247425
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.339763
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.264306
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.315125
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.267790
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.343361
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.274069
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.504899
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.247649

Test set: Average loss: 0.2624, Accuracy: 9291/10000 (93%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.351262
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.326590
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.239100
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.251807
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.316687
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.341806
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.276778
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.194993
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.287611

Test set: Average loss: 0.2562, Accuracy: 9330/10000 (93%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.75, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.09 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 497855
