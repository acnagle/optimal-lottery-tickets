Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.688149
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.259782
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.977365
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.930581
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.803980
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.693814
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.697378
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.529352
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.710539

Test set: Average loss: 0.5731, Accuracy: 8420/10000 (84%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.528779
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.580921
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.618150
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.681429
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.573031
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.489524
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.516302
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.597575
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.546924

Test set: Average loss: 0.5110, Accuracy: 8563/10000 (86%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.453347
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.476093
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.573901
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.614228
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.598239
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.488119
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.638791
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.563834
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.669018

Test set: Average loss: 0.4683, Accuracy: 8675/10000 (87%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.464731
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.631628
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.427579
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.578100
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.601495
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.607903
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.589524
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.394035
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.626925

Test set: Average loss: 0.5001, Accuracy: 8618/10000 (86%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.565172
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.494351
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.429701
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.521701
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.410109
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.608907
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.439158
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.572655
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.521841

Test set: Average loss: 0.4726, Accuracy: 8660/10000 (87%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.523259
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.533528
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.497824
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.366511
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.394022
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.444341
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.411214
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.504754
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.515868

Test set: Average loss: 0.5132, Accuracy: 8509/10000 (85%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.437302
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.472635
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.473579
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.500103
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.436618
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.658673
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.611567
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.635820
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.477604

Test set: Average loss: 0.5158, Accuracy: 8518/10000 (85%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.770735
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.496351
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.680098
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.608244
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.657732
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.454481
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.863364
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.662965
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.558247

Test set: Average loss: 0.5081, Accuracy: 8641/10000 (86%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.459363
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.530505
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.547028
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.518319
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.441413
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.535264
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.434154
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.647290
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.561277

Test set: Average loss: 0.5059, Accuracy: 8556/10000 (86%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.538817
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.535978
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.410759
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.552372
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.469585
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.510615
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.588810
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.499206
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.546454

Test set: Average loss: 0.4525, Accuracy: 8749/10000 (87%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.536691
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.453914
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.411411
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.502135
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.627402
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.395815
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.415624
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.357630
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.473350

Test set: Average loss: 0.4404, Accuracy: 8788/10000 (88%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.398843
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.469452
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.472701
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.326334
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.353840
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.606391
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.441311
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.396038
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.334431

Test set: Average loss: 0.4037, Accuracy: 8902/10000 (89%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.341582
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.361496
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.381719
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.405784
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.547662
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.543810
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.264692
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.510882
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.397624

Test set: Average loss: 0.3985, Accuracy: 8935/10000 (89%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.42 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 99571
