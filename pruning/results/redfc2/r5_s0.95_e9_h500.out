Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.688149
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.259782
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.977365
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.930581
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.803980
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.693814
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.697378
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.529352
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.710539

Test set: Average loss: 0.5731, Accuracy: 8420/10000 (84%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.552858
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.569466
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.575399
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.660920
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.578739
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.530692
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.557309
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.666141
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.584768

Test set: Average loss: 0.4812, Accuracy: 8671/10000 (87%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.388292
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.568990
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.554234
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.507286
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.585971
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.483537
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.588646
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.488916
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.662942

Test set: Average loss: 0.4728, Accuracy: 8685/10000 (87%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.417427
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.632602
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.432619
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.578742
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.623553
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.560016
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.558052
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.370075
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.623406

Test set: Average loss: 0.4667, Accuracy: 8764/10000 (88%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.548008
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.456254
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.348996
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.511849
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.425457
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.568694
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.435830
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.558009
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.456139

Test set: Average loss: 0.4541, Accuracy: 8718/10000 (87%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.449749
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.433693
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.406934
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.364598
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.352789
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.356688
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.347565
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.413264
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.498733

Test set: Average loss: 0.4425, Accuracy: 8776/10000 (88%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.368265
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.454959
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.388347
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.374443
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.332249
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.489326
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.493411
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.492040
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.427294

Test set: Average loss: 0.4373, Accuracy: 8821/10000 (88%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.614973
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.401803
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.532587
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.479007
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.492863
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.333298
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.738521
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.482533
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.370916

Test set: Average loss: 0.4170, Accuracy: 8874/10000 (89%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.363687
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.408547
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.418340
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.482417
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.382109
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.456425
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.356637
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.524969
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.405523

Test set: Average loss: 0.3954, Accuracy: 8929/10000 (89%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.00 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 99571
