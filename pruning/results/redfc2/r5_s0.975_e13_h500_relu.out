Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.975, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.275497
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.264668
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.248509
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.278043
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.261737
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.237067
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.227030
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.250111
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.256490

Test set: Average loss: 2.2380, Accuracy: 2144/10000 (21%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.232095
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.229059
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.244990
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.256316
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.242891
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.255633
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.226106
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.238969
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.247832

Test set: Average loss: 2.2358, Accuracy: 2144/10000 (21%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.195323
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.257113
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.242114
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.201108
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.239007
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.237203
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.252520
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.239550
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.224711

Test set: Average loss: 2.2296, Accuracy: 2851/10000 (29%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.248571
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.229378
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.235519
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.247485
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.193515
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.223575
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.214860
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.240987
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.272112

Test set: Average loss: 2.2198, Accuracy: 2837/10000 (28%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.218136
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.230932
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.182746
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.236379
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.213853
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.217206
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.244623
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.215468
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.242525

Test set: Average loss: 2.2092, Accuracy: 2863/10000 (29%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.212422
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.208914
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.253141
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.190715
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.206654
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.224341
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.202942
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.203549
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.187294

Test set: Average loss: 2.2070, Accuracy: 2744/10000 (27%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.203734
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.215281
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.204957
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.219957
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.196916
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.212025
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.223533
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.222654
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.218791

Test set: Average loss: 2.2092, Accuracy: 2639/10000 (26%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.227262
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.207465
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.220341
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.220739
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.182566
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.216682
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.221756
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.220449
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.226660

Test set: Average loss: 2.2057, Accuracy: 2832/10000 (28%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.215238
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.191565
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.213571
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.208288
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.164767
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.215816
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.223951
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.234343
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.173403

Test set: Average loss: 2.2016, Accuracy: 2652/10000 (27%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.191749
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.185643
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.223737
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.233907
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.214229
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.230269
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.217376
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.197535
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.216057

Test set: Average loss: 2.2013, Accuracy: 2838/10000 (28%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.208806
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.189296
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.187411
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.212087
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.214694
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.168323
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.209403
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.185838
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.196402

Test set: Average loss: 2.2006, Accuracy: 2785/10000 (28%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.210758
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.216300
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.188905
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.181111
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.235367
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.213649
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.188080
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.231417
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.178931

Test set: Average loss: 2.2057, Accuracy: 2811/10000 (28%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.195938
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.205485
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.172196
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.224808
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.221887
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.216078
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.170443
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.190725
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.197163

Test set: Average loss: 2.2036, Accuracy: 2848/10000 (28%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.975, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.43 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 49785
