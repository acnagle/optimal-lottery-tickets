Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.9, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.659353
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.427297
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.388729
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.330133
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.270936
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.301834
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.206731
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.221573
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.199756

Test set: Average loss: 1.1385, Accuracy: 7013/10000 (70%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.078709
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.094149
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.176137
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.279543
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.119634
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.145226
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.040690
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.167220
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.130546

Test set: Average loss: 1.0366, Accuracy: 7637/10000 (76%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.037181
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.985783
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.118665
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.001067
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.971301
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.013092
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.083834
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.061617
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.118423

Test set: Average loss: 1.0255, Accuracy: 7683/10000 (77%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.916836
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.119566
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.102650
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.101594
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.152989
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.076398
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.060225
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.927288
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.173220

Test set: Average loss: 1.0453, Accuracy: 7543/10000 (75%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.077913
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.107307
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.984567
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.129230
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.950330
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.168332
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.955030
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.048554
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.094646

Test set: Average loss: 1.0485, Accuracy: 7453/10000 (75%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.094976
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.132467
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.087911
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.948828
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.983955
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.983347
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.910469
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.950220
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.986980

Test set: Average loss: 1.0366, Accuracy: 7581/10000 (76%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.982400
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.060700
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.996132
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.133335
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.930449
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.121485
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.214869
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.093275
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.999081

Test set: Average loss: 1.0294, Accuracy: 7501/10000 (75%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.230123
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.005579
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.115282
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.093612
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.128547
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.875928
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.186052
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.031957
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.999594

Test set: Average loss: 0.9895, Accuracy: 7837/10000 (78%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.939364
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.012553
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.968899
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.025074
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.974641
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.040170
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.025044
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.228360
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.002787

Test set: Average loss: 0.9724, Accuracy: 7821/10000 (78%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.005268
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.982745
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.950806
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.119021
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.052074
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.060482
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.096539
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.898146
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.093719

Test set: Average loss: 0.9673, Accuracy: 7867/10000 (79%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.9, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.09 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 199141
