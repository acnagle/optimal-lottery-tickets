Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.9, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.659353
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.427297
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.388729
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.330133
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.270936
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.301834
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.206731
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.221573
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.199756

Test set: Average loss: 1.1385, Accuracy: 7013/10000 (70%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.094561
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.089813
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.177799
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.294247
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.113561
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.145595
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.037384
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.167317
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.095650

Test set: Average loss: 1.0342, Accuracy: 7640/10000 (76%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.002771
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.930012
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.116672
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.033840
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.968469
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.025928
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.106837
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.022798
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.081690

Test set: Average loss: 1.0434, Accuracy: 7631/10000 (76%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.975198
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.116951
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.147652
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.116421
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.202899
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.120073
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.020534
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.947502
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.168654

Test set: Average loss: 1.0552, Accuracy: 7466/10000 (75%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.090464
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.126680
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.003258
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.156100
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.968830
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.186730
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.980761
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.014347
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.100248

Test set: Average loss: 1.0836, Accuracy: 7051/10000 (71%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.092128
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.183291
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.148163
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.990245
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.951595
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.043698
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.978084
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.997358
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.014870

Test set: Average loss: 1.0882, Accuracy: 7272/10000 (73%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.053162
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.115916
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.059227
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.127491
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.975580
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.245327
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.222629
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.184431
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.100053

Test set: Average loss: 1.0838, Accuracy: 7064/10000 (71%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.271360
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.121507
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.172417
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.160057
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.220088
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.962507
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.266742
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.176407
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.123361

Test set: Average loss: 1.1006, Accuracy: 6935/10000 (69%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.039663
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.108385
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.079618
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.068893
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.052968
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.075540
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.118081
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.296983
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.092390

Test set: Average loss: 1.0876, Accuracy: 7299/10000 (73%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.081889
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.090777
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.015447
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.117888
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.130808
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.135225
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.178219
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.047447
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.240993

Test set: Average loss: 1.0558, Accuracy: 7474/10000 (75%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.031782
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.940422
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.007894
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.994742
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.256603
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.952809
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.041343
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.022733
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.975808

Test set: Average loss: 1.0183, Accuracy: 7542/10000 (75%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.048765
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.161294
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.041821
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.986786
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.032592
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.076578
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.947225
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.923716
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.970719

Test set: Average loss: 0.9853, Accuracy: 7819/10000 (78%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.076475
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.985672
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.909114
Train Epoch: 13 [25536/60000 (43%)]	Loss: 1.032459
Train Epoch: 13 [31936/60000 (53%)]	Loss: 1.037181
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.952242
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.920842
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.992969
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.088315

Test set: Average loss: 0.9776, Accuracy: 7837/10000 (78%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.9, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.40 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 199141
