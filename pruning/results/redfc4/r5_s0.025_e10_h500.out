Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 11.195581
Train Epoch: 1 [12736/60000 (21%)]	Loss: 11.993148
Train Epoch: 1 [19136/60000 (32%)]	Loss: 18.973293
Train Epoch: 1 [25536/60000 (43%)]	Loss: 13.737168
Train Epoch: 1 [31936/60000 (53%)]	Loss: 21.636826
Train Epoch: 1 [38336/60000 (64%)]	Loss: 23.369637
Train Epoch: 1 [44736/60000 (75%)]	Loss: 18.424709
Train Epoch: 1 [51136/60000 (85%)]	Loss: 25.789642
Train Epoch: 1 [57536/60000 (96%)]	Loss: 34.440708

Test set: Average loss: 33.2640, Accuracy: 5096/10000 (51%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 45.244995
Train Epoch: 2 [12736/60000 (21%)]	Loss: 27.084530
Train Epoch: 2 [19136/60000 (32%)]	Loss: 36.322815
Train Epoch: 2 [25536/60000 (43%)]	Loss: 45.579193
Train Epoch: 2 [31936/60000 (53%)]	Loss: 37.701591
Train Epoch: 2 [38336/60000 (64%)]	Loss: 43.859203
Train Epoch: 2 [44736/60000 (75%)]	Loss: 40.026325
Train Epoch: 2 [51136/60000 (85%)]	Loss: 41.896690
Train Epoch: 2 [57536/60000 (96%)]	Loss: 41.414623

Test set: Average loss: 62.9577, Accuracy: 3232/10000 (32%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 48.751915
Train Epoch: 3 [12736/60000 (21%)]	Loss: 43.687908
Train Epoch: 3 [19136/60000 (32%)]	Loss: 42.072865
Train Epoch: 3 [25536/60000 (43%)]	Loss: 57.554707
Train Epoch: 3 [31936/60000 (53%)]	Loss: 45.889999
Train Epoch: 3 [38336/60000 (64%)]	Loss: 49.440090
Train Epoch: 3 [44736/60000 (75%)]	Loss: 52.180496
Train Epoch: 3 [51136/60000 (85%)]	Loss: 54.901848
Train Epoch: 3 [57536/60000 (96%)]	Loss: 56.564636

Test set: Average loss: 53.8789, Accuracy: 3363/10000 (34%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 43.874836
Train Epoch: 4 [12736/60000 (21%)]	Loss: 77.296219
Train Epoch: 4 [19136/60000 (32%)]	Loss: 46.974030
Train Epoch: 4 [25536/60000 (43%)]	Loss: 60.022781
Train Epoch: 4 [31936/60000 (53%)]	Loss: 55.100639
Train Epoch: 4 [38336/60000 (64%)]	Loss: 57.040230
Train Epoch: 4 [44736/60000 (75%)]	Loss: 53.953926
Train Epoch: 4 [51136/60000 (85%)]	Loss: 55.278473
Train Epoch: 4 [57536/60000 (96%)]	Loss: 48.164066

Test set: Average loss: 57.1400, Accuracy: 3070/10000 (31%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 60.840733
Train Epoch: 5 [12736/60000 (21%)]	Loss: 59.968426
Train Epoch: 5 [19136/60000 (32%)]	Loss: 71.469025
Train Epoch: 5 [25536/60000 (43%)]	Loss: 50.494312
Train Epoch: 5 [31936/60000 (53%)]	Loss: 70.518013
Train Epoch: 5 [38336/60000 (64%)]	Loss: 57.436672
Train Epoch: 5 [44736/60000 (75%)]	Loss: 62.437878
Train Epoch: 5 [51136/60000 (85%)]	Loss: 64.296295
Train Epoch: 5 [57536/60000 (96%)]	Loss: 65.225227

Test set: Average loss: 65.1755, Accuracy: 2622/10000 (26%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 64.743576
Train Epoch: 6 [12736/60000 (21%)]	Loss: 60.224007
Train Epoch: 6 [19136/60000 (32%)]	Loss: 77.785477
Train Epoch: 6 [25536/60000 (43%)]	Loss: 65.291435
Train Epoch: 6 [31936/60000 (53%)]	Loss: 79.492493
Train Epoch: 6 [38336/60000 (64%)]	Loss: 63.775997
Train Epoch: 6 [44736/60000 (75%)]	Loss: 68.840340
Train Epoch: 6 [51136/60000 (85%)]	Loss: 59.337246
Train Epoch: 6 [57536/60000 (96%)]	Loss: 68.759254

Test set: Average loss: 66.3287, Accuracy: 2953/10000 (30%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 59.281578
Train Epoch: 7 [12736/60000 (21%)]	Loss: 71.956741
Train Epoch: 7 [19136/60000 (32%)]	Loss: 74.334564
Train Epoch: 7 [25536/60000 (43%)]	Loss: 63.054340
Train Epoch: 7 [31936/60000 (53%)]	Loss: 65.101341
Train Epoch: 7 [38336/60000 (64%)]	Loss: 66.252197
Train Epoch: 7 [44736/60000 (75%)]	Loss: 60.931976
Train Epoch: 7 [51136/60000 (85%)]	Loss: 58.824371
Train Epoch: 7 [57536/60000 (96%)]	Loss: 71.880859

Test set: Average loss: 65.9349, Accuracy: 2773/10000 (28%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 68.286522
Train Epoch: 8 [12736/60000 (21%)]	Loss: 53.593273
Train Epoch: 8 [19136/60000 (32%)]	Loss: 62.080830
Train Epoch: 8 [25536/60000 (43%)]	Loss: 73.076416
Train Epoch: 8 [31936/60000 (53%)]	Loss: 70.489456
Train Epoch: 8 [38336/60000 (64%)]	Loss: 65.752716
Train Epoch: 8 [44736/60000 (75%)]	Loss: 68.653755
Train Epoch: 8 [51136/60000 (85%)]	Loss: 67.402901
Train Epoch: 8 [57536/60000 (96%)]	Loss: 48.709126

Test set: Average loss: 64.2365, Accuracy: 2555/10000 (26%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 64.135414
Train Epoch: 9 [12736/60000 (21%)]	Loss: 57.097218
Train Epoch: 9 [19136/60000 (32%)]	Loss: 76.273796
Train Epoch: 9 [25536/60000 (43%)]	Loss: 59.573792
Train Epoch: 9 [31936/60000 (53%)]	Loss: 57.264816
Train Epoch: 9 [38336/60000 (64%)]	Loss: 66.137299
Train Epoch: 9 [44736/60000 (75%)]	Loss: 65.148582
Train Epoch: 9 [51136/60000 (85%)]	Loss: 58.721703
Train Epoch: 9 [57536/60000 (96%)]	Loss: 77.564156

Test set: Average loss: 60.0620, Accuracy: 2626/10000 (26%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 62.632725
Train Epoch: 10 [12736/60000 (21%)]	Loss: 54.915882
Train Epoch: 10 [19136/60000 (32%)]	Loss: 58.888336
Train Epoch: 10 [25536/60000 (43%)]	Loss: 76.840797
Train Epoch: 10 [31936/60000 (53%)]	Loss: 87.130058
Train Epoch: 10 [38336/60000 (64%)]	Loss: 64.953049
Train Epoch: 10 [44736/60000 (75%)]	Loss: 65.006676
Train Epoch: 10 [51136/60000 (85%)]	Loss: 79.378357
Train Epoch: 10 [57536/60000 (96%)]	Loss: 65.539848

Test set: Average loss: 66.3172, Accuracy: 2221/10000 (22%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.00 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4384009
