Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 11.195581
Train Epoch: 1 [12736/60000 (21%)]	Loss: 11.993148
Train Epoch: 1 [19136/60000 (32%)]	Loss: 18.973293
Train Epoch: 1 [25536/60000 (43%)]	Loss: 13.737168
Train Epoch: 1 [31936/60000 (53%)]	Loss: 21.636826
Train Epoch: 1 [38336/60000 (64%)]	Loss: 23.369637
Train Epoch: 1 [44736/60000 (75%)]	Loss: 18.424709
Train Epoch: 1 [51136/60000 (85%)]	Loss: 25.789642
Train Epoch: 1 [57536/60000 (96%)]	Loss: 34.440708

Test set: Average loss: 33.2640, Accuracy: 5096/10000 (51%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 43.825069
Train Epoch: 2 [12736/60000 (21%)]	Loss: 25.880714
Train Epoch: 2 [19136/60000 (32%)]	Loss: 48.042961
Train Epoch: 2 [25536/60000 (43%)]	Loss: 31.482729
Train Epoch: 2 [31936/60000 (53%)]	Loss: 28.632967
Train Epoch: 2 [38336/60000 (64%)]	Loss: 38.542103
Train Epoch: 2 [44736/60000 (75%)]	Loss: 37.548340
Train Epoch: 2 [51136/60000 (85%)]	Loss: 39.807598
Train Epoch: 2 [57536/60000 (96%)]	Loss: 39.890194

Test set: Average loss: 45.4289, Accuracy: 3882/10000 (39%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 53.235935
Train Epoch: 3 [12736/60000 (21%)]	Loss: 53.495403
Train Epoch: 3 [19136/60000 (32%)]	Loss: 52.010502
Train Epoch: 3 [25536/60000 (43%)]	Loss: 57.939320
Train Epoch: 3 [31936/60000 (53%)]	Loss: 51.492489
Train Epoch: 3 [38336/60000 (64%)]	Loss: 51.952518
Train Epoch: 3 [44736/60000 (75%)]	Loss: 52.658916
Train Epoch: 3 [51136/60000 (85%)]	Loss: 47.626854
Train Epoch: 3 [57536/60000 (96%)]	Loss: 62.197308

Test set: Average loss: 52.4662, Accuracy: 3387/10000 (34%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 57.783318
Train Epoch: 4 [12736/60000 (21%)]	Loss: 84.134636
Train Epoch: 4 [19136/60000 (32%)]	Loss: 46.675213
Train Epoch: 4 [25536/60000 (43%)]	Loss: 59.432964
Train Epoch: 4 [31936/60000 (53%)]	Loss: 63.850883
Train Epoch: 4 [38336/60000 (64%)]	Loss: 65.524910
Train Epoch: 4 [44736/60000 (75%)]	Loss: 55.543129
Train Epoch: 4 [51136/60000 (85%)]	Loss: 65.090263
Train Epoch: 4 [57536/60000 (96%)]	Loss: 36.366173

Test set: Average loss: 57.7080, Accuracy: 3158/10000 (32%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 72.912704
Train Epoch: 5 [12736/60000 (21%)]	Loss: 69.619461
Train Epoch: 5 [19136/60000 (32%)]	Loss: 69.342407
Train Epoch: 5 [25536/60000 (43%)]	Loss: 60.843479
Train Epoch: 5 [31936/60000 (53%)]	Loss: 63.471043
Train Epoch: 5 [38336/60000 (64%)]	Loss: 62.677212
Train Epoch: 5 [44736/60000 (75%)]	Loss: 63.558880
Train Epoch: 5 [51136/60000 (85%)]	Loss: 64.467461
Train Epoch: 5 [57536/60000 (96%)]	Loss: 69.665916

Test set: Average loss: 64.6941, Accuracy: 2499/10000 (25%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 56.993889
Train Epoch: 6 [12736/60000 (21%)]	Loss: 61.524029
Train Epoch: 6 [19136/60000 (32%)]	Loss: 74.746414
Train Epoch: 6 [25536/60000 (43%)]	Loss: 74.355156
Train Epoch: 6 [31936/60000 (53%)]	Loss: 82.391762
Train Epoch: 6 [38336/60000 (64%)]	Loss: 69.224876
Train Epoch: 6 [44736/60000 (75%)]	Loss: 68.650024
Train Epoch: 6 [51136/60000 (85%)]	Loss: 55.052139
Train Epoch: 6 [57536/60000 (96%)]	Loss: 66.329582

Test set: Average loss: 65.1546, Accuracy: 2851/10000 (29%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 67.983673
Train Epoch: 7 [12736/60000 (21%)]	Loss: 67.169182
Train Epoch: 7 [19136/60000 (32%)]	Loss: 76.578674
Train Epoch: 7 [25536/60000 (43%)]	Loss: 54.248474
Train Epoch: 7 [31936/60000 (53%)]	Loss: 68.387329
Train Epoch: 7 [38336/60000 (64%)]	Loss: 68.908234
Train Epoch: 7 [44736/60000 (75%)]	Loss: 62.987362
Train Epoch: 7 [51136/60000 (85%)]	Loss: 60.997925
Train Epoch: 7 [57536/60000 (96%)]	Loss: 85.441826

Test set: Average loss: 66.9680, Accuracy: 2719/10000 (27%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 66.788368
Train Epoch: 8 [12736/60000 (21%)]	Loss: 53.564800
Train Epoch: 8 [19136/60000 (32%)]	Loss: 63.022217
Train Epoch: 8 [25536/60000 (43%)]	Loss: 73.965858
Train Epoch: 8 [31936/60000 (53%)]	Loss: 72.869637
Train Epoch: 8 [38336/60000 (64%)]	Loss: 61.887424
Train Epoch: 8 [44736/60000 (75%)]	Loss: 71.905571
Train Epoch: 8 [51136/60000 (85%)]	Loss: 64.509033
Train Epoch: 8 [57536/60000 (96%)]	Loss: 53.007122

Test set: Average loss: 68.3010, Accuracy: 2390/10000 (24%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 66.445450
Train Epoch: 9 [12736/60000 (21%)]	Loss: 61.172733
Train Epoch: 9 [19136/60000 (32%)]	Loss: 73.382057
Train Epoch: 9 [25536/60000 (43%)]	Loss: 60.864193
Train Epoch: 9 [31936/60000 (53%)]	Loss: 59.629951
Train Epoch: 9 [38336/60000 (64%)]	Loss: 67.667244
Train Epoch: 9 [44736/60000 (75%)]	Loss: 62.756615
Train Epoch: 9 [51136/60000 (85%)]	Loss: 70.715950
Train Epoch: 9 [57536/60000 (96%)]	Loss: 69.262985

Test set: Average loss: 70.0928, Accuracy: 2184/10000 (22%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 69.962914
Train Epoch: 10 [12736/60000 (21%)]	Loss: 67.430420
Train Epoch: 10 [19136/60000 (32%)]	Loss: 54.264225
Train Epoch: 10 [25536/60000 (43%)]	Loss: 86.235138
Train Epoch: 10 [31936/60000 (53%)]	Loss: 92.332794
Train Epoch: 10 [38336/60000 (64%)]	Loss: 67.043610
Train Epoch: 10 [44736/60000 (75%)]	Loss: 68.199081
Train Epoch: 10 [51136/60000 (85%)]	Loss: 90.496407
Train Epoch: 10 [57536/60000 (96%)]	Loss: 67.363525

Test set: Average loss: 66.7343, Accuracy: 2398/10000 (24%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 53.510525
Train Epoch: 11 [12736/60000 (21%)]	Loss: 63.015957
Train Epoch: 11 [19136/60000 (32%)]	Loss: 69.702927
Train Epoch: 11 [25536/60000 (43%)]	Loss: 64.253082
Train Epoch: 11 [31936/60000 (53%)]	Loss: 62.121758
Train Epoch: 11 [38336/60000 (64%)]	Loss: 60.590515
Train Epoch: 11 [44736/60000 (75%)]	Loss: 57.240761
Train Epoch: 11 [51136/60000 (85%)]	Loss: 64.737541
Train Epoch: 11 [57536/60000 (96%)]	Loss: 68.361366

Test set: Average loss: 75.2011, Accuracy: 1927/10000 (19%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.20 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4384009
