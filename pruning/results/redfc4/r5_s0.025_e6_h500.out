Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 11.195581
Train Epoch: 1 [12736/60000 (21%)]	Loss: 11.993148
Train Epoch: 1 [19136/60000 (32%)]	Loss: 18.973293
Train Epoch: 1 [25536/60000 (43%)]	Loss: 13.737168
Train Epoch: 1 [31936/60000 (53%)]	Loss: 21.636826
Train Epoch: 1 [38336/60000 (64%)]	Loss: 23.369637
Train Epoch: 1 [44736/60000 (75%)]	Loss: 18.424709
Train Epoch: 1 [51136/60000 (85%)]	Loss: 25.789642
Train Epoch: 1 [57536/60000 (96%)]	Loss: 34.440708

Test set: Average loss: 33.2640, Accuracy: 5096/10000 (51%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 54.272758
Train Epoch: 2 [12736/60000 (21%)]	Loss: 47.836216
Train Epoch: 2 [19136/60000 (32%)]	Loss: 39.461208
Train Epoch: 2 [25536/60000 (43%)]	Loss: 40.508392
Train Epoch: 2 [31936/60000 (53%)]	Loss: 24.246124
Train Epoch: 2 [38336/60000 (64%)]	Loss: 43.945847
Train Epoch: 2 [44736/60000 (75%)]	Loss: 45.282196
Train Epoch: 2 [51136/60000 (85%)]	Loss: 44.221107
Train Epoch: 2 [57536/60000 (96%)]	Loss: 38.013119

Test set: Average loss: 48.4949, Accuracy: 3647/10000 (36%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 50.818096
Train Epoch: 3 [12736/60000 (21%)]	Loss: 51.087307
Train Epoch: 3 [19136/60000 (32%)]	Loss: 52.299362
Train Epoch: 3 [25536/60000 (43%)]	Loss: 62.397011
Train Epoch: 3 [31936/60000 (53%)]	Loss: 40.430000
Train Epoch: 3 [38336/60000 (64%)]	Loss: 50.156799
Train Epoch: 3 [44736/60000 (75%)]	Loss: 52.049786
Train Epoch: 3 [51136/60000 (85%)]	Loss: 45.020721
Train Epoch: 3 [57536/60000 (96%)]	Loss: 54.933552

Test set: Average loss: 52.1692, Accuracy: 3444/10000 (34%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 47.867313
Train Epoch: 4 [12736/60000 (21%)]	Loss: 79.323196
Train Epoch: 4 [19136/60000 (32%)]	Loss: 44.846004
Train Epoch: 4 [25536/60000 (43%)]	Loss: 65.503464
Train Epoch: 4 [31936/60000 (53%)]	Loss: 61.754124
Train Epoch: 4 [38336/60000 (64%)]	Loss: 61.067207
Train Epoch: 4 [44736/60000 (75%)]	Loss: 53.255535
Train Epoch: 4 [51136/60000 (85%)]	Loss: 52.844322
Train Epoch: 4 [57536/60000 (96%)]	Loss: 48.134743

Test set: Average loss: 57.3246, Accuracy: 3237/10000 (32%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 54.383862
Train Epoch: 5 [12736/60000 (21%)]	Loss: 59.689041
Train Epoch: 5 [19136/60000 (32%)]	Loss: 66.415909
Train Epoch: 5 [25536/60000 (43%)]	Loss: 53.003609
Train Epoch: 5 [31936/60000 (53%)]	Loss: 63.026760
Train Epoch: 5 [38336/60000 (64%)]	Loss: 51.518707
Train Epoch: 5 [44736/60000 (75%)]	Loss: 52.719345
Train Epoch: 5 [51136/60000 (85%)]	Loss: 54.484333
Train Epoch: 5 [57536/60000 (96%)]	Loss: 64.010933

Test set: Average loss: 57.4231, Accuracy: 2830/10000 (28%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 55.495487
Train Epoch: 6 [12736/60000 (21%)]	Loss: 58.082821
Train Epoch: 6 [19136/60000 (32%)]	Loss: 68.537193
Train Epoch: 6 [25536/60000 (43%)]	Loss: 61.044903
Train Epoch: 6 [31936/60000 (53%)]	Loss: 69.502586
Train Epoch: 6 [38336/60000 (64%)]	Loss: 61.462437
Train Epoch: 6 [44736/60000 (75%)]	Loss: 57.971928
Train Epoch: 6 [51136/60000 (85%)]	Loss: 49.607224
Train Epoch: 6 [57536/60000 (96%)]	Loss: 54.858986

Test set: Average loss: 56.3698, Accuracy: 3169/10000 (32%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.20 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4384009
