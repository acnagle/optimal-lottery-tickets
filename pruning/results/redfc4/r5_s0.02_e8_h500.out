Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=8, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 7.253252
Train Epoch: 1 [12736/60000 (21%)]	Loss: 25.429331
Train Epoch: 1 [19136/60000 (32%)]	Loss: 34.917747
Train Epoch: 1 [25536/60000 (43%)]	Loss: 38.026100
Train Epoch: 1 [31936/60000 (53%)]	Loss: 29.555229
Train Epoch: 1 [38336/60000 (64%)]	Loss: 27.226795
Train Epoch: 1 [44736/60000 (75%)]	Loss: 33.277142
Train Epoch: 1 [51136/60000 (85%)]	Loss: 45.097240
Train Epoch: 1 [57536/60000 (96%)]	Loss: 48.676105

Test set: Average loss: 47.5576, Accuracy: 3854/10000 (39%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 49.822819
Train Epoch: 2 [12736/60000 (21%)]	Loss: 40.788193
Train Epoch: 2 [19136/60000 (32%)]	Loss: 56.475792
Train Epoch: 2 [25536/60000 (43%)]	Loss: 51.794804
Train Epoch: 2 [31936/60000 (53%)]	Loss: 48.920914
Train Epoch: 2 [38336/60000 (64%)]	Loss: 64.174675
Train Epoch: 2 [44736/60000 (75%)]	Loss: 60.227974
Train Epoch: 2 [51136/60000 (85%)]	Loss: 66.569626
Train Epoch: 2 [57536/60000 (96%)]	Loss: 61.255474

Test set: Average loss: 67.2675, Accuracy: 2289/10000 (23%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 61.005363
Train Epoch: 3 [12736/60000 (21%)]	Loss: 70.504288
Train Epoch: 3 [19136/60000 (32%)]	Loss: 68.029503
Train Epoch: 3 [25536/60000 (43%)]	Loss: 86.950165
Train Epoch: 3 [31936/60000 (53%)]	Loss: 64.479584
Train Epoch: 3 [38336/60000 (64%)]	Loss: 75.166206
Train Epoch: 3 [44736/60000 (75%)]	Loss: 68.841087
Train Epoch: 3 [51136/60000 (85%)]	Loss: 80.181190
Train Epoch: 3 [57536/60000 (96%)]	Loss: 81.432098

Test set: Average loss: 79.8696, Accuracy: 2182/10000 (22%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 75.453751
Train Epoch: 4 [12736/60000 (21%)]	Loss: 103.202652
Train Epoch: 4 [19136/60000 (32%)]	Loss: 72.169502
Train Epoch: 4 [25536/60000 (43%)]	Loss: 88.499237
Train Epoch: 4 [31936/60000 (53%)]	Loss: 95.102966
Train Epoch: 4 [38336/60000 (64%)]	Loss: 89.887062
Train Epoch: 4 [44736/60000 (75%)]	Loss: 82.641624
Train Epoch: 4 [51136/60000 (85%)]	Loss: 85.276939
Train Epoch: 4 [57536/60000 (96%)]	Loss: 60.712433

Test set: Average loss: 83.9070, Accuracy: 2116/10000 (21%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 89.421478
Train Epoch: 5 [12736/60000 (21%)]	Loss: 85.842690
Train Epoch: 5 [19136/60000 (32%)]	Loss: 89.418465
Train Epoch: 5 [25536/60000 (43%)]	Loss: 80.843201
Train Epoch: 5 [31936/60000 (53%)]	Loss: 91.640579
Train Epoch: 5 [38336/60000 (64%)]	Loss: 76.489281
Train Epoch: 5 [44736/60000 (75%)]	Loss: 91.176910
Train Epoch: 5 [51136/60000 (85%)]	Loss: 82.445412
Train Epoch: 5 [57536/60000 (96%)]	Loss: 100.472153

Test set: Average loss: 86.6016, Accuracy: 1871/10000 (19%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 78.012596
Train Epoch: 6 [12736/60000 (21%)]	Loss: 76.923393
Train Epoch: 6 [19136/60000 (32%)]	Loss: 109.549011
Train Epoch: 6 [25536/60000 (43%)]	Loss: 89.251884
Train Epoch: 6 [31936/60000 (53%)]	Loss: 109.519394
Train Epoch: 6 [38336/60000 (64%)]	Loss: 93.106316
Train Epoch: 6 [44736/60000 (75%)]	Loss: 83.627571
Train Epoch: 6 [51136/60000 (85%)]	Loss: 84.524918
Train Epoch: 6 [57536/60000 (96%)]	Loss: 96.192963

Test set: Average loss: 87.7941, Accuracy: 2021/10000 (20%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 102.072495
Train Epoch: 7 [12736/60000 (21%)]	Loss: 85.841492
Train Epoch: 7 [19136/60000 (32%)]	Loss: 93.019508
Train Epoch: 7 [25536/60000 (43%)]	Loss: 83.682816
Train Epoch: 7 [31936/60000 (53%)]	Loss: 81.220192
Train Epoch: 7 [38336/60000 (64%)]	Loss: 102.216995
Train Epoch: 7 [44736/60000 (75%)]	Loss: 86.542625
Train Epoch: 7 [51136/60000 (85%)]	Loss: 81.034355
Train Epoch: 7 [57536/60000 (96%)]	Loss: 100.050705

Test set: Average loss: 91.0281, Accuracy: 1818/10000 (18%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 83.321762
Train Epoch: 8 [12736/60000 (21%)]	Loss: 59.511482
Train Epoch: 8 [19136/60000 (32%)]	Loss: 85.383308
Train Epoch: 8 [25536/60000 (43%)]	Loss: 89.046318
Train Epoch: 8 [31936/60000 (53%)]	Loss: 77.543922
Train Epoch: 8 [38336/60000 (64%)]	Loss: 83.538689
Train Epoch: 8 [44736/60000 (75%)]	Loss: 93.149071
Train Epoch: 8 [51136/60000 (85%)]	Loss: 89.794701
Train Epoch: 8 [57536/60000 (96%)]	Loss: 70.556137

Test set: Average loss: 89.8698, Accuracy: 1732/10000 (17%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=8, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.60 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
