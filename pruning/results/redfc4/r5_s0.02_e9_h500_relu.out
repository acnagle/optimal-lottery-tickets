Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.904168
Train Epoch: 1 [12736/60000 (21%)]	Loss: 3.295049
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.552685
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.573287
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.663903
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.334898
Train Epoch: 1 [44736/60000 (75%)]	Loss: 4.441846
Train Epoch: 1 [51136/60000 (85%)]	Loss: 3.991321
Train Epoch: 1 [57536/60000 (96%)]	Loss: 3.708132

Test set: Average loss: 4.7151, Accuracy: 5944/10000 (59%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 8.084052
Train Epoch: 2 [12736/60000 (21%)]	Loss: 4.940996
Train Epoch: 2 [19136/60000 (32%)]	Loss: 4.396262
Train Epoch: 2 [25536/60000 (43%)]	Loss: 5.074279
Train Epoch: 2 [31936/60000 (53%)]	Loss: 3.569201
Train Epoch: 2 [38336/60000 (64%)]	Loss: 6.418973
Train Epoch: 2 [44736/60000 (75%)]	Loss: 5.324110
Train Epoch: 2 [51136/60000 (85%)]	Loss: 8.825500
Train Epoch: 2 [57536/60000 (96%)]	Loss: 5.944705

Test set: Average loss: 5.6829, Accuracy: 5620/10000 (56%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 6.707324
Train Epoch: 3 [12736/60000 (21%)]	Loss: 7.401541
Train Epoch: 3 [19136/60000 (32%)]	Loss: 7.280144
Train Epoch: 3 [25536/60000 (43%)]	Loss: 5.737602
Train Epoch: 3 [31936/60000 (53%)]	Loss: 7.327627
Train Epoch: 3 [38336/60000 (64%)]	Loss: 9.374818
Train Epoch: 3 [44736/60000 (75%)]	Loss: 10.238908
Train Epoch: 3 [51136/60000 (85%)]	Loss: 7.409640
Train Epoch: 3 [57536/60000 (96%)]	Loss: 11.204972

Test set: Average loss: 11.1845, Accuracy: 3482/10000 (35%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 10.022503
Train Epoch: 4 [12736/60000 (21%)]	Loss: 14.997396
Train Epoch: 4 [19136/60000 (32%)]	Loss: 11.457402
Train Epoch: 4 [25536/60000 (43%)]	Loss: 14.334935
Train Epoch: 4 [31936/60000 (53%)]	Loss: 14.022069
Train Epoch: 4 [38336/60000 (64%)]	Loss: 12.805586
Train Epoch: 4 [44736/60000 (75%)]	Loss: 14.489610
Train Epoch: 4 [51136/60000 (85%)]	Loss: 12.811825
Train Epoch: 4 [57536/60000 (96%)]	Loss: 8.932825

Test set: Average loss: 15.1047, Accuracy: 2624/10000 (26%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 14.010384
Train Epoch: 5 [12736/60000 (21%)]	Loss: 12.434369
Train Epoch: 5 [19136/60000 (32%)]	Loss: 12.644083
Train Epoch: 5 [25536/60000 (43%)]	Loss: 12.967176
Train Epoch: 5 [31936/60000 (53%)]	Loss: 17.253330
Train Epoch: 5 [38336/60000 (64%)]	Loss: 16.977493
Train Epoch: 5 [44736/60000 (75%)]	Loss: 16.284716
Train Epoch: 5 [51136/60000 (85%)]	Loss: 16.163673
Train Epoch: 5 [57536/60000 (96%)]	Loss: 18.033047

Test set: Average loss: 16.4062, Accuracy: 2421/10000 (24%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 14.960687
Train Epoch: 6 [12736/60000 (21%)]	Loss: 17.715029
Train Epoch: 6 [19136/60000 (32%)]	Loss: 18.749836
Train Epoch: 6 [25536/60000 (43%)]	Loss: 16.538774
Train Epoch: 6 [31936/60000 (53%)]	Loss: 15.876614
Train Epoch: 6 [38336/60000 (64%)]	Loss: 16.839369
Train Epoch: 6 [44736/60000 (75%)]	Loss: 20.107197
Train Epoch: 6 [51136/60000 (85%)]	Loss: 17.241615
Train Epoch: 6 [57536/60000 (96%)]	Loss: 18.186581

Test set: Average loss: 16.8064, Accuracy: 2196/10000 (22%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 16.042400
Train Epoch: 7 [12736/60000 (21%)]	Loss: 13.838860
Train Epoch: 7 [19136/60000 (32%)]	Loss: 16.152514
Train Epoch: 7 [25536/60000 (43%)]	Loss: 15.833949
Train Epoch: 7 [31936/60000 (53%)]	Loss: 18.574364
Train Epoch: 7 [38336/60000 (64%)]	Loss: 17.510616
Train Epoch: 7 [44736/60000 (75%)]	Loss: 16.921564
Train Epoch: 7 [51136/60000 (85%)]	Loss: 15.551780
Train Epoch: 7 [57536/60000 (96%)]	Loss: 18.594896

Test set: Average loss: 17.7454, Accuracy: 2102/10000 (21%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 17.143988
Train Epoch: 8 [12736/60000 (21%)]	Loss: 11.608864
Train Epoch: 8 [19136/60000 (32%)]	Loss: 18.618807
Train Epoch: 8 [25536/60000 (43%)]	Loss: 15.993293
Train Epoch: 8 [31936/60000 (53%)]	Loss: 15.886947
Train Epoch: 8 [38336/60000 (64%)]	Loss: 20.709846
Train Epoch: 8 [44736/60000 (75%)]	Loss: 15.209532
Train Epoch: 8 [51136/60000 (85%)]	Loss: 17.471357
Train Epoch: 8 [57536/60000 (96%)]	Loss: 17.268854

Test set: Average loss: 16.8113, Accuracy: 2232/10000 (22%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 16.176594
Train Epoch: 9 [12736/60000 (21%)]	Loss: 15.460410
Train Epoch: 9 [19136/60000 (32%)]	Loss: 17.307240
Train Epoch: 9 [25536/60000 (43%)]	Loss: 13.531028
Train Epoch: 9 [31936/60000 (53%)]	Loss: 16.550838
Train Epoch: 9 [38336/60000 (64%)]	Loss: 17.478411
Train Epoch: 9 [44736/60000 (75%)]	Loss: 18.352777
Train Epoch: 9 [51136/60000 (85%)]	Loss: 13.505750
Train Epoch: 9 [57536/60000 (96%)]	Loss: 19.709667

Test set: Average loss: 17.5275, Accuracy: 2118/10000 (21%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.81 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
