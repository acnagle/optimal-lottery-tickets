Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=True, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.302583
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.302449
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.302224
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.302287
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.301998
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.299843
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.300602
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.295126
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.287957

Test set: Average loss: 2.2887, Accuracy: 1160/10000 (12%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.276298
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.285112
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.294620
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.248178
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.241091
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.259904
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.248826
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.202315
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.158175

Test set: Average loss: 2.1856, Accuracy: 1685/10000 (17%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.145265
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.053774
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.113629
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.066030
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.055253
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.077360
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.067575
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.051488
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.037740

Test set: Average loss: 2.0313, Accuracy: 2806/10000 (28%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.011237
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.042948
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.987365
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.007592
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.016757
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.974604
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.005805
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.009170
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.951198

Test set: Average loss: 1.9752, Accuracy: 3102/10000 (31%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.998603
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.041315
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.951386
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.927677
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.917337
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.013004
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.939856
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.947019
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.975429

Test set: Average loss: 1.9513, Accuracy: 3441/10000 (34%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.965146
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.937619
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.944748
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.949606
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.934276
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.936745
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.953458
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.940733
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.975765

Test set: Average loss: 1.9379, Accuracy: 3373/10000 (34%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.21 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 224821
