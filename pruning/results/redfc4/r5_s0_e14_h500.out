Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.0, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 157.418182
Train Epoch: 1 [12736/60000 (21%)]	Loss: 230.465469
Train Epoch: 1 [19136/60000 (32%)]	Loss: 225.913086
Train Epoch: 1 [25536/60000 (43%)]	Loss: 201.067261
Train Epoch: 1 [31936/60000 (53%)]	Loss: 181.535690
Train Epoch: 1 [38336/60000 (64%)]	Loss: 215.293457
Train Epoch: 1 [44736/60000 (75%)]	Loss: 217.300156
Train Epoch: 1 [51136/60000 (85%)]	Loss: 192.630768
Train Epoch: 1 [57536/60000 (96%)]	Loss: 210.543350

Test set: Average loss: 205.6305, Accuracy: 1047/10000 (10%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 180.367004
Train Epoch: 2 [12736/60000 (21%)]	Loss: 213.071793
Train Epoch: 2 [19136/60000 (32%)]	Loss: 200.291168
Train Epoch: 2 [25536/60000 (43%)]	Loss: 211.101120
Train Epoch: 2 [31936/60000 (53%)]	Loss: 194.288696
Train Epoch: 2 [38336/60000 (64%)]	Loss: 219.276886
Train Epoch: 2 [44736/60000 (75%)]	Loss: 207.023071
Train Epoch: 2 [51136/60000 (85%)]	Loss: 230.074081
Train Epoch: 2 [57536/60000 (96%)]	Loss: 224.027695

Test set: Average loss: 205.6195, Accuracy: 1047/10000 (10%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 174.036102
Train Epoch: 3 [12736/60000 (21%)]	Loss: 213.826889
Train Epoch: 3 [19136/60000 (32%)]	Loss: 219.364822
Train Epoch: 3 [25536/60000 (43%)]	Loss: 223.442078
Train Epoch: 3 [31936/60000 (53%)]	Loss: 209.358154
Train Epoch: 3 [38336/60000 (64%)]	Loss: 201.746155
Train Epoch: 3 [44736/60000 (75%)]	Loss: 199.699829
Train Epoch: 3 [51136/60000 (85%)]	Loss: 190.701996
Train Epoch: 3 [57536/60000 (96%)]	Loss: 205.713120

Test set: Average loss: 205.7529, Accuracy: 1047/10000 (10%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 184.803558
Train Epoch: 4 [12736/60000 (21%)]	Loss: 238.878876
Train Epoch: 4 [19136/60000 (32%)]	Loss: 208.699936
Train Epoch: 4 [25536/60000 (43%)]	Loss: 219.471375
Train Epoch: 4 [31936/60000 (53%)]	Loss: 203.553497
Train Epoch: 4 [38336/60000 (64%)]	Loss: 218.683136
Train Epoch: 4 [44736/60000 (75%)]	Loss: 183.872879
Train Epoch: 4 [51136/60000 (85%)]	Loss: 207.569504
Train Epoch: 4 [57536/60000 (96%)]	Loss: 167.328613

Test set: Average loss: 205.3506, Accuracy: 1047/10000 (10%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 212.958664
Train Epoch: 5 [12736/60000 (21%)]	Loss: 200.323425
Train Epoch: 5 [19136/60000 (32%)]	Loss: 189.882965
Train Epoch: 5 [25536/60000 (43%)]	Loss: 202.514557
Train Epoch: 5 [31936/60000 (53%)]	Loss: 218.446594
Train Epoch: 5 [38336/60000 (64%)]	Loss: 192.091980
Train Epoch: 5 [44736/60000 (75%)]	Loss: 212.992462
Train Epoch: 5 [51136/60000 (85%)]	Loss: 193.908920
Train Epoch: 5 [57536/60000 (96%)]	Loss: 213.269791

Test set: Average loss: 205.5177, Accuracy: 1047/10000 (10%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 193.586884
Train Epoch: 6 [12736/60000 (21%)]	Loss: 186.157120
Train Epoch: 6 [19136/60000 (32%)]	Loss: 224.166183
Train Epoch: 6 [25536/60000 (43%)]	Loss: 199.816681
Train Epoch: 6 [31936/60000 (53%)]	Loss: 226.185654
Train Epoch: 6 [38336/60000 (64%)]	Loss: 207.062424
Train Epoch: 6 [44736/60000 (75%)]	Loss: 200.587830
Train Epoch: 6 [51136/60000 (85%)]	Loss: 216.682343
Train Epoch: 6 [57536/60000 (96%)]	Loss: 219.214325

Test set: Average loss: 205.5156, Accuracy: 1047/10000 (10%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 205.254974
Train Epoch: 7 [12736/60000 (21%)]	Loss: 210.182693
Train Epoch: 7 [19136/60000 (32%)]	Loss: 216.626648
Train Epoch: 7 [25536/60000 (43%)]	Loss: 197.557800
Train Epoch: 7 [31936/60000 (53%)]	Loss: 223.480637
Train Epoch: 7 [38336/60000 (64%)]	Loss: 217.818390
Train Epoch: 7 [44736/60000 (75%)]	Loss: 203.119781
Train Epoch: 7 [51136/60000 (85%)]	Loss: 191.747147
Train Epoch: 7 [57536/60000 (96%)]	Loss: 223.144241

Test set: Average loss: 205.6564, Accuracy: 1047/10000 (10%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 198.506287
Train Epoch: 8 [12736/60000 (21%)]	Loss: 187.521515
Train Epoch: 8 [19136/60000 (32%)]	Loss: 219.489487
Train Epoch: 8 [25536/60000 (43%)]	Loss: 201.906357
Train Epoch: 8 [31936/60000 (53%)]	Loss: 204.141342
Train Epoch: 8 [38336/60000 (64%)]	Loss: 197.670837
Train Epoch: 8 [44736/60000 (75%)]	Loss: 196.319565
Train Epoch: 8 [51136/60000 (85%)]	Loss: 198.226913
Train Epoch: 8 [57536/60000 (96%)]	Loss: 197.152832

Test set: Average loss: 205.6012, Accuracy: 1047/10000 (10%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 196.143814
Train Epoch: 9 [12736/60000 (21%)]	Loss: 205.523743
Train Epoch: 9 [19136/60000 (32%)]	Loss: 206.469635
Train Epoch: 9 [25536/60000 (43%)]	Loss: 190.652344
Train Epoch: 9 [31936/60000 (53%)]	Loss: 209.094315
Train Epoch: 9 [38336/60000 (64%)]	Loss: 208.936935
Train Epoch: 9 [44736/60000 (75%)]	Loss: 205.688370
Train Epoch: 9 [51136/60000 (85%)]	Loss: 212.295792
Train Epoch: 9 [57536/60000 (96%)]	Loss: 238.796646

Test set: Average loss: 205.7383, Accuracy: 1047/10000 (10%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 217.615280
Train Epoch: 10 [12736/60000 (21%)]	Loss: 196.748886
Train Epoch: 10 [19136/60000 (32%)]	Loss: 210.514771
Train Epoch: 10 [25536/60000 (43%)]	Loss: 215.338562
Train Epoch: 10 [31936/60000 (53%)]	Loss: 249.494675
Train Epoch: 10 [38336/60000 (64%)]	Loss: 210.227707
Train Epoch: 10 [44736/60000 (75%)]	Loss: 217.853348
Train Epoch: 10 [51136/60000 (85%)]	Loss: 231.579575
Train Epoch: 10 [57536/60000 (96%)]	Loss: 203.323669

Test set: Average loss: 205.9168, Accuracy: 1047/10000 (10%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 195.864624
Train Epoch: 11 [12736/60000 (21%)]	Loss: 206.100632
Train Epoch: 11 [19136/60000 (32%)]	Loss: 225.810745
Train Epoch: 11 [25536/60000 (43%)]	Loss: 169.914627
Train Epoch: 11 [31936/60000 (53%)]	Loss: 212.022079
Train Epoch: 11 [38336/60000 (64%)]	Loss: 199.187714
Train Epoch: 11 [44736/60000 (75%)]	Loss: 205.054565
Train Epoch: 11 [51136/60000 (85%)]	Loss: 211.992859
Train Epoch: 11 [57536/60000 (96%)]	Loss: 222.431366

Test set: Average loss: 205.7532, Accuracy: 1047/10000 (10%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 185.480286
Train Epoch: 12 [12736/60000 (21%)]	Loss: 213.864914
Train Epoch: 12 [19136/60000 (32%)]	Loss: 224.915070
Train Epoch: 12 [25536/60000 (43%)]	Loss: 210.383423
Train Epoch: 12 [31936/60000 (53%)]	Loss: 204.524063
Train Epoch: 12 [38336/60000 (64%)]	Loss: 215.126862
Train Epoch: 12 [44736/60000 (75%)]	Loss: 204.087708
Train Epoch: 12 [51136/60000 (85%)]	Loss: 212.956787
Train Epoch: 12 [57536/60000 (96%)]	Loss: 193.553406

Test set: Average loss: 205.6480, Accuracy: 1047/10000 (10%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 210.851364
Train Epoch: 13 [12736/60000 (21%)]	Loss: 192.070801
Train Epoch: 13 [19136/60000 (32%)]	Loss: 232.454071
Train Epoch: 13 [25536/60000 (43%)]	Loss: 214.654816
Train Epoch: 13 [31936/60000 (53%)]	Loss: 212.796661
Train Epoch: 13 [38336/60000 (64%)]	Loss: 202.459900
Train Epoch: 13 [44736/60000 (75%)]	Loss: 210.986588
Train Epoch: 13 [51136/60000 (85%)]	Loss: 190.461060
Train Epoch: 13 [57536/60000 (96%)]	Loss: 199.917465

Test set: Average loss: 205.8013, Accuracy: 1047/10000 (10%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 245.212097
Train Epoch: 14 [12736/60000 (21%)]	Loss: 169.807770
Train Epoch: 14 [19136/60000 (32%)]	Loss: 227.939194
Train Epoch: 14 [25536/60000 (43%)]	Loss: 194.807922
Train Epoch: 14 [31936/60000 (53%)]	Loss: 216.302261
Train Epoch: 14 [38336/60000 (64%)]	Loss: 190.466446
Train Epoch: 14 [44736/60000 (75%)]	Loss: 208.639236
Train Epoch: 14 [51136/60000 (85%)]	Loss: 221.227768
Train Epoch: 14 [57536/60000 (96%)]	Loss: 188.667572

Test set: Average loss: 205.5700, Accuracy: 1047/10000 (10%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.0, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.79 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4496420
