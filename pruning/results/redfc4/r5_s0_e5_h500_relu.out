Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=5, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.0, use_relu=True, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 38.691383
Train Epoch: 1 [12736/60000 (21%)]	Loss: 47.710945
Train Epoch: 1 [19136/60000 (32%)]	Loss: 43.611263
Train Epoch: 1 [25536/60000 (43%)]	Loss: 41.637352
Train Epoch: 1 [31936/60000 (53%)]	Loss: 39.589924
Train Epoch: 1 [38336/60000 (64%)]	Loss: 45.375179
Train Epoch: 1 [44736/60000 (75%)]	Loss: 43.226616
Train Epoch: 1 [51136/60000 (85%)]	Loss: 37.791367
Train Epoch: 1 [57536/60000 (96%)]	Loss: 40.037407

Test set: Average loss: 41.6253, Accuracy: 1340/10000 (13%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 30.151495
Train Epoch: 2 [12736/60000 (21%)]	Loss: 44.133835
Train Epoch: 2 [19136/60000 (32%)]	Loss: 45.287285
Train Epoch: 2 [25536/60000 (43%)]	Loss: 39.701027
Train Epoch: 2 [31936/60000 (53%)]	Loss: 42.230289
Train Epoch: 2 [38336/60000 (64%)]	Loss: 37.854401
Train Epoch: 2 [44736/60000 (75%)]	Loss: 46.130058
Train Epoch: 2 [51136/60000 (85%)]	Loss: 40.037510
Train Epoch: 2 [57536/60000 (96%)]	Loss: 42.865688

Test set: Average loss: 41.5690, Accuracy: 1340/10000 (13%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 32.218151
Train Epoch: 3 [12736/60000 (21%)]	Loss: 37.723251
Train Epoch: 3 [19136/60000 (32%)]	Loss: 37.545769
Train Epoch: 3 [25536/60000 (43%)]	Loss: 38.415375
Train Epoch: 3 [31936/60000 (53%)]	Loss: 41.311199
Train Epoch: 3 [38336/60000 (64%)]	Loss: 46.224094
Train Epoch: 3 [44736/60000 (75%)]	Loss: 39.438995
Train Epoch: 3 [51136/60000 (85%)]	Loss: 32.623119
Train Epoch: 3 [57536/60000 (96%)]	Loss: 39.728981

Test set: Average loss: 41.6480, Accuracy: 1340/10000 (13%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 38.117096
Train Epoch: 4 [12736/60000 (21%)]	Loss: 44.204456
Train Epoch: 4 [19136/60000 (32%)]	Loss: 45.616474
Train Epoch: 4 [25536/60000 (43%)]	Loss: 45.118916
Train Epoch: 4 [31936/60000 (53%)]	Loss: 44.130817
Train Epoch: 4 [38336/60000 (64%)]	Loss: 38.249641
Train Epoch: 4 [44736/60000 (75%)]	Loss: 40.857487
Train Epoch: 4 [51136/60000 (85%)]	Loss: 35.202278
Train Epoch: 4 [57536/60000 (96%)]	Loss: 30.805527

Test set: Average loss: 41.5773, Accuracy: 1340/10000 (13%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 42.207470
Train Epoch: 5 [12736/60000 (21%)]	Loss: 38.744415
Train Epoch: 5 [19136/60000 (32%)]	Loss: 38.280228
Train Epoch: 5 [25536/60000 (43%)]	Loss: 40.385723
Train Epoch: 5 [31936/60000 (53%)]	Loss: 42.233936
Train Epoch: 5 [38336/60000 (64%)]	Loss: 37.202766
Train Epoch: 5 [44736/60000 (75%)]	Loss: 42.943741
Train Epoch: 5 [51136/60000 (85%)]	Loss: 39.723515
Train Epoch: 5 [57536/60000 (96%)]	Loss: 44.337925

Test set: Average loss: 41.6588, Accuracy: 1340/10000 (13%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=5, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.0, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.00 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4496420
