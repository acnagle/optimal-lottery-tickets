Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=10, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='redlenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005) 

Training a RedLeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.321551
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.169610
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.637318
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.712404
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.864735
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.819310
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.776016
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.740030
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.868175

Test set: Average loss: 0.6299, Accuracy: 7892/10000 (79%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.454257
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.504194
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.821312
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.545718
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.790177
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.857865
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.614213
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.696815
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.641296

Test set: Average loss: 0.5226, Accuracy: 8363/10000 (84%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.606556
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.747895
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.542849
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.470831
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.878894
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.883974
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.745091
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.736372
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.639474

Test set: Average loss: 0.7932, Accuracy: 7387/10000 (74%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.931980
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.571580
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.765749
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.781252
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.548963
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.610699
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.741675
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.651652
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.184962

Test set: Average loss: 0.8107, Accuracy: 7348/10000 (73%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.541449
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.864006
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.772396
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.012551
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.923402
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.545690
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.659895
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.979622
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.756568

Test set: Average loss: 0.8470, Accuracy: 7283/10000 (73%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.708468
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.535523
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.978662
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.899945
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.025995
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.832861
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.253989
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.901955
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.952665

Test set: Average loss: 0.9657, Accuracy: 6926/10000 (69%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.749979
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.096049
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.896014
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.771658
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.784724
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.254335
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.900455
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.918420
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.957588

Test set: Average loss: 0.9163, Accuracy: 6893/10000 (69%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.770784
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.077097
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.856413
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.882000
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.909302
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.046745
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.876202
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.124370
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.770695

Test set: Average loss: 0.9995, Accuracy: 6972/10000 (70%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.685041
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.959377
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.908713
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.044636
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.774765
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.804827
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.996448
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.011033
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.991268

Test set: Average loss: 0.9000, Accuracy: 7126/10000 (71%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.577867
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.892189
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.837715
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.971241
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.844048
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.936137
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.994168
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.575480
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.791400

Test set: Average loss: 0.9862, Accuracy: 6705/10000 (67%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=10, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='redlenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.08 minutes
Total number of parameters in model: 300170
Number of parameters in pruned model: 294218
