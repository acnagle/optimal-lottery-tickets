Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.794131
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.097545
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.174879
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.264423
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.279152
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.400858
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.501635
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.769526
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.732135

Test set: Average loss: 1.6869, Accuracy: 4362/10000 (44%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.685082
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.808178
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.723911
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.972766
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.903249
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.851747
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.890629
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.217030
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.922808

Test set: Average loss: 2.0224, Accuracy: 3313/10000 (33%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.023767
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.247901
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.939204
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.171566
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.095820
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.968943
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.044785
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.128437
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.139755

Test set: Average loss: 2.1703, Accuracy: 2619/10000 (26%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.263839
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.038402
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.078967
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.010996
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.172807
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.191381
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.368173
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.257204
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.465112

Test set: Average loss: 2.2098, Accuracy: 2383/10000 (24%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.248659
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.278985
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.305713
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.288400
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.246706
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.338724
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.363317
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.057902
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.333774

Test set: Average loss: 2.2955, Accuracy: 2155/10000 (22%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.327538
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.262766
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.120399
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.287732
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.247962
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.274176
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.237772
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.266296
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.300203

Test set: Average loss: 2.3150, Accuracy: 2165/10000 (22%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.291442
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.366518
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.456388
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.260552
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.257891
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.364788
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.458251
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.168836
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.290826

Test set: Average loss: 2.3349, Accuracy: 2154/10000 (22%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.405302
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.401753
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.583352
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.387138
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.491962
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.378169
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.175053
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.337373
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.272082

Test set: Average loss: 2.3312, Accuracy: 2017/10000 (20%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.074212
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.160410
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.535023
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.507027
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.543228
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.492693
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.502059
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.610574
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.419045

Test set: Average loss: 2.3425, Accuracy: 2071/10000 (21%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.413242
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.303374
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.562833
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.250052
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.361615
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.548070
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.425663
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.397147
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.402441

Test set: Average loss: 2.3577, Accuracy: 2041/10000 (20%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.173477
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.470345
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.406002
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.510502
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.449096
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.415409
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.461442
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.264678
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.348405

Test set: Average loss: 2.3631, Accuracy: 2048/10000 (20%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.436230
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.370208
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.461855
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.000564
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.309617
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.266057
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.349132
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.555615
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.497925

Test set: Average loss: 2.3817, Accuracy: 1968/10000 (20%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.366466
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.367652
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.566331
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.563692
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.490666
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.300812
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.560935
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.410681
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.386427

Test set: Average loss: 2.3587, Accuracy: 1996/10000 (20%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.39 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1971438
