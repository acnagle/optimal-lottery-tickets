Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.436037
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.454252
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.342930
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.439145
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.410088
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.350460
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.460197
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.588626
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.498986

Test set: Average loss: 0.6032, Accuracy: 8047/10000 (80%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.512700
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.557183
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.624650
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.928028
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.790242
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.814449
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.767790
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.921946
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.830112

Test set: Average loss: 0.9961, Accuracy: 6929/10000 (69%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.938681
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.092965
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.962200
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.288619
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.238003
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.193407
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.342572
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.471440
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.458072

Test set: Average loss: 1.4826, Accuracy: 5123/10000 (51%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.600301
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.479789
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.432446
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.450611
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.584050
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.605502
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.759555
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.655041
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.906902

Test set: Average loss: 1.6957, Accuracy: 4439/10000 (44%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.724088
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.822968
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.746789
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.882270
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.737916
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.892153
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.854664
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.613513
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.875458

Test set: Average loss: 1.8287, Accuracy: 4030/10000 (40%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.895202
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.874025
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.704299
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.826197
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.809211
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.824994
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.776686
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.852404
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.852091

Test set: Average loss: 1.8981, Accuracy: 3930/10000 (39%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.810997
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.897562
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.995086
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.818668
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.861373
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.984653
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.044365
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.826169
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.910179

Test set: Average loss: 1.9406, Accuracy: 3830/10000 (38%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.031825
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.007121
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.134046
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.995420
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.131336
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.994086
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.761704
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.935283
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.925345

Test set: Average loss: 1.9779, Accuracy: 3311/10000 (33%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.658887
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.772208
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.102255
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.136884
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.130035
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.125852
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.084910
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.235103
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.082841

Test set: Average loss: 1.9778, Accuracy: 3460/10000 (35%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.077031
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.911701
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.156502
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.888800
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.938239
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.120142
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.103096
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.014865
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.036387

Test set: Average loss: 1.9713, Accuracy: 3606/10000 (36%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.848186
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.003447
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.036947
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.138263
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.133435
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.029953
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.064598
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.959816
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.020063

Test set: Average loss: 2.0258, Accuracy: 3448/10000 (34%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.085348
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.001750
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.034517
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.709190
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.955039
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.932760
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.001206
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.183727
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.155095

Test set: Average loss: 2.0071, Accuracy: 3421/10000 (34%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.002470
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.991686
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.190199
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.168735
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.136689
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.998605
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.176763
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.055053
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.049866

Test set: Average loss: 2.0174, Accuracy: 3188/10000 (32%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.40 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1951524
