Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.5, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.407745
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.351249
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.274962
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.288066
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.233345
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.135031
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.314829
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.300489
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.148695

Test set: Average loss: 0.1637, Accuracy: 9531/10000 (95%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.182723
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.120447
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.119178
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.204183
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.180079
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.099813
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.159704
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.104331
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.057677

Test set: Average loss: 0.1271, Accuracy: 9637/10000 (96%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.096310
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.052412
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.048992
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.074563
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.186033
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.064395
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.100745
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.318954
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.054690

Test set: Average loss: 0.1093, Accuracy: 9683/10000 (97%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.075983
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.185337
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.047985
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.094853
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.115623
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.107402
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.126328
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.078574
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.063561

Test set: Average loss: 0.1109, Accuracy: 9662/10000 (97%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.029427
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.022171
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.040341
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.115634
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.126180
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.095998
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.048662
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.025601
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.036411

Test set: Average loss: 0.1051, Accuracy: 9683/10000 (97%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.117060
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.033791
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.064212
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.120466
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.027288
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.096729
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.061477
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.110947
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.043056

Test set: Average loss: 0.0929, Accuracy: 9724/10000 (97%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.025486
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.043008
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.068731
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.048751
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.093151
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.119764
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.139874
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.023520
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.035964

Test set: Average loss: 0.0869, Accuracy: 9737/10000 (97%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.081729
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.042990
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.035669
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.089095
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.156557
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.061283
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.050573
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.028084
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.100849

Test set: Average loss: 0.0840, Accuracy: 9747/10000 (97%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.052972
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.020396
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.023281
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.040190
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.028401
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.025015
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.036090
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.056646
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.080532

Test set: Average loss: 0.0787, Accuracy: 9761/10000 (98%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.067502
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.026058
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.040265
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.039699
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.090342
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.020742
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.040159
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.021812
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.035946

Test set: Average loss: 0.0738, Accuracy: 9786/10000 (98%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.025550
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.008115
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.018973
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.019220
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.037209
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.045322
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.023679
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.012207
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.023275

Test set: Average loss: 0.0664, Accuracy: 9819/10000 (98%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.012667
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.027613
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.022152
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.010945
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.018447
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.046459
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.014745
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.038277
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.046689

Test set: Average loss: 0.0660, Accuracy: 9810/10000 (98%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.036472
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.013068
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.027651
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.043642
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.026686
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.059903
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.015059
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.050385
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.053981

Test set: Average loss: 0.0648, Accuracy: 9815/10000 (98%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 0.026924
Train Epoch: 14 [12736/60000 (21%)]	Loss: 0.023514
Train Epoch: 14 [19136/60000 (32%)]	Loss: 0.032097
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.020413
Train Epoch: 14 [31936/60000 (53%)]	Loss: 0.021226
Train Epoch: 14 [38336/60000 (64%)]	Loss: 0.013800
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.013733
Train Epoch: 14 [51136/60000 (85%)]	Loss: 0.062976
Train Epoch: 14 [57536/60000 (96%)]	Loss: 0.040196

Test set: Average loss: 0.0636, Accuracy: 9811/10000 (98%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.5, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.48 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 995676
