Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.675023
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.556639
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.438839
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.459859
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.406084
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.311373
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.412441
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.399826
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.326061

Test set: Average loss: 0.3124, Accuracy: 9192/10000 (92%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.356206
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.222890
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.302027
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.391436
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.402379
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.188597
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.218094
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.202877
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.233337

Test set: Average loss: 0.2677, Accuracy: 9252/10000 (93%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.212971
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.146800
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.194029
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.234667
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.287264
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.157966
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.265547
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.435749
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.151161

Test set: Average loss: 0.2384, Accuracy: 9343/10000 (93%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.279084
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.299523
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.278284
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.254780
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.275946
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.269026
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.212305
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.218907
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.211137

Test set: Average loss: 0.2233, Accuracy: 9394/10000 (94%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.118554
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.122048
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.150710
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.226237
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.253073
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.413818
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.193527
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.146252
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.171457

Test set: Average loss: 0.2105, Accuracy: 9447/10000 (94%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.248644
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.197496
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.209706
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.254886
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.211033
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.192303
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.149470
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.219628
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.162202

Test set: Average loss: 0.2070, Accuracy: 9418/10000 (94%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.129264
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.143294
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.239228
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.140282
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.268523
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.199789
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.217348
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.141485
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.201999

Test set: Average loss: 0.1951, Accuracy: 9468/10000 (95%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.390479
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.192964
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.124975
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.233057
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.278177
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.132543
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.155049
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.170000
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.275690

Test set: Average loss: 0.1910, Accuracy: 9491/10000 (95%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.151212
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.122849
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.141549
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.116847
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.184867
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.149845
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.185551
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.250458
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.328762

Test set: Average loss: 0.1898, Accuracy: 9467/10000 (95%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.170636
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.157347
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.140060
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.158203
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.255779
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.123203
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.245044
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.112649
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.212702

Test set: Average loss: 0.1860, Accuracy: 9506/10000 (95%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.155015
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.103280
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.106614
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.139394
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.195333
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.181189
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.110829
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.104972
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.147892

Test set: Average loss: 0.1840, Accuracy: 9502/10000 (95%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.147772
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.166476
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.235477
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.103411
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.182814
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.255804
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.138541
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.215975
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.219689

Test set: Average loss: 0.1849, Accuracy: 9514/10000 (95%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.217185
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.109094
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.171098
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.221091
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.200108
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.374328
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.097502
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.257704
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.233440

Test set: Average loss: 0.1841, Accuracy: 9507/10000 (95%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 0.220568
Train Epoch: 14 [12736/60000 (21%)]	Loss: 0.189251
Train Epoch: 14 [19136/60000 (32%)]	Loss: 0.182007
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.189611
Train Epoch: 14 [31936/60000 (53%)]	Loss: 0.212722
Train Epoch: 14 [38336/60000 (64%)]	Loss: 0.147700
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.136394
Train Epoch: 14 [51136/60000 (85%)]	Loss: 0.178464
Train Epoch: 14 [57536/60000 (96%)]	Loss: 0.198115

Test set: Average loss: 0.1829, Accuracy: 9521/10000 (95%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.46 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 99567
