Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.460646
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.325525
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.622307
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.544986
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.545340
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.554977
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.576526
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.543963
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.877355

Test set: Average loss: 0.6750, Accuracy: 8073/10000 (81%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.481197
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.907064
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.842227
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.806994
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.956280
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.874428
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.076021
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.079294
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.085590

Test set: Average loss: 1.1771, Accuracy: 6270/10000 (63%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.187029
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.211581
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.109221
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.390902
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.279773
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.671209
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.303981
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.560125
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.615031

Test set: Average loss: 1.4698, Accuracy: 5508/10000 (55%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.729424
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.567733
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.674964
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.728938
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.566938
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.712216
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.692990
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.674195
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.783400

Test set: Average loss: 1.7449, Accuracy: 4234/10000 (42%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.834948
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.649038
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.887208
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.687149
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.712901
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.718752
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.822786
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.896288
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.071207

Test set: Average loss: 1.8880, Accuracy: 3668/10000 (37%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.835055
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.038655
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.130252
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.001636
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.019691
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.096204
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.048550
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.921624
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.832902

Test set: Average loss: 1.9846, Accuracy: 3282/10000 (33%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.121381
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.010116
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.127266
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.122924
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.047358
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.220479
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.139816
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.165545
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.989457

Test set: Average loss: 2.0709, Accuracy: 2731/10000 (27%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.960564
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.096630
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.101661
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.914883
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.069787
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.979790
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.922535
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.205836
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.137682

Test set: Average loss: 2.0706, Accuracy: 2814/10000 (28%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.987920
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.008085
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.038429
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.919952
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.067170
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.971845
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.054489
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.171321
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.289909

Test set: Average loss: 2.1028, Accuracy: 2822/10000 (28%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.262382
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.033803
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.096798
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.267989
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.057303
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.128268
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.218120
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.183400
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.315162

Test set: Average loss: 2.1236, Accuracy: 2690/10000 (27%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.929143
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.117869
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.104014
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.230311
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.124629
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.899073
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.031473
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.287942
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.168710

Test set: Average loss: 2.1197, Accuracy: 2698/10000 (27%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.205401
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.155074
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.115933
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.348542
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.209701
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.130040
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.095735
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.300644
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.159714

Test set: Average loss: 2.1636, Accuracy: 2565/10000 (26%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.133911
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.038205
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.238982
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.213959
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.263997
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.430074
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.128531
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.048131
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.298788

Test set: Average loss: 2.1702, Accuracy: 2474/10000 (25%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 2.234167
Train Epoch: 14 [12736/60000 (21%)]	Loss: 2.080839
Train Epoch: 14 [19136/60000 (32%)]	Loss: 2.216199
Train Epoch: 14 [25536/60000 (43%)]	Loss: 2.321896
Train Epoch: 14 [31936/60000 (53%)]	Loss: 2.163429
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.172554
Train Epoch: 14 [44736/60000 (75%)]	Loss: 2.144739
Train Epoch: 14 [51136/60000 (85%)]	Loss: 2.118353
Train Epoch: 14 [57536/60000 (96%)]	Loss: 2.009167

Test set: Average loss: 2.1465, Accuracy: 2499/10000 (25%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.49 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 4451542
