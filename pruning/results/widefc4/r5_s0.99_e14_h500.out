Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.99, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.302414
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.302206
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.301864
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.301445
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.299501
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.296583
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.279047
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.187970
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.938409

Test set: Average loss: 1.6486, Accuracy: 4805/10000 (48%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.355477
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.324506
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.269299
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.068883
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.990323
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.849603
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.973743
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.909816
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.721813

Test set: Average loss: 0.7411, Accuracy: 8233/10000 (82%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.875957
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.674463
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.728352
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.901321
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.670499
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.664518
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.702194
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.698790
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.751592

Test set: Average loss: 0.6483, Accuracy: 8493/10000 (85%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.724125
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.648234
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.666753
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.649894
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.757666
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.581340
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.736450
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.664212
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.599077

Test set: Average loss: 0.6171, Accuracy: 8532/10000 (85%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.617006
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.558721
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.488933
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.461768
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.751464
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.650694
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.719726
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.685369
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.792901

Test set: Average loss: 0.6169, Accuracy: 8490/10000 (85%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.532695
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.709823
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.612594
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.534857
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.564312
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.726081
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.666388
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.504628
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.693322

Test set: Average loss: 0.5917, Accuracy: 8622/10000 (86%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.591217
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.602760
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.600279
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.628128
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.648613
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.631290
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.588017
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.705117
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.678942

Test set: Average loss: 0.5871, Accuracy: 8644/10000 (86%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.577373
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.583703
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.694881
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.723307
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.745850
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.658163
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.650336
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.618224
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.666278

Test set: Average loss: 0.5813, Accuracy: 8661/10000 (87%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.669306
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.602849
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.499893
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.494819
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.758912
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.642152
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.561028
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.631843
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.731538

Test set: Average loss: 0.5779, Accuracy: 8667/10000 (87%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.581175
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.672048
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.460479
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.603440
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.725253
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.641712
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.581748
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.555237
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.594083

Test set: Average loss: 0.5710, Accuracy: 8660/10000 (87%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.455627
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.592899
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.636885
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.492580
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.567381
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.619354
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.604834
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.615270
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.638862

Test set: Average loss: 0.5701, Accuracy: 8680/10000 (87%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.678140
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.578678
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.459982
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.542844
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.662170
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.586112
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.540685
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.571226
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.592017

Test set: Average loss: 0.5703, Accuracy: 8661/10000 (87%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.516587
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.430114
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.592369
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.568305
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.616505
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.651763
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.505503
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.497345
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.580840

Test set: Average loss: 0.5697, Accuracy: 8653/10000 (87%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 0.640676
Train Epoch: 14 [12736/60000 (21%)]	Loss: 0.757250
Train Epoch: 14 [19136/60000 (32%)]	Loss: 0.584557
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.640915
Train Epoch: 14 [31936/60000 (53%)]	Loss: 0.487027
Train Epoch: 14 [38336/60000 (64%)]	Loss: 0.556951
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.537654
Train Epoch: 14 [51136/60000 (85%)]	Loss: 0.539773
Train Epoch: 14 [57536/60000 (96%)]	Loss: 0.476871

Test set: Average loss: 0.5699, Accuracy: 8669/10000 (87%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.99, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.48 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 44965
