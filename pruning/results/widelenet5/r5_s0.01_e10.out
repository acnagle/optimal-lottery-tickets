Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=10, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.010781
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.832156
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.755229
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.777992
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.848336
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.913699
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.866158
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.922926
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.941242

Test set: Average loss: 1.9726, Accuracy: 4485/10000 (45%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.026225
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.019773
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.974993
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.061782
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.087954
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.974753
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.057816
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.038198
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.984154

Test set: Average loss: 2.1002, Accuracy: 3397/10000 (34%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.115837
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.219807
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.115389
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.109568
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.111622
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.097492
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.209947
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.202350
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.199320

Test set: Average loss: 2.1722, Accuracy: 2602/10000 (26%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.081506
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.230866
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.288913
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.100412
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.266105
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.100764
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.234582
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.283062
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.170757

Test set: Average loss: 2.2019, Accuracy: 2599/10000 (26%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.178920
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.135226
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.144065
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.116608
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.131697
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.275279
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.318210
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.295460
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.265670

Test set: Average loss: 2.2461, Accuracy: 1749/10000 (17%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.223735
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.216995
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.311359
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.245396
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.204887
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.221700
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.173286
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.214000
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.159404

Test set: Average loss: 2.2479, Accuracy: 2031/10000 (20%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.293560
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.250109
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.252083
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.251375
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.299122
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.230545
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.343953
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.278655
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.183071

Test set: Average loss: 2.2489, Accuracy: 1713/10000 (17%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.128333
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.229488
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.176264
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.284523
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.249243
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.275191
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.312161
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.201231
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.164072

Test set: Average loss: 2.2544, Accuracy: 1998/10000 (20%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.288712
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.201001
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.178714
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.354720
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.296503
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.209423
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.292946
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.301245
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.329492

Test set: Average loss: 2.2502, Accuracy: 1445/10000 (14%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.289788
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.245387
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.211481
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.278018
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.307353
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.273861
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.246912
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.269246
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.300217

Test set: Average loss: 2.2642, Accuracy: 2059/10000 (21%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=10, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.07 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 297436
