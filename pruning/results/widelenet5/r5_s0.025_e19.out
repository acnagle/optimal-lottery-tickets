Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=19, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.913022
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.648698
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.386873
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.195027
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.141619
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.929965
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.932596
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.918575
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.957327

Test set: Average loss: 0.9901, Accuracy: 8182/10000 (82%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.123650
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.006861
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.126619
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.231596
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.261699
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.194958
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.386552
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.216965
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.211138

Test set: Average loss: 1.4518, Accuracy: 5763/10000 (58%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.466130
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.480780
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.441118
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.515013
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.282839
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.536205
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.513468
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.473780
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.579393

Test set: Average loss: 1.5088, Accuracy: 6133/10000 (61%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.488813
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.637403
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.602441
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.506595
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.688938
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.564831
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.621830
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.864132
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.661684

Test set: Average loss: 1.6510, Accuracy: 5284/10000 (53%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.583598
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.586674
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.642888
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.584613
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.634110
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.772938
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.780880
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.930046
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.790317

Test set: Average loss: 1.7167, Accuracy: 5820/10000 (58%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.744951
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.753951
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.829905
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.755286
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.695036
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.851713
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.735813
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.697067
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.675767

Test set: Average loss: 1.8466, Accuracy: 3712/10000 (37%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.843748
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.838355
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.830742
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.829602
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.864388
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.805128
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.943787
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.840465
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.820051

Test set: Average loss: 1.8240, Accuracy: 5405/10000 (54%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.715735
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.859333
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.807137
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.896260
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.804166
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.853341
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.936948
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.814218
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.832995

Test set: Average loss: 1.8766, Accuracy: 4931/10000 (49%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.909821
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.943367
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.852713
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.949624
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.955319
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.879834
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.922993
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.932788
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.995258

Test set: Average loss: 1.8931, Accuracy: 4970/10000 (50%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.936561
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.948164
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.923416
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.978544
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.960382
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.883463
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.836594
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.937049
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.008952

Test set: Average loss: 1.9637, Accuracy: 4465/10000 (45%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.827777
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.974169
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.978026
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.961489
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.975708
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.994234
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.973655
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.979264
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.932144

Test set: Average loss: 1.9537, Accuracy: 4110/10000 (41%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.967321
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.971035
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.998691
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.990366
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.962723
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.044473
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.952605
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.014281
Train Epoch: 12 [57536/60000 (96%)]	Loss: 1.998916

Test set: Average loss: 1.9752, Accuracy: 3987/10000 (40%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.985060
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.962410
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.832291
Train Epoch: 13 [25536/60000 (43%)]	Loss: 1.895198
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.058329
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.956699
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.015144
Train Epoch: 13 [51136/60000 (85%)]	Loss: 1.973692
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.076230

Test set: Average loss: 1.9992, Accuracy: 4083/10000 (41%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 1.977349
Train Epoch: 14 [12736/60000 (21%)]	Loss: 1.914240
Train Epoch: 14 [19136/60000 (32%)]	Loss: 1.958847
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.818446
Train Epoch: 14 [31936/60000 (53%)]	Loss: 1.961300
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.033808
Train Epoch: 14 [44736/60000 (75%)]	Loss: 1.930801
Train Epoch: 14 [51136/60000 (85%)]	Loss: 1.963375
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.856925

Test set: Average loss: 1.9875, Accuracy: 4249/10000 (42%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 1.950284
Train Epoch: 15 [12736/60000 (21%)]	Loss: 2.067080
Train Epoch: 15 [19136/60000 (32%)]	Loss: 2.038975
Train Epoch: 15 [25536/60000 (43%)]	Loss: 1.942213
Train Epoch: 15 [31936/60000 (53%)]	Loss: 2.052630
Train Epoch: 15 [38336/60000 (64%)]	Loss: 1.938133
Train Epoch: 15 [44736/60000 (75%)]	Loss: 2.003552
Train Epoch: 15 [51136/60000 (85%)]	Loss: 2.047122
Train Epoch: 15 [57536/60000 (96%)]	Loss: 2.004004

Test set: Average loss: 1.9909, Accuracy: 4220/10000 (42%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 2.063046
Train Epoch: 16 [12736/60000 (21%)]	Loss: 2.039773
Train Epoch: 16 [19136/60000 (32%)]	Loss: 2.046646
Train Epoch: 16 [25536/60000 (43%)]	Loss: 1.944004
Train Epoch: 16 [31936/60000 (53%)]	Loss: 2.076767
Train Epoch: 16 [38336/60000 (64%)]	Loss: 1.956005
Train Epoch: 16 [44736/60000 (75%)]	Loss: 1.968953
Train Epoch: 16 [51136/60000 (85%)]	Loss: 1.965406
Train Epoch: 16 [57536/60000 (96%)]	Loss: 2.042011

Test set: Average loss: 1.9964, Accuracy: 4553/10000 (46%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 2.050058
Train Epoch: 17 [12736/60000 (21%)]	Loss: 1.994481
Train Epoch: 17 [19136/60000 (32%)]	Loss: 1.927123
Train Epoch: 17 [25536/60000 (43%)]	Loss: 1.864534
Train Epoch: 17 [31936/60000 (53%)]	Loss: 2.094330
Train Epoch: 17 [38336/60000 (64%)]	Loss: 2.001961
Train Epoch: 17 [44736/60000 (75%)]	Loss: 2.038574
Train Epoch: 17 [51136/60000 (85%)]	Loss: 2.004227
Train Epoch: 17 [57536/60000 (96%)]	Loss: 1.990092

Test set: Average loss: 1.9965, Accuracy: 4405/10000 (44%)

Train Epoch: 18 [6336/60000 (11%)]	Loss: 1.937947
Train Epoch: 18 [12736/60000 (21%)]	Loss: 2.069728
Train Epoch: 18 [19136/60000 (32%)]	Loss: 1.974435
Train Epoch: 18 [25536/60000 (43%)]	Loss: 1.971482
Train Epoch: 18 [31936/60000 (53%)]	Loss: 1.949999
Train Epoch: 18 [38336/60000 (64%)]	Loss: 1.998323
Train Epoch: 18 [44736/60000 (75%)]	Loss: 2.029226
Train Epoch: 18 [51136/60000 (85%)]	Loss: 1.954761
Train Epoch: 18 [57536/60000 (96%)]	Loss: 2.036547

Test set: Average loss: 2.0083, Accuracy: 3994/10000 (40%)

Train Epoch: 19 [6336/60000 (11%)]	Loss: 1.997283
Train Epoch: 19 [12736/60000 (21%)]	Loss: 2.086522
Train Epoch: 19 [19136/60000 (32%)]	Loss: 1.899113
Train Epoch: 19 [25536/60000 (43%)]	Loss: 1.969298
Train Epoch: 19 [31936/60000 (53%)]	Loss: 1.990047
Train Epoch: 19 [38336/60000 (64%)]	Loss: 2.037050
Train Epoch: 19 [44736/60000 (75%)]	Loss: 1.964117
Train Epoch: 19 [51136/60000 (85%)]	Loss: 2.012961
Train Epoch: 19 [57536/60000 (96%)]	Loss: 1.972290

Test set: Average loss: 2.0060, Accuracy: 4404/10000 (44%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=19, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.06 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 292968
