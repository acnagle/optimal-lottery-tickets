Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.929791
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.686202
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.429822
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.234510
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.212617
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.099264
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.142983
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.243952
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.275224

Test set: Average loss: 1.3239, Accuracy: 6756/10000 (68%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.460355
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.360502
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.425839
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.470303
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.578918
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.437932
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.591398
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.498582
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.458067

Test set: Average loss: 1.6083, Accuracy: 5772/10000 (58%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.678022
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.752724
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.665107
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.721727
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.594435
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.735142
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.732692
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.739181
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.801957

Test set: Average loss: 1.7326, Accuracy: 5970/10000 (60%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.727696
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.776602
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.854804
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.731420
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.904903
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.763094
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.832959
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.935022
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.837595

Test set: Average loss: 1.8399, Accuracy: 4953/10000 (50%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.788518
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.793170
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.826428
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.797643
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.763680
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.939975
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.996578
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.076785
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.934367

Test set: Average loss: 1.8993, Accuracy: 5110/10000 (51%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.911006
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.949158
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.003368
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.927832
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.899828
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.958300
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.906676
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.897584
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.816404

Test set: Average loss: 1.9840, Accuracy: 3635/10000 (36%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.011763
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.961017
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.995200
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.988689
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.032742
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.949066
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.100019
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.992257
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.943609

Test set: Average loss: 1.9623, Accuracy: 4731/10000 (47%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.873153
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.963282
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.898753
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.048507
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.923818
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.998428
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.062711
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.965911
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.945394

Test set: Average loss: 2.0088, Accuracy: 4183/10000 (42%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.047940
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.012168
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.972592
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.071713
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.086581
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.042728
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.064948
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.072468
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.130616

Test set: Average loss: 2.0263, Accuracy: 4227/10000 (42%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.052865
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.071653
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.016580
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.062337
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.090916
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.006878
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.009330
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.042820
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.092734

Test set: Average loss: 2.0604, Accuracy: 3904/10000 (39%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.975255
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.142620
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.102944
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.074027
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.073120
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.085897
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.060808
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.057210
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.068889

Test set: Average loss: 2.0447, Accuracy: 3980/10000 (40%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.085941
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.095296
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.093859
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.100679
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.089072
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.135988
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.070849
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.099495
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.108178

Test set: Average loss: 2.0771, Accuracy: 3547/10000 (35%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.120943
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.069736
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.969248
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.003173
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.153079
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.046757
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.109314
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.089343
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.171168

Test set: Average loss: 2.0938, Accuracy: 3192/10000 (32%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 2.099643
Train Epoch: 14 [12736/60000 (21%)]	Loss: 2.020870
Train Epoch: 14 [19136/60000 (32%)]	Loss: 2.047197
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.971634
Train Epoch: 14 [31936/60000 (53%)]	Loss: 2.045694
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.110012
Train Epoch: 14 [44736/60000 (75%)]	Loss: 2.035218
Train Epoch: 14 [51136/60000 (85%)]	Loss: 2.055493
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.975418

Test set: Average loss: 2.1000, Accuracy: 3414/10000 (34%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 2.054877
Train Epoch: 15 [12736/60000 (21%)]	Loss: 2.170424
Train Epoch: 15 [19136/60000 (32%)]	Loss: 2.120779
Train Epoch: 15 [25536/60000 (43%)]	Loss: 2.043613
Train Epoch: 15 [31936/60000 (53%)]	Loss: 2.158519
Train Epoch: 15 [38336/60000 (64%)]	Loss: 2.034616
Train Epoch: 15 [44736/60000 (75%)]	Loss: 2.103809
Train Epoch: 15 [51136/60000 (85%)]	Loss: 2.147343
Train Epoch: 15 [57536/60000 (96%)]	Loss: 2.092957

Test set: Average loss: 2.1009, Accuracy: 3728/10000 (37%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 2.169423
Train Epoch: 16 [12736/60000 (21%)]	Loss: 2.131682
Train Epoch: 16 [19136/60000 (32%)]	Loss: 2.154665
Train Epoch: 16 [25536/60000 (43%)]	Loss: 2.063488
Train Epoch: 16 [31936/60000 (53%)]	Loss: 2.155720
Train Epoch: 16 [38336/60000 (64%)]	Loss: 2.045543
Train Epoch: 16 [44736/60000 (75%)]	Loss: 2.071371
Train Epoch: 16 [51136/60000 (85%)]	Loss: 2.046673
Train Epoch: 16 [57536/60000 (96%)]	Loss: 2.134279

Test set: Average loss: 2.0908, Accuracy: 3966/10000 (40%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 2.164356
Train Epoch: 17 [12736/60000 (21%)]	Loss: 2.061901
Train Epoch: 17 [19136/60000 (32%)]	Loss: 2.044810
Train Epoch: 17 [25536/60000 (43%)]	Loss: 1.941577
Train Epoch: 17 [31936/60000 (53%)]	Loss: 2.165995
Train Epoch: 17 [38336/60000 (64%)]	Loss: 2.091024
Train Epoch: 17 [44736/60000 (75%)]	Loss: 2.115583
Train Epoch: 17 [51136/60000 (85%)]	Loss: 2.081513
Train Epoch: 17 [57536/60000 (96%)]	Loss: 2.076181

Test set: Average loss: 2.0935, Accuracy: 3680/10000 (37%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.81 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 294457
