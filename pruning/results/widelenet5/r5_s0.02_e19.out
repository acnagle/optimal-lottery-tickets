Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=19, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.929791
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.686202
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.429822
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.234510
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.212617
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.099264
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.142983
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.243952
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.275224

Test set: Average loss: 1.3239, Accuracy: 6756/10000 (68%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.435214
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.364948
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.401504
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.537774
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.608939
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.467329
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.634784
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.452332
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.454483

Test set: Average loss: 1.5927, Accuracy: 5795/10000 (58%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.629440
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.711280
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.686584
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.705053
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.602865
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.690393
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.784413
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.724222
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.784462

Test set: Average loss: 1.7445, Accuracy: 5639/10000 (56%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.740352
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.820948
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.837203
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.714253
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.881016
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.760113
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.834660
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.927503
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.829157

Test set: Average loss: 1.8386, Accuracy: 5193/10000 (52%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.793444
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.819352
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.854656
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.789720
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.815538
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.946790
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.953678
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.054064
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.946236

Test set: Average loss: 1.9089, Accuracy: 5143/10000 (51%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.908281
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.952010
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.993880
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.936499
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.890833
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.974680
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.887892
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.909649
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.819444

Test set: Average loss: 2.0007, Accuracy: 3423/10000 (34%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.010816
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.984130
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.000671
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.961115
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.029508
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.956993
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.076717
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.011862
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.979302

Test set: Average loss: 1.9816, Accuracy: 4626/10000 (46%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.876449
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.997880
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.907559
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.058337
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.957706
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.013057
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.083555
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.970579
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.956227

Test set: Average loss: 2.0253, Accuracy: 3894/10000 (39%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.057440
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.044642
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.989323
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.107862
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.069009
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.035122
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.052400
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.083392
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.130086

Test set: Average loss: 2.0428, Accuracy: 3666/10000 (37%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.068758
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.051899
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.023387
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.090278
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.094496
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.007176
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.029041
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.069729
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.108581

Test set: Average loss: 2.0884, Accuracy: 3750/10000 (38%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.990805
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.179239
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.121547
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.106703
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.078880
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.105475
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.056461
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.093587
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.072091

Test set: Average loss: 2.0769, Accuracy: 3718/10000 (37%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.063276
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.106407
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.104908
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.108259
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.110852
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.157461
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.099135
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.125583
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.090716

Test set: Average loss: 2.0953, Accuracy: 2939/10000 (29%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.115091
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.100260
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.028835
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.014041
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.184235
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.066438
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.113763
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.094413
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.189143

Test set: Average loss: 2.0946, Accuracy: 3393/10000 (34%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 2.108634
Train Epoch: 14 [12736/60000 (21%)]	Loss: 2.065117
Train Epoch: 14 [19136/60000 (32%)]	Loss: 2.090292
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.991327
Train Epoch: 14 [31936/60000 (53%)]	Loss: 2.065225
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.136445
Train Epoch: 14 [44736/60000 (75%)]	Loss: 2.056798
Train Epoch: 14 [51136/60000 (85%)]	Loss: 2.058029
Train Epoch: 14 [57536/60000 (96%)]	Loss: 2.006510

Test set: Average loss: 2.1317, Accuracy: 2778/10000 (28%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 2.049939
Train Epoch: 15 [12736/60000 (21%)]	Loss: 2.214513
Train Epoch: 15 [19136/60000 (32%)]	Loss: 2.157872
Train Epoch: 15 [25536/60000 (43%)]	Loss: 2.069376
Train Epoch: 15 [31936/60000 (53%)]	Loss: 2.155923
Train Epoch: 15 [38336/60000 (64%)]	Loss: 2.057225
Train Epoch: 15 [44736/60000 (75%)]	Loss: 2.091692
Train Epoch: 15 [51136/60000 (85%)]	Loss: 2.168625
Train Epoch: 15 [57536/60000 (96%)]	Loss: 2.097976

Test set: Average loss: 2.1167, Accuracy: 3531/10000 (35%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 2.202076
Train Epoch: 16 [12736/60000 (21%)]	Loss: 2.146609
Train Epoch: 16 [19136/60000 (32%)]	Loss: 2.166743
Train Epoch: 16 [25536/60000 (43%)]	Loss: 2.097682
Train Epoch: 16 [31936/60000 (53%)]	Loss: 2.178445
Train Epoch: 16 [38336/60000 (64%)]	Loss: 2.068156
Train Epoch: 16 [44736/60000 (75%)]	Loss: 2.102677
Train Epoch: 16 [51136/60000 (85%)]	Loss: 2.115004
Train Epoch: 16 [57536/60000 (96%)]	Loss: 2.153322

Test set: Average loss: 2.1156, Accuracy: 3888/10000 (39%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 2.176603
Train Epoch: 17 [12736/60000 (21%)]	Loss: 2.095839
Train Epoch: 17 [19136/60000 (32%)]	Loss: 2.051543
Train Epoch: 17 [25536/60000 (43%)]	Loss: 1.964718
Train Epoch: 17 [31936/60000 (53%)]	Loss: 2.195248
Train Epoch: 17 [38336/60000 (64%)]	Loss: 2.130251
Train Epoch: 17 [44736/60000 (75%)]	Loss: 2.145074
Train Epoch: 17 [51136/60000 (85%)]	Loss: 2.088411
Train Epoch: 17 [57536/60000 (96%)]	Loss: 2.095635

Test set: Average loss: 2.1188, Accuracy: 3472/10000 (35%)

Train Epoch: 18 [6336/60000 (11%)]	Loss: 2.045608
Train Epoch: 18 [12736/60000 (21%)]	Loss: 2.208627
Train Epoch: 18 [19136/60000 (32%)]	Loss: 2.131490
Train Epoch: 18 [25536/60000 (43%)]	Loss: 2.103152
Train Epoch: 18 [31936/60000 (53%)]	Loss: 2.054385
Train Epoch: 18 [38336/60000 (64%)]	Loss: 2.144099
Train Epoch: 18 [44736/60000 (75%)]	Loss: 2.152884
Train Epoch: 18 [51136/60000 (85%)]	Loss: 2.077795
Train Epoch: 18 [57536/60000 (96%)]	Loss: 2.141048

Test set: Average loss: 2.1160, Accuracy: 3281/10000 (33%)

Train Epoch: 19 [6336/60000 (11%)]	Loss: 2.154305
Train Epoch: 19 [12736/60000 (21%)]	Loss: 2.232813
Train Epoch: 19 [19136/60000 (32%)]	Loss: 2.032322
Train Epoch: 19 [25536/60000 (43%)]	Loss: 2.059376
Train Epoch: 19 [31936/60000 (53%)]	Loss: 2.138141
Train Epoch: 19 [38336/60000 (64%)]	Loss: 2.142822
Train Epoch: 19 [44736/60000 (75%)]	Loss: 2.087402
Train Epoch: 19 [51136/60000 (85%)]	Loss: 2.143583
Train Epoch: 19 [57536/60000 (96%)]	Loss: 2.113926

Test set: Average loss: 2.1159, Accuracy: 3716/10000 (37%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=19, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.07 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 294457
