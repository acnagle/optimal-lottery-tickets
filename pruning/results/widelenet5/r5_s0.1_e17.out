Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.1, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.772954
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.453424
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.231643
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.976652
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.934958
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.726931
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.742867
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.683650
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.590871

Test set: Average loss: 0.6860, Accuracy: 8604/10000 (86%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.618640
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.526924
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.612966
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.624217
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.601069
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.511520
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.643910
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.407989
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.439265

Test set: Average loss: 0.4747, Accuracy: 8952/10000 (90%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.396595
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.450933
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.403572
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.440050
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.263244
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.419036
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.381921
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.325924
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.492239

Test set: Average loss: 0.4015, Accuracy: 9088/10000 (91%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.458333
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.368941
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.371920
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.307800
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.452916
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.367998
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.364420
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.426934
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.309231

Test set: Average loss: 0.3509, Accuracy: 9170/10000 (92%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.254143
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.314486
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.383663
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.241482
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.327268
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.292923
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.326245
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.328152
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.288287

Test set: Average loss: 0.3143, Accuracy: 9218/10000 (92%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.263591
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.389450
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.276589
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.337371
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.219359
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.460916
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.272526
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.288849
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.298556

Test set: Average loss: 0.2923, Accuracy: 9311/10000 (93%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.264866
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.368074
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.254795
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.499716
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.286204
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.220981
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.366286
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.206550
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.314949

Test set: Average loss: 0.2742, Accuracy: 9335/10000 (93%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.259020
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.334990
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.270461
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.338824
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.218480
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.245322
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.383635
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.277676
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.239571

Test set: Average loss: 0.2671, Accuracy: 9343/10000 (93%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.310422
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.393079
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.303833
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.328154
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.175379
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.366469
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.207519
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.288320
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.315538

Test set: Average loss: 0.2547, Accuracy: 9371/10000 (94%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.331068
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.274700
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.264333
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.265451
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.345321
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.239988
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.179148
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.232781
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.245585

Test set: Average loss: 0.2457, Accuracy: 9379/10000 (94%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.211931
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.196057
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.258580
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.296999
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.273657
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.195055
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.348010
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.301193
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.264337

Test set: Average loss: 0.2401, Accuracy: 9415/10000 (94%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.275571
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.192414
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.223116
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.210541
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.227372
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.262266
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.265836
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.298377
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.271992

Test set: Average loss: 0.2360, Accuracy: 9416/10000 (94%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.286076
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.206366
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.138979
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.202378
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.243591
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.216200
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.344580
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.189446
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.270394

Test set: Average loss: 0.2357, Accuracy: 9417/10000 (94%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 0.233496
Train Epoch: 14 [12736/60000 (21%)]	Loss: 0.311899
Train Epoch: 14 [19136/60000 (32%)]	Loss: 0.267380
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.280866
Train Epoch: 14 [31936/60000 (53%)]	Loss: 0.317299
Train Epoch: 14 [38336/60000 (64%)]	Loss: 0.230651
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.137900
Train Epoch: 14 [51136/60000 (85%)]	Loss: 0.251433
Train Epoch: 14 [57536/60000 (96%)]	Loss: 0.286251

Test set: Average loss: 0.2336, Accuracy: 9407/10000 (94%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 0.231939
Train Epoch: 15 [12736/60000 (21%)]	Loss: 0.214482
Train Epoch: 15 [19136/60000 (32%)]	Loss: 0.260240
Train Epoch: 15 [25536/60000 (43%)]	Loss: 0.282746
Train Epoch: 15 [31936/60000 (53%)]	Loss: 0.177911
Train Epoch: 15 [38336/60000 (64%)]	Loss: 0.266454
Train Epoch: 15 [44736/60000 (75%)]	Loss: 0.231003
Train Epoch: 15 [51136/60000 (85%)]	Loss: 0.200490
Train Epoch: 15 [57536/60000 (96%)]	Loss: 0.370093

Test set: Average loss: 0.2289, Accuracy: 9440/10000 (94%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 0.271655
Train Epoch: 16 [12736/60000 (21%)]	Loss: 0.251914
Train Epoch: 16 [19136/60000 (32%)]	Loss: 0.303531
Train Epoch: 16 [25536/60000 (43%)]	Loss: 0.113568
Train Epoch: 16 [31936/60000 (53%)]	Loss: 0.395122
Train Epoch: 16 [38336/60000 (64%)]	Loss: 0.143071
Train Epoch: 16 [44736/60000 (75%)]	Loss: 0.170645
Train Epoch: 16 [51136/60000 (85%)]	Loss: 0.189111
Train Epoch: 16 [57536/60000 (96%)]	Loss: 0.183765

Test set: Average loss: 0.2336, Accuracy: 9409/10000 (94%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 0.218736
Train Epoch: 17 [12736/60000 (21%)]	Loss: 0.201868
Train Epoch: 17 [19136/60000 (32%)]	Loss: 0.296969
Train Epoch: 17 [25536/60000 (43%)]	Loss: 0.151314
Train Epoch: 17 [31936/60000 (53%)]	Loss: 0.207502
Train Epoch: 17 [38336/60000 (64%)]	Loss: 0.250527
Train Epoch: 17 [44736/60000 (75%)]	Loss: 0.162620
Train Epoch: 17 [51136/60000 (85%)]	Loss: 0.397658
Train Epoch: 17 [57536/60000 (96%)]	Loss: 0.171491

Test set: Average loss: 0.2294, Accuracy: 9422/10000 (94%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.1, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.82 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 270628
