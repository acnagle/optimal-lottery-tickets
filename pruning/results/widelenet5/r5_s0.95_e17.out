Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.298577
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.299106
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.298120
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.288540
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.285546
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.280891
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.271248
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.263660
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.253363

Test set: Average loss: 2.2621, Accuracy: 3350/10000 (34%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.256756
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.235855
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.234073
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.219019
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.216949
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.149388
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.167849
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.091829
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.039011

Test set: Average loss: 2.0957, Accuracy: 3929/10000 (39%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.040728
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.023222
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.989279
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.006129
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.869453
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.954129
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.881228
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.820368
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.772923

Test set: Average loss: 1.7898, Accuracy: 4836/10000 (48%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.724335
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.671921
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.752136
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.626117
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.671976
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.650515
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.571411
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.597936
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.535667

Test set: Average loss: 1.5378, Accuracy: 6006/10000 (60%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.482252
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.488758
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.578611
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.362808
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.392045
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.552405
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.499639
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.484677
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.437675

Test set: Average loss: 1.3772, Accuracy: 6297/10000 (63%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.332112
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.443729
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.360140
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.282583
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.249430
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.440019
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.277920
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.181798
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.256956

Test set: Average loss: 1.2772, Accuracy: 6750/10000 (68%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.254453
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.313244
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.333215
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.258744
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.227395
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.316013
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.326938
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.250784
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.256386

Test set: Average loss: 1.2163, Accuracy: 6989/10000 (70%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.145734
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.290591
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.128890
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.284239
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.097694
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.203633
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.375629
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.101542
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.132881

Test set: Average loss: 1.1764, Accuracy: 7078/10000 (71%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.254229
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.183063
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.219230
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.215106
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.153899
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.320232
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.147820
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.188484
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.304847

Test set: Average loss: 1.1431, Accuracy: 7248/10000 (72%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.154090
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.240250
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.118073
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.192358
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.142112
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.000400
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.994856
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.186090
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.140345

Test set: Average loss: 1.1223, Accuracy: 7282/10000 (73%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.059673
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.170284
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.098850
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.142706
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.210254
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.164036
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.223904
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.109091
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.137370

Test set: Average loss: 1.1079, Accuracy: 7362/10000 (74%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.241055
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.079621
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.161612
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.119924
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.998456
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.239992
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.096365
Train Epoch: 12 [51136/60000 (85%)]	Loss: 1.136106
Train Epoch: 12 [57536/60000 (96%)]	Loss: 1.231424

Test set: Average loss: 1.0983, Accuracy: 7393/10000 (74%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.097436
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.101543
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.002841
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.988074
Train Epoch: 13 [31936/60000 (53%)]	Loss: 1.112460
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.161439
Train Epoch: 13 [44736/60000 (75%)]	Loss: 1.204497
Train Epoch: 13 [51136/60000 (85%)]	Loss: 1.092537
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.222089

Test set: Average loss: 1.0912, Accuracy: 7452/10000 (75%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 1.073710
Train Epoch: 14 [12736/60000 (21%)]	Loss: 1.095972
Train Epoch: 14 [19136/60000 (32%)]	Loss: 1.103943
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.989838
Train Epoch: 14 [31936/60000 (53%)]	Loss: 1.203445
Train Epoch: 14 [38336/60000 (64%)]	Loss: 1.161150
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.931054
Train Epoch: 14 [51136/60000 (85%)]	Loss: 1.048903
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.160469

Test set: Average loss: 1.0873, Accuracy: 7454/10000 (75%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 1.232060
Train Epoch: 15 [12736/60000 (21%)]	Loss: 1.114526
Train Epoch: 15 [19136/60000 (32%)]	Loss: 1.148357
Train Epoch: 15 [25536/60000 (43%)]	Loss: 1.214732
Train Epoch: 15 [31936/60000 (53%)]	Loss: 1.062211
Train Epoch: 15 [38336/60000 (64%)]	Loss: 1.099476
Train Epoch: 15 [44736/60000 (75%)]	Loss: 1.009881
Train Epoch: 15 [51136/60000 (85%)]	Loss: 1.079560
Train Epoch: 15 [57536/60000 (96%)]	Loss: 1.188614

Test set: Average loss: 1.0860, Accuracy: 7510/10000 (75%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 1.145023
Train Epoch: 16 [12736/60000 (21%)]	Loss: 1.112566
Train Epoch: 16 [19136/60000 (32%)]	Loss: 1.111696
Train Epoch: 16 [25536/60000 (43%)]	Loss: 0.871601
Train Epoch: 16 [31936/60000 (53%)]	Loss: 1.182259
Train Epoch: 16 [38336/60000 (64%)]	Loss: 0.979026
Train Epoch: 16 [44736/60000 (75%)]	Loss: 1.029261
Train Epoch: 16 [51136/60000 (85%)]	Loss: 1.117510
Train Epoch: 16 [57536/60000 (96%)]	Loss: 1.059608

Test set: Average loss: 1.0843, Accuracy: 7500/10000 (75%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 1.138712
Train Epoch: 17 [12736/60000 (21%)]	Loss: 1.101688
Train Epoch: 17 [19136/60000 (32%)]	Loss: 1.003716
Train Epoch: 17 [25536/60000 (43%)]	Loss: 1.036554
Train Epoch: 17 [31936/60000 (53%)]	Loss: 1.160099
Train Epoch: 17 [38336/60000 (64%)]	Loss: 1.072655
Train Epoch: 17 [44736/60000 (75%)]	Loss: 1.158053
Train Epoch: 17 [51136/60000 (85%)]	Loss: 1.280153
Train Epoch: 17 [57536/60000 (96%)]	Loss: 1.078981

Test set: Average loss: 1.0837, Accuracy: 7468/10000 (75%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.83 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 17444
