Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.975, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.301938
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.301590
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.301120
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.298983
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.299002
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.298904
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.297332
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.291933
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.291145

Test set: Average loss: 2.2948, Accuracy: 2090/10000 (21%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.293570
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.287174
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.290523
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.284435
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.288402
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.272865
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.277154
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.265934
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.258496

Test set: Average loss: 2.2696, Accuracy: 1995/10000 (20%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.250151
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.251794
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.248847
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.258578
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.222190
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.251055
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.224733
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.204767
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.189703

Test set: Average loss: 2.1907, Accuracy: 2912/10000 (29%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.156626
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.108138
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.163629
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.099612
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.117256
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.128158
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.097174
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.064921
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.004546

Test set: Average loss: 2.0436, Accuracy: 3466/10000 (35%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.015270
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.023019
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.072004
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.927008
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.968265
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.053574
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.014498
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.004690
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.008771

Test set: Average loss: 1.9055, Accuracy: 3825/10000 (38%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.878126
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.949058
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.879962
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.858153
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.809569
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.924505
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.880278
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.744298
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.783893

Test set: Average loss: 1.8164, Accuracy: 4491/10000 (45%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.799775
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.837723
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.900364
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.748771
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.753475
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.831917
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.876435
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.814790
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.813692

Test set: Average loss: 1.7650, Accuracy: 4599/10000 (46%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.732495
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.811316
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.689068
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.846582
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.643031
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.786499
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.844535
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.662327
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.644868

Test set: Average loss: 1.7315, Accuracy: 4833/10000 (48%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.770718
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.664230
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.781905
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.720134
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.742943
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.830455
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.770750
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.752417
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.833079

Test set: Average loss: 1.7088, Accuracy: 4856/10000 (49%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.710579
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.809917
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.690284
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.702982
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.671537
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.567726
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.623613
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.718352
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.759081

Test set: Average loss: 1.6943, Accuracy: 4943/10000 (49%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.604518
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.771608
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.632251
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.679662
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.782344
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.733160
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.758473
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.703135
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.729674

Test set: Average loss: 1.6840, Accuracy: 5038/10000 (50%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.774481
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.630974
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.783592
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.677286
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.591215
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.847546
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.682853
Train Epoch: 12 [51136/60000 (85%)]	Loss: 1.712656
Train Epoch: 12 [57536/60000 (96%)]	Loss: 1.788448

Test set: Average loss: 1.6768, Accuracy: 5110/10000 (51%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.720116
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.699197
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.703720
Train Epoch: 13 [25536/60000 (43%)]	Loss: 1.612493
Train Epoch: 13 [31936/60000 (53%)]	Loss: 1.674371
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.703940
Train Epoch: 13 [44736/60000 (75%)]	Loss: 1.755594
Train Epoch: 13 [51136/60000 (85%)]	Loss: 1.691682
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.753904

Test set: Average loss: 1.6708, Accuracy: 5138/10000 (51%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 1.664586
Train Epoch: 14 [12736/60000 (21%)]	Loss: 1.662508
Train Epoch: 14 [19136/60000 (32%)]	Loss: 1.616490
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.530986
Train Epoch: 14 [31936/60000 (53%)]	Loss: 1.693522
Train Epoch: 14 [38336/60000 (64%)]	Loss: 1.751037
Train Epoch: 14 [44736/60000 (75%)]	Loss: 1.537451
Train Epoch: 14 [51136/60000 (85%)]	Loss: 1.630304
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.662527

Test set: Average loss: 1.6695, Accuracy: 5204/10000 (52%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 1.747515
Train Epoch: 15 [12736/60000 (21%)]	Loss: 1.731589
Train Epoch: 15 [19136/60000 (32%)]	Loss: 1.735467
Train Epoch: 15 [25536/60000 (43%)]	Loss: 1.731812
Train Epoch: 15 [31936/60000 (53%)]	Loss: 1.624018
Train Epoch: 15 [38336/60000 (64%)]	Loss: 1.588445
Train Epoch: 15 [44736/60000 (75%)]	Loss: 1.582883
Train Epoch: 15 [51136/60000 (85%)]	Loss: 1.678211
Train Epoch: 15 [57536/60000 (96%)]	Loss: 1.689675

Test set: Average loss: 1.6670, Accuracy: 5170/10000 (52%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 1.722672
Train Epoch: 16 [12736/60000 (21%)]	Loss: 1.682369
Train Epoch: 16 [19136/60000 (32%)]	Loss: 1.700382
Train Epoch: 16 [25536/60000 (43%)]	Loss: 1.511128
Train Epoch: 16 [31936/60000 (53%)]	Loss: 1.734855
Train Epoch: 16 [38336/60000 (64%)]	Loss: 1.615927
Train Epoch: 16 [44736/60000 (75%)]	Loss: 1.660038
Train Epoch: 16 [51136/60000 (85%)]	Loss: 1.698871
Train Epoch: 16 [57536/60000 (96%)]	Loss: 1.691820

Test set: Average loss: 1.6670, Accuracy: 5273/10000 (53%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 1.718211
Train Epoch: 17 [12736/60000 (21%)]	Loss: 1.671344
Train Epoch: 17 [19136/60000 (32%)]	Loss: 1.601542
Train Epoch: 17 [25536/60000 (43%)]	Loss: 1.659693
Train Epoch: 17 [31936/60000 (53%)]	Loss: 1.757238
Train Epoch: 17 [38336/60000 (64%)]	Loss: 1.643068
Train Epoch: 17 [44736/60000 (75%)]	Loss: 1.749125
Train Epoch: 17 [51136/60000 (85%)]	Loss: 1.746760
Train Epoch: 17 [57536/60000 (96%)]	Loss: 1.665421

Test set: Average loss: 1.6664, Accuracy: 5306/10000 (53%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.975, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.85 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 9997
