Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=13, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.99, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.302672
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.302631
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.302143
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.301878
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.302043
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.302071
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.301524
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.301441
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.301168

Test set: Average loss: 2.3017, Accuracy: 1892/10000 (19%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.301690
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.301023
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.301791
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.300641
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.301332
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.299010
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.300070
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.299104
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.298139

Test set: Average loss: 2.2997, Accuracy: 2022/10000 (20%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.297329
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.297823
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.298770
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.299856
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.296508
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.298865
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.297117
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.297090
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.294520

Test set: Average loss: 2.2949, Accuracy: 1696/10000 (17%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.292036
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.288905
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.294479
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.288373
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.291126
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.291339
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.291277
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.285676
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.277758

Test set: Average loss: 2.2834, Accuracy: 2050/10000 (20%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.274678
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.281960
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.284596
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.267424
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.271477
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.287329
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.285625
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.277937
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.286232

Test set: Average loss: 2.2668, Accuracy: 2260/10000 (23%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.254073
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.274559
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.263363
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.265447
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.248116
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.260002
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.260891
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.241199
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.241734

Test set: Average loss: 2.2454, Accuracy: 2265/10000 (23%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.233743
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.250112
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.254852
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.217352
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.225499
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.235614
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.249495
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.247133
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.231703

Test set: Average loss: 2.2275, Accuracy: 2302/10000 (23%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.210059
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.239983
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.192003
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.231630
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.181350
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.250541
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.232108
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.197930
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.179119

Test set: Average loss: 2.2112, Accuracy: 2248/10000 (22%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.225672
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.178135
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.200100
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.203490
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.225372
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.215166
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.224137
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.212938
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.231045

Test set: Average loss: 2.2012, Accuracy: 2268/10000 (23%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.201600
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.218678
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.175364
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.195585
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.177037
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.178051
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.187341
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.201210
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.216116

Test set: Average loss: 2.1946, Accuracy: 2266/10000 (23%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.142065
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.232201
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.156912
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.200323
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.207190
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.186873
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.182376
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.197634
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.215714

Test set: Average loss: 2.1905, Accuracy: 2249/10000 (22%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.232572
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.161587
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.226367
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.191673
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.187417
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.233027
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.216354
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.203342
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.204955

Test set: Average loss: 2.1893, Accuracy: 2263/10000 (23%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.186884
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.192567
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.204176
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.179152
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.184250
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.182949
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.231384
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.203950
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.201990

Test set: Average loss: 2.1882, Accuracy: 2261/10000 (23%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=13, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.99, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.40 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 5529
