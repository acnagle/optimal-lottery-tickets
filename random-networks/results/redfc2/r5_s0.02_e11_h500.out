Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.147642
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.262311
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.183369
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.792807
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.409176
Train Epoch: 1 [38336/60000 (64%)]	Loss: 4.080945
Train Epoch: 1 [44736/60000 (75%)]	Loss: 5.067652
Train Epoch: 1 [51136/60000 (85%)]	Loss: 5.304578
Train Epoch: 1 [57536/60000 (96%)]	Loss: 6.211132

Test set: Average loss: 6.7495, Accuracy: 3214/10000 (32%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 6.925942
Train Epoch: 2 [12736/60000 (21%)]	Loss: 7.882135
Train Epoch: 2 [19136/60000 (32%)]	Loss: 9.491666
Train Epoch: 2 [25536/60000 (43%)]	Loss: 11.306217
Train Epoch: 2 [31936/60000 (53%)]	Loss: 11.621301
Train Epoch: 2 [38336/60000 (64%)]	Loss: 10.895604
Train Epoch: 2 [44736/60000 (75%)]	Loss: 10.690189
Train Epoch: 2 [51136/60000 (85%)]	Loss: 10.819516
Train Epoch: 2 [57536/60000 (96%)]	Loss: 12.807493

Test set: Average loss: 11.0211, Accuracy: 1557/10000 (16%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 11.938098
Train Epoch: 3 [12736/60000 (21%)]	Loss: 13.465026
Train Epoch: 3 [19136/60000 (32%)]	Loss: 13.444983
Train Epoch: 3 [25536/60000 (43%)]	Loss: 12.360391
Train Epoch: 3 [31936/60000 (53%)]	Loss: 13.220455
Train Epoch: 3 [38336/60000 (64%)]	Loss: 13.930740
Train Epoch: 3 [44736/60000 (75%)]	Loss: 12.767385
Train Epoch: 3 [51136/60000 (85%)]	Loss: 13.978656
Train Epoch: 3 [57536/60000 (96%)]	Loss: 12.589844

Test set: Average loss: 13.1948, Accuracy: 1218/10000 (12%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 13.051798
Train Epoch: 4 [12736/60000 (21%)]	Loss: 13.912684
Train Epoch: 4 [19136/60000 (32%)]	Loss: 12.725747
Train Epoch: 4 [25536/60000 (43%)]	Loss: 15.453677
Train Epoch: 4 [31936/60000 (53%)]	Loss: 11.944512
Train Epoch: 4 [38336/60000 (64%)]	Loss: 11.967003
Train Epoch: 4 [44736/60000 (75%)]	Loss: 13.281424
Train Epoch: 4 [51136/60000 (85%)]	Loss: 11.732029
Train Epoch: 4 [57536/60000 (96%)]	Loss: 13.390841

Test set: Average loss: 13.7843, Accuracy: 1162/10000 (12%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 13.635765
Train Epoch: 5 [12736/60000 (21%)]	Loss: 14.783665
Train Epoch: 5 [19136/60000 (32%)]	Loss: 14.260252
Train Epoch: 5 [25536/60000 (43%)]	Loss: 13.705625
Train Epoch: 5 [31936/60000 (53%)]	Loss: 12.577321
Train Epoch: 5 [38336/60000 (64%)]	Loss: 13.515910
Train Epoch: 5 [44736/60000 (75%)]	Loss: 17.846395
Train Epoch: 5 [51136/60000 (85%)]	Loss: 17.285194
Train Epoch: 5 [57536/60000 (96%)]	Loss: 13.914151

Test set: Average loss: 14.6907, Accuracy: 1022/10000 (10%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 13.702322
Train Epoch: 6 [12736/60000 (21%)]	Loss: 12.431587
Train Epoch: 6 [19136/60000 (32%)]	Loss: 13.048649
Train Epoch: 6 [25536/60000 (43%)]	Loss: 13.906767
Train Epoch: 6 [31936/60000 (53%)]	Loss: 13.810967
Train Epoch: 6 [38336/60000 (64%)]	Loss: 16.972580
Train Epoch: 6 [44736/60000 (75%)]	Loss: 14.840442
Train Epoch: 6 [51136/60000 (85%)]	Loss: 15.273971
Train Epoch: 6 [57536/60000 (96%)]	Loss: 16.308748

Test set: Average loss: 14.2042, Accuracy: 1099/10000 (11%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 14.689343
Train Epoch: 7 [12736/60000 (21%)]	Loss: 14.472996
Train Epoch: 7 [19136/60000 (32%)]	Loss: 13.217888
Train Epoch: 7 [25536/60000 (43%)]	Loss: 13.094124
Train Epoch: 7 [31936/60000 (53%)]	Loss: 11.411434
Train Epoch: 7 [38336/60000 (64%)]	Loss: 15.167308
Train Epoch: 7 [44736/60000 (75%)]	Loss: 14.211507
Train Epoch: 7 [51136/60000 (85%)]	Loss: 13.614739
Train Epoch: 7 [57536/60000 (96%)]	Loss: 13.726756

Test set: Average loss: 14.5825, Accuracy: 1113/10000 (11%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 15.568071
Train Epoch: 8 [12736/60000 (21%)]	Loss: 15.373817
Train Epoch: 8 [19136/60000 (32%)]	Loss: 12.409021
Train Epoch: 8 [25536/60000 (43%)]	Loss: 14.662120
Train Epoch: 8 [31936/60000 (53%)]	Loss: 15.422496
Train Epoch: 8 [38336/60000 (64%)]	Loss: 13.997307
Train Epoch: 8 [44736/60000 (75%)]	Loss: 14.943536
Train Epoch: 8 [51136/60000 (85%)]	Loss: 13.330608
Train Epoch: 8 [57536/60000 (96%)]	Loss: 13.850249

Test set: Average loss: 15.0245, Accuracy: 1057/10000 (11%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 12.867383
Train Epoch: 9 [12736/60000 (21%)]	Loss: 13.565129
Train Epoch: 9 [19136/60000 (32%)]	Loss: 13.943416
Train Epoch: 9 [25536/60000 (43%)]	Loss: 14.699548
Train Epoch: 9 [31936/60000 (53%)]	Loss: 13.681113
Train Epoch: 9 [38336/60000 (64%)]	Loss: 15.620788
Train Epoch: 9 [44736/60000 (75%)]	Loss: 15.370261
Train Epoch: 9 [51136/60000 (85%)]	Loss: 16.462595
Train Epoch: 9 [57536/60000 (96%)]	Loss: 14.096598

Test set: Average loss: 15.1731, Accuracy: 1089/10000 (11%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 14.447266
Train Epoch: 10 [12736/60000 (21%)]	Loss: 13.614254
Train Epoch: 10 [19136/60000 (32%)]	Loss: 16.226507
Train Epoch: 10 [25536/60000 (43%)]	Loss: 14.399194
Train Epoch: 10 [31936/60000 (53%)]	Loss: 17.042381
Train Epoch: 10 [38336/60000 (64%)]	Loss: 15.991718
Train Epoch: 10 [44736/60000 (75%)]	Loss: 13.806057
Train Epoch: 10 [51136/60000 (85%)]	Loss: 16.881714
Train Epoch: 10 [57536/60000 (96%)]	Loss: 14.902771

Test set: Average loss: 15.0834, Accuracy: 1057/10000 (11%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 14.341146
Train Epoch: 11 [12736/60000 (21%)]	Loss: 12.338262
Train Epoch: 11 [19136/60000 (32%)]	Loss: 11.674052
Train Epoch: 11 [25536/60000 (43%)]	Loss: 14.755479
Train Epoch: 11 [31936/60000 (53%)]	Loss: 13.680845
Train Epoch: 11 [38336/60000 (64%)]	Loss: 14.468970
Train Epoch: 11 [44736/60000 (75%)]	Loss: 15.841913
Train Epoch: 11 [51136/60000 (85%)]	Loss: 12.983877
Train Epoch: 11 [57536/60000 (96%)]	Loss: 14.405677

Test set: Average loss: 15.4202, Accuracy: 995/10000 (10%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.21 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1951591
