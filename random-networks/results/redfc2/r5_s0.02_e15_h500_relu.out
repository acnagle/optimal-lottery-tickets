Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.776434
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.822870
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.917803
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.081477
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.142877
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.278734
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.052520
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.387055
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.192310

Test set: Average loss: 1.7530, Accuracy: 5650/10000 (56%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.846393
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.268058
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.390407
Train Epoch: 2 [25536/60000 (43%)]	Loss: 3.437955
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.386265
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.970849
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.658247
Train Epoch: 2 [51136/60000 (85%)]	Loss: 3.589622
Train Epoch: 2 [57536/60000 (96%)]	Loss: 3.180373

Test set: Average loss: 3.5552, Accuracy: 3046/10000 (30%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 3.720772
Train Epoch: 3 [12736/60000 (21%)]	Loss: 4.281640
Train Epoch: 3 [19136/60000 (32%)]	Loss: 4.360762
Train Epoch: 3 [25536/60000 (43%)]	Loss: 3.363073
Train Epoch: 3 [31936/60000 (53%)]	Loss: 5.112204
Train Epoch: 3 [38336/60000 (64%)]	Loss: 4.835675
Train Epoch: 3 [44736/60000 (75%)]	Loss: 5.166738
Train Epoch: 3 [51136/60000 (85%)]	Loss: 5.191554
Train Epoch: 3 [57536/60000 (96%)]	Loss: 5.112922

Test set: Average loss: 5.3335, Accuracy: 1528/10000 (15%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 6.352428
Train Epoch: 4 [12736/60000 (21%)]	Loss: 4.686099
Train Epoch: 4 [19136/60000 (32%)]	Loss: 6.055799
Train Epoch: 4 [25536/60000 (43%)]	Loss: 6.155991
Train Epoch: 4 [31936/60000 (53%)]	Loss: 5.402371
Train Epoch: 4 [38336/60000 (64%)]	Loss: 6.006409
Train Epoch: 4 [44736/60000 (75%)]	Loss: 5.726214
Train Epoch: 4 [51136/60000 (85%)]	Loss: 5.900031
Train Epoch: 4 [57536/60000 (96%)]	Loss: 6.887791

Test set: Average loss: 6.1045, Accuracy: 1265/10000 (13%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 6.176877
Train Epoch: 5 [12736/60000 (21%)]	Loss: 6.465450
Train Epoch: 5 [19136/60000 (32%)]	Loss: 7.442191
Train Epoch: 5 [25536/60000 (43%)]	Loss: 6.895175
Train Epoch: 5 [31936/60000 (53%)]	Loss: 6.328051
Train Epoch: 5 [38336/60000 (64%)]	Loss: 6.433849
Train Epoch: 5 [44736/60000 (75%)]	Loss: 6.891284
Train Epoch: 5 [51136/60000 (85%)]	Loss: 7.097599
Train Epoch: 5 [57536/60000 (96%)]	Loss: 6.728303

Test set: Average loss: 6.6901, Accuracy: 1112/10000 (11%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 6.349105
Train Epoch: 6 [12736/60000 (21%)]	Loss: 6.773642
Train Epoch: 6 [19136/60000 (32%)]	Loss: 6.057780
Train Epoch: 6 [25536/60000 (43%)]	Loss: 7.412892
Train Epoch: 6 [31936/60000 (53%)]	Loss: 6.694254
Train Epoch: 6 [38336/60000 (64%)]	Loss: 6.893020
Train Epoch: 6 [44736/60000 (75%)]	Loss: 7.446459
Train Epoch: 6 [51136/60000 (85%)]	Loss: 7.179146
Train Epoch: 6 [57536/60000 (96%)]	Loss: 6.838322

Test set: Average loss: 6.8805, Accuracy: 1118/10000 (11%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 6.894267
Train Epoch: 7 [12736/60000 (21%)]	Loss: 7.002366
Train Epoch: 7 [19136/60000 (32%)]	Loss: 6.544910
Train Epoch: 7 [25536/60000 (43%)]	Loss: 6.121621
Train Epoch: 7 [31936/60000 (53%)]	Loss: 6.222558
Train Epoch: 7 [38336/60000 (64%)]	Loss: 7.156667
Train Epoch: 7 [44736/60000 (75%)]	Loss: 6.835871
Train Epoch: 7 [51136/60000 (85%)]	Loss: 7.910703
Train Epoch: 7 [57536/60000 (96%)]	Loss: 8.061092

Test set: Average loss: 7.1880, Accuracy: 973/10000 (10%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 8.234199
Train Epoch: 8 [12736/60000 (21%)]	Loss: 7.601419
Train Epoch: 8 [19136/60000 (32%)]	Loss: 6.317826
Train Epoch: 8 [25536/60000 (43%)]	Loss: 7.190918
Train Epoch: 8 [31936/60000 (53%)]	Loss: 6.412859
Train Epoch: 8 [38336/60000 (64%)]	Loss: 7.204555
Train Epoch: 8 [44736/60000 (75%)]	Loss: 7.933132
Train Epoch: 8 [51136/60000 (85%)]	Loss: 7.155296
Train Epoch: 8 [57536/60000 (96%)]	Loss: 7.871585

Test set: Average loss: 7.2024, Accuracy: 961/10000 (10%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 6.921349
Train Epoch: 9 [12736/60000 (21%)]	Loss: 7.217838
Train Epoch: 9 [19136/60000 (32%)]	Loss: 7.172002
Train Epoch: 9 [25536/60000 (43%)]	Loss: 8.549092
Train Epoch: 9 [31936/60000 (53%)]	Loss: 6.933206
Train Epoch: 9 [38336/60000 (64%)]	Loss: 7.959460
Train Epoch: 9 [44736/60000 (75%)]	Loss: 8.251732
Train Epoch: 9 [51136/60000 (85%)]	Loss: 8.116138
Train Epoch: 9 [57536/60000 (96%)]	Loss: 7.804715

Test set: Average loss: 7.5262, Accuracy: 953/10000 (10%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 7.157577
Train Epoch: 10 [12736/60000 (21%)]	Loss: 7.121363
Train Epoch: 10 [19136/60000 (32%)]	Loss: 7.152332
Train Epoch: 10 [25536/60000 (43%)]	Loss: 8.213826
Train Epoch: 10 [31936/60000 (53%)]	Loss: 7.832993
Train Epoch: 10 [38336/60000 (64%)]	Loss: 7.456264
Train Epoch: 10 [44736/60000 (75%)]	Loss: 7.452139
Train Epoch: 10 [51136/60000 (85%)]	Loss: 7.898987
Train Epoch: 10 [57536/60000 (96%)]	Loss: 7.048233

Test set: Average loss: 7.6557, Accuracy: 895/10000 (9%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 6.961587
Train Epoch: 11 [12736/60000 (21%)]	Loss: 6.339280
Train Epoch: 11 [19136/60000 (32%)]	Loss: 7.185998
Train Epoch: 11 [25536/60000 (43%)]	Loss: 8.229036
Train Epoch: 11 [31936/60000 (53%)]	Loss: 7.101700
Train Epoch: 11 [38336/60000 (64%)]	Loss: 7.495178
Train Epoch: 11 [44736/60000 (75%)]	Loss: 9.456763
Train Epoch: 11 [51136/60000 (85%)]	Loss: 6.891802
Train Epoch: 11 [57536/60000 (96%)]	Loss: 8.023957

Test set: Average loss: 7.8084, Accuracy: 888/10000 (9%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 7.042531
Train Epoch: 12 [12736/60000 (21%)]	Loss: 8.395173
Train Epoch: 12 [19136/60000 (32%)]	Loss: 8.288392
Train Epoch: 12 [25536/60000 (43%)]	Loss: 7.467916
Train Epoch: 12 [31936/60000 (53%)]	Loss: 8.905936
Train Epoch: 12 [38336/60000 (64%)]	Loss: 8.510753
Train Epoch: 12 [44736/60000 (75%)]	Loss: 8.688490
Train Epoch: 12 [51136/60000 (85%)]	Loss: 8.233804
Train Epoch: 12 [57536/60000 (96%)]	Loss: 7.801635

Test set: Average loss: 7.9389, Accuracy: 832/10000 (8%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 7.264258
Train Epoch: 13 [12736/60000 (21%)]	Loss: 8.511040
Train Epoch: 13 [19136/60000 (32%)]	Loss: 8.186985
Train Epoch: 13 [25536/60000 (43%)]	Loss: 8.384105
Train Epoch: 13 [31936/60000 (53%)]	Loss: 7.413675
Train Epoch: 13 [38336/60000 (64%)]	Loss: 8.311165
Train Epoch: 13 [44736/60000 (75%)]	Loss: 7.714412
Train Epoch: 13 [51136/60000 (85%)]	Loss: 8.329840
Train Epoch: 13 [57536/60000 (96%)]	Loss: 8.016004

Test set: Average loss: 7.9343, Accuracy: 799/10000 (8%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 8.440732
Train Epoch: 14 [12736/60000 (21%)]	Loss: 8.286674
Train Epoch: 14 [19136/60000 (32%)]	Loss: 8.591457
Train Epoch: 14 [25536/60000 (43%)]	Loss: 8.643602
Train Epoch: 14 [31936/60000 (53%)]	Loss: 8.523177
Train Epoch: 14 [38336/60000 (64%)]	Loss: 8.997256
Train Epoch: 14 [44736/60000 (75%)]	Loss: 8.811993
Train Epoch: 14 [51136/60000 (85%)]	Loss: 8.619402
Train Epoch: 14 [57536/60000 (96%)]	Loss: 8.434324

Test set: Average loss: 8.3001, Accuracy: 743/10000 (7%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 8.519260
Train Epoch: 15 [12736/60000 (21%)]	Loss: 6.832271
Train Epoch: 15 [19136/60000 (32%)]	Loss: 6.963745
Train Epoch: 15 [25536/60000 (43%)]	Loss: 7.419763
Train Epoch: 15 [31936/60000 (53%)]	Loss: 8.929938
Train Epoch: 15 [38336/60000 (64%)]	Loss: 7.789138
Train Epoch: 15 [44736/60000 (75%)]	Loss: 6.790427
Train Epoch: 15 [51136/60000 (85%)]	Loss: 8.252458
Train Epoch: 15 [57536/60000 (96%)]	Loss: 8.398204

Test set: Average loss: 8.0193, Accuracy: 806/10000 (8%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.64 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1951591
