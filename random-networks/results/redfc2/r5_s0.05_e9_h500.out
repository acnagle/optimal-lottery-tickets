Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=False, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.241253
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.863802
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.203591
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.565550
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.517444
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.452271
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.543688
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.464651
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.347895

Test set: Average loss: 0.4721, Accuracy: 8679/10000 (87%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.403191
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.611297
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.412086
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.843489
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.731475
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.533587
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.608362
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.002850
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.079991

Test set: Average loss: 0.8355, Accuracy: 8088/10000 (81%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.474904
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.373637
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.183772
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.224921
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.392056
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.891782
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.118865
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.578432
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.218017

Test set: Average loss: 2.5703, Accuracy: 6420/10000 (64%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 3.766484
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.742137
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.944340
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.727584
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.968068
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.656395
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.624530
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.773212
Train Epoch: 4 [57536/60000 (96%)]	Loss: 3.099535

Test set: Average loss: 2.7786, Accuracy: 5957/10000 (60%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 3.887942
Train Epoch: 5 [12736/60000 (21%)]	Loss: 4.939410
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.703511
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.711009
Train Epoch: 5 [31936/60000 (53%)]	Loss: 3.019566
Train Epoch: 5 [38336/60000 (64%)]	Loss: 4.180295
Train Epoch: 5 [44736/60000 (75%)]	Loss: 4.745467
Train Epoch: 5 [51136/60000 (85%)]	Loss: 4.863060
Train Epoch: 5 [57536/60000 (96%)]	Loss: 3.918541

Test set: Average loss: 4.4572, Accuracy: 4998/10000 (50%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 4.645474
Train Epoch: 6 [12736/60000 (21%)]	Loss: 3.766433
Train Epoch: 6 [19136/60000 (32%)]	Loss: 5.449112
Train Epoch: 6 [25536/60000 (43%)]	Loss: 3.452070
Train Epoch: 6 [31936/60000 (53%)]	Loss: 4.769572
Train Epoch: 6 [38336/60000 (64%)]	Loss: 6.442482
Train Epoch: 6 [44736/60000 (75%)]	Loss: 4.215024
Train Epoch: 6 [51136/60000 (85%)]	Loss: 4.540086
Train Epoch: 6 [57536/60000 (96%)]	Loss: 5.209710

Test set: Average loss: 4.6972, Accuracy: 5011/10000 (50%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 5.046168
Train Epoch: 7 [12736/60000 (21%)]	Loss: 3.005687
Train Epoch: 7 [19136/60000 (32%)]	Loss: 4.383083
Train Epoch: 7 [25536/60000 (43%)]	Loss: 4.082739
Train Epoch: 7 [31936/60000 (53%)]	Loss: 3.504550
Train Epoch: 7 [38336/60000 (64%)]	Loss: 5.092907
Train Epoch: 7 [44736/60000 (75%)]	Loss: 4.579137
Train Epoch: 7 [51136/60000 (85%)]	Loss: 5.270727
Train Epoch: 7 [57536/60000 (96%)]	Loss: 5.244135

Test set: Average loss: 6.1297, Accuracy: 4003/10000 (40%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 7.145713
Train Epoch: 8 [12736/60000 (21%)]	Loss: 5.509912
Train Epoch: 8 [19136/60000 (32%)]	Loss: 6.286238
Train Epoch: 8 [25536/60000 (43%)]	Loss: 5.323031
Train Epoch: 8 [31936/60000 (53%)]	Loss: 6.541245
Train Epoch: 8 [38336/60000 (64%)]	Loss: 4.858027
Train Epoch: 8 [44736/60000 (75%)]	Loss: 5.456779
Train Epoch: 8 [51136/60000 (85%)]	Loss: 4.596982
Train Epoch: 8 [57536/60000 (96%)]	Loss: 4.776342

Test set: Average loss: 5.0327, Accuracy: 4280/10000 (43%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 4.562548
Train Epoch: 9 [12736/60000 (21%)]	Loss: 3.326854
Train Epoch: 9 [19136/60000 (32%)]	Loss: 4.552422
Train Epoch: 9 [25536/60000 (43%)]	Loss: 4.717303
Train Epoch: 9 [31936/60000 (53%)]	Loss: 5.358486
Train Epoch: 9 [38336/60000 (64%)]	Loss: 4.888812
Train Epoch: 9 [44736/60000 (75%)]	Loss: 5.999431
Train Epoch: 9 [51136/60000 (85%)]	Loss: 6.664651
Train Epoch: 9 [57536/60000 (96%)]	Loss: 5.006311

Test set: Average loss: 4.6971, Accuracy: 4520/10000 (45%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=False, wd=0.0005)


Total time spent pruning/training: 0.99 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1891849
