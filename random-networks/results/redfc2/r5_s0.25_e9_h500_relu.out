Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.25, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.439571
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.597966
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.555136
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.332761
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.393297
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.170032
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.181197
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.135713
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.164318

Test set: Average loss: 0.2116, Accuracy: 9353/10000 (94%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.159419
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.287665
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.113835
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.171859
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.158437
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.213519
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.231682
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.281985
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.281702

Test set: Average loss: 0.1847, Accuracy: 9431/10000 (94%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.175852
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.139948
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.231972
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.210370
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.252970
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.205941
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.185510
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.248727
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.428170

Test set: Average loss: 0.1715, Accuracy: 9493/10000 (95%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.089192
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.376093
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.138014
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.164262
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.315271
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.253139
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.211042
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.116577
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.160125

Test set: Average loss: 0.1844, Accuracy: 9437/10000 (94%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.209565
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.190950
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.159188
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.151634
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.105983
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.185659
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.126953
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.190101
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.100904

Test set: Average loss: 0.1675, Accuracy: 9500/10000 (95%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.112388
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.124212
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.185692
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.082081
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.102660
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.101651
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.108616
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.101664
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.177873

Test set: Average loss: 0.1401, Accuracy: 9602/10000 (96%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.051131
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.060348
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.082868
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.070386
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.045722
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.106142
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.189524
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.122465
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.061324

Test set: Average loss: 0.1245, Accuracy: 9643/10000 (96%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.230862
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.064667
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.066109
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.122002
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.163776
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.034236
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.273447
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.054632
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.054436

Test set: Average loss: 0.1098, Accuracy: 9694/10000 (97%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.077584
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.068202
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.034670
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.101973
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.080548
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.087077
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.083499
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.187534
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.039990

Test set: Average loss: 0.0987, Accuracy: 9703/10000 (97%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.25, use_relu=True, wd=0.0005)


Total time spent pruning/training: 0.98 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 1493565
