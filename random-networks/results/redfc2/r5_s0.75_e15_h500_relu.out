Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.75, use_relu=True, wd=0.0005) 

Pruning a Two-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.743024
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.527251
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.715980
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.490911
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.442560
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.425960
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.415416
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.299683
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.384081

Test set: Average loss: 0.3648, Accuracy: 8973/10000 (90%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.368952
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.449957
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.339606
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.434019
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.307788
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.305795
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.393742
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.446254
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.409475

Test set: Average loss: 0.3328, Accuracy: 9057/10000 (91%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.247440
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.258172
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.376545
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.389669
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.397796
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.371171
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.422541
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.377476
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.581921

Test set: Average loss: 0.3200, Accuracy: 9100/10000 (91%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.224700
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.552721
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.336809
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.406076
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.469127
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.526832
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.337634
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.243682
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.396257

Test set: Average loss: 0.3211, Accuracy: 9132/10000 (91%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.405638
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.284108
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.232464
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.483794
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.256706
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.442460
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.253921
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.443065
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.322705

Test set: Average loss: 0.3305, Accuracy: 9048/10000 (90%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.312480
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.319197
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.326440
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.328966
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.243189
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.317600
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.280392
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.291480
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.422449

Test set: Average loss: 0.3407, Accuracy: 9094/10000 (91%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.256002
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.283107
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.328796
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.245440
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.269679
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.330893
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.465039
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.357709
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.343702

Test set: Average loss: 0.3519, Accuracy: 9010/10000 (90%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.547476
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.338644
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.419090
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.431305
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.368379
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.297785
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.597098
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.403081
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.320621

Test set: Average loss: 0.3594, Accuracy: 8979/10000 (90%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.274861
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.427019
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.334386
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.384863
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.376400
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.432726
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.352177
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.591743
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.292352

Test set: Average loss: 0.3473, Accuracy: 9021/10000 (90%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.475693
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.419226
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.316470
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.315410
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.338864
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.450131
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.398227
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.257894
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.470670

Test set: Average loss: 0.3454, Accuracy: 9038/10000 (90%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.334249
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.328320
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.215310
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.300962
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.458368
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.309471
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.288977
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.390466
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.330865

Test set: Average loss: 0.3245, Accuracy: 9115/10000 (91%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.280259
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.330201
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.320146
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.200202
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.239649
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.455374
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.292306
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.310053
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.186441

Test set: Average loss: 0.3037, Accuracy: 9218/10000 (92%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.295442
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.251198
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.290211
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.330120
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.462433
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.445453
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.199722
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.307119
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.354163

Test set: Average loss: 0.2869, Accuracy: 9220/10000 (92%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 0.204738
Train Epoch: 14 [12736/60000 (21%)]	Loss: 0.333074
Train Epoch: 14 [19136/60000 (32%)]	Loss: 0.237815
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.109625
Train Epoch: 14 [31936/60000 (53%)]	Loss: 0.234138
Train Epoch: 14 [38336/60000 (64%)]	Loss: 0.242631
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.281440
Train Epoch: 14 [51136/60000 (85%)]	Loss: 0.216381
Train Epoch: 14 [57536/60000 (96%)]	Loss: 0.327830

Test set: Average loss: 0.2786, Accuracy: 9250/10000 (92%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 0.158540
Train Epoch: 15 [12736/60000 (21%)]	Loss: 0.176846
Train Epoch: 15 [19136/60000 (32%)]	Loss: 0.281893
Train Epoch: 15 [25536/60000 (43%)]	Loss: 0.251918
Train Epoch: 15 [31936/60000 (53%)]	Loss: 0.227373
Train Epoch: 15 [38336/60000 (64%)]	Loss: 0.428018
Train Epoch: 15 [44736/60000 (75%)]	Loss: 0.311736
Train Epoch: 15 [51136/60000 (85%)]	Loss: 0.475026
Train Epoch: 15 [57536/60000 (96%)]	Loss: 0.257294

Test set: Average loss: 0.2739, Accuracy: 9271/10000 (93%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.75, use_relu=True, wd=0.0005)


Total time spent pruning/training: 1.64 minutes
Total number of parameters in model: 1991420
Number of parameters in pruned model: 497855
