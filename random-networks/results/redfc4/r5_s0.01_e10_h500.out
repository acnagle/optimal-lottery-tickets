Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 31.911699
Train Epoch: 1 [12736/60000 (21%)]	Loss: 61.911823
Train Epoch: 1 [19136/60000 (32%)]	Loss: 91.175339
Train Epoch: 1 [25536/60000 (43%)]	Loss: 93.368652
Train Epoch: 1 [31936/60000 (53%)]	Loss: 97.209946
Train Epoch: 1 [38336/60000 (64%)]	Loss: 109.680611
Train Epoch: 1 [44736/60000 (75%)]	Loss: 124.418159
Train Epoch: 1 [51136/60000 (85%)]	Loss: 113.008614
Train Epoch: 1 [57536/60000 (96%)]	Loss: 135.440491

Test set: Average loss: 126.7376, Accuracy: 1154/10000 (12%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 115.289207
Train Epoch: 2 [12736/60000 (21%)]	Loss: 134.090530
Train Epoch: 2 [19136/60000 (32%)]	Loss: 130.444077
Train Epoch: 2 [25536/60000 (43%)]	Loss: 136.768433
Train Epoch: 2 [31936/60000 (53%)]	Loss: 126.604630
Train Epoch: 2 [38336/60000 (64%)]	Loss: 149.103561
Train Epoch: 2 [44736/60000 (75%)]	Loss: 137.541550
Train Epoch: 2 [51136/60000 (85%)]	Loss: 152.541626
Train Epoch: 2 [57536/60000 (96%)]	Loss: 156.415497

Test set: Average loss: 147.0787, Accuracy: 1103/10000 (11%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 123.281570
Train Epoch: 3 [12736/60000 (21%)]	Loss: 147.648575
Train Epoch: 3 [19136/60000 (32%)]	Loss: 160.417480
Train Epoch: 3 [25536/60000 (43%)]	Loss: 162.440796
Train Epoch: 3 [31936/60000 (53%)]	Loss: 139.785477
Train Epoch: 3 [38336/60000 (64%)]	Loss: 136.682922
Train Epoch: 3 [44736/60000 (75%)]	Loss: 139.830963
Train Epoch: 3 [51136/60000 (85%)]	Loss: 144.320648
Train Epoch: 3 [57536/60000 (96%)]	Loss: 142.805344

Test set: Average loss: 144.7987, Accuracy: 1113/10000 (11%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 140.822586
Train Epoch: 4 [12736/60000 (21%)]	Loss: 187.996872
Train Epoch: 4 [19136/60000 (32%)]	Loss: 155.742569
Train Epoch: 4 [25536/60000 (43%)]	Loss: 160.389664
Train Epoch: 4 [31936/60000 (53%)]	Loss: 157.035233
Train Epoch: 4 [38336/60000 (64%)]	Loss: 167.280777
Train Epoch: 4 [44736/60000 (75%)]	Loss: 139.334259
Train Epoch: 4 [51136/60000 (85%)]	Loss: 152.507538
Train Epoch: 4 [57536/60000 (96%)]	Loss: 122.016060

Test set: Average loss: 154.1026, Accuracy: 1113/10000 (11%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 162.397171
Train Epoch: 5 [12736/60000 (21%)]	Loss: 151.325699
Train Epoch: 5 [19136/60000 (32%)]	Loss: 142.972275
Train Epoch: 5 [25536/60000 (43%)]	Loss: 154.363098
Train Epoch: 5 [31936/60000 (53%)]	Loss: 162.188263
Train Epoch: 5 [38336/60000 (64%)]	Loss: 142.092194
Train Epoch: 5 [44736/60000 (75%)]	Loss: 157.129654
Train Epoch: 5 [51136/60000 (85%)]	Loss: 149.328796
Train Epoch: 5 [57536/60000 (96%)]	Loss: 170.511078

Test set: Average loss: 151.0756, Accuracy: 1109/10000 (11%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 139.497864
Train Epoch: 6 [12736/60000 (21%)]	Loss: 139.553467
Train Epoch: 6 [19136/60000 (32%)]	Loss: 173.204529
Train Epoch: 6 [25536/60000 (43%)]	Loss: 153.758026
Train Epoch: 6 [31936/60000 (53%)]	Loss: 177.469543
Train Epoch: 6 [38336/60000 (64%)]	Loss: 151.521500
Train Epoch: 6 [44736/60000 (75%)]	Loss: 146.009766
Train Epoch: 6 [51136/60000 (85%)]	Loss: 168.049194
Train Epoch: 6 [57536/60000 (96%)]	Loss: 169.652130

Test set: Average loss: 160.2231, Accuracy: 1077/10000 (11%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 159.194748
Train Epoch: 7 [12736/60000 (21%)]	Loss: 159.769592
Train Epoch: 7 [19136/60000 (32%)]	Loss: 164.578308
Train Epoch: 7 [25536/60000 (43%)]	Loss: 142.607056
Train Epoch: 7 [31936/60000 (53%)]	Loss: 164.410309
Train Epoch: 7 [38336/60000 (64%)]	Loss: 170.269592
Train Epoch: 7 [44736/60000 (75%)]	Loss: 146.880386
Train Epoch: 7 [51136/60000 (85%)]	Loss: 144.461624
Train Epoch: 7 [57536/60000 (96%)]	Loss: 176.170044

Test set: Average loss: 157.3304, Accuracy: 1093/10000 (11%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 152.308762
Train Epoch: 8 [12736/60000 (21%)]	Loss: 137.779648
Train Epoch: 8 [19136/60000 (32%)]	Loss: 162.171417
Train Epoch: 8 [25536/60000 (43%)]	Loss: 154.240845
Train Epoch: 8 [31936/60000 (53%)]	Loss: 152.433685
Train Epoch: 8 [38336/60000 (64%)]	Loss: 146.598007
Train Epoch: 8 [44736/60000 (75%)]	Loss: 150.340790
Train Epoch: 8 [51136/60000 (85%)]	Loss: 153.105652
Train Epoch: 8 [57536/60000 (96%)]	Loss: 147.834030

Test set: Average loss: 154.1815, Accuracy: 1087/10000 (11%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 146.246078
Train Epoch: 9 [12736/60000 (21%)]	Loss: 156.907776
Train Epoch: 9 [19136/60000 (32%)]	Loss: 166.991714
Train Epoch: 9 [25536/60000 (43%)]	Loss: 141.265015
Train Epoch: 9 [31936/60000 (53%)]	Loss: 151.677826
Train Epoch: 9 [38336/60000 (64%)]	Loss: 159.019424
Train Epoch: 9 [44736/60000 (75%)]	Loss: 160.864609
Train Epoch: 9 [51136/60000 (85%)]	Loss: 161.761032
Train Epoch: 9 [57536/60000 (96%)]	Loss: 177.332291

Test set: Average loss: 158.9110, Accuracy: 1091/10000 (11%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 161.776749
Train Epoch: 10 [12736/60000 (21%)]	Loss: 151.485565
Train Epoch: 10 [19136/60000 (32%)]	Loss: 154.467056
Train Epoch: 10 [25536/60000 (43%)]	Loss: 169.376434
Train Epoch: 10 [31936/60000 (53%)]	Loss: 180.321915
Train Epoch: 10 [38336/60000 (64%)]	Loss: 158.435455
Train Epoch: 10 [44736/60000 (75%)]	Loss: 166.728119
Train Epoch: 10 [51136/60000 (85%)]	Loss: 179.932495
Train Epoch: 10 [57536/60000 (96%)]	Loss: 155.520325

Test set: Average loss: 156.8836, Accuracy: 1051/10000 (11%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.00 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4451455
