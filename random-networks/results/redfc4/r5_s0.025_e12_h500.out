Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=12, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 11.195581
Train Epoch: 1 [12736/60000 (21%)]	Loss: 11.993148
Train Epoch: 1 [19136/60000 (32%)]	Loss: 18.973293
Train Epoch: 1 [25536/60000 (43%)]	Loss: 13.737168
Train Epoch: 1 [31936/60000 (53%)]	Loss: 21.636826
Train Epoch: 1 [38336/60000 (64%)]	Loss: 23.369637
Train Epoch: 1 [44736/60000 (75%)]	Loss: 18.424709
Train Epoch: 1 [51136/60000 (85%)]	Loss: 25.789642
Train Epoch: 1 [57536/60000 (96%)]	Loss: 34.440708

Test set: Average loss: 33.2640, Accuracy: 5096/10000 (51%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 42.687767
Train Epoch: 2 [12736/60000 (21%)]	Loss: 31.117571
Train Epoch: 2 [19136/60000 (32%)]	Loss: 34.557667
Train Epoch: 2 [25536/60000 (43%)]	Loss: 36.931225
Train Epoch: 2 [31936/60000 (53%)]	Loss: 38.189724
Train Epoch: 2 [38336/60000 (64%)]	Loss: 37.236618
Train Epoch: 2 [44736/60000 (75%)]	Loss: 45.044376
Train Epoch: 2 [51136/60000 (85%)]	Loss: 50.701656
Train Epoch: 2 [57536/60000 (96%)]	Loss: 39.330307

Test set: Average loss: 61.4418, Accuracy: 3280/10000 (33%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 51.110317
Train Epoch: 3 [12736/60000 (21%)]	Loss: 47.893570
Train Epoch: 3 [19136/60000 (32%)]	Loss: 62.323944
Train Epoch: 3 [25536/60000 (43%)]	Loss: 59.762562
Train Epoch: 3 [31936/60000 (53%)]	Loss: 41.738144
Train Epoch: 3 [38336/60000 (64%)]	Loss: 56.162521
Train Epoch: 3 [44736/60000 (75%)]	Loss: 54.070686
Train Epoch: 3 [51136/60000 (85%)]	Loss: 49.766804
Train Epoch: 3 [57536/60000 (96%)]	Loss: 58.025707

Test set: Average loss: 57.9153, Accuracy: 3363/10000 (34%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 51.576302
Train Epoch: 4 [12736/60000 (21%)]	Loss: 85.008163
Train Epoch: 4 [19136/60000 (32%)]	Loss: 47.895481
Train Epoch: 4 [25536/60000 (43%)]	Loss: 65.910431
Train Epoch: 4 [31936/60000 (53%)]	Loss: 63.058537
Train Epoch: 4 [38336/60000 (64%)]	Loss: 58.721573
Train Epoch: 4 [44736/60000 (75%)]	Loss: 56.447723
Train Epoch: 4 [51136/60000 (85%)]	Loss: 57.818249
Train Epoch: 4 [57536/60000 (96%)]	Loss: 45.657906

Test set: Average loss: 58.7939, Accuracy: 2963/10000 (30%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 68.180580
Train Epoch: 5 [12736/60000 (21%)]	Loss: 59.616360
Train Epoch: 5 [19136/60000 (32%)]	Loss: 73.060753
Train Epoch: 5 [25536/60000 (43%)]	Loss: 56.709305
Train Epoch: 5 [31936/60000 (53%)]	Loss: 69.214310
Train Epoch: 5 [38336/60000 (64%)]	Loss: 65.084610
Train Epoch: 5 [44736/60000 (75%)]	Loss: 62.449265
Train Epoch: 5 [51136/60000 (85%)]	Loss: 60.907169
Train Epoch: 5 [57536/60000 (96%)]	Loss: 70.090851

Test set: Average loss: 63.8286, Accuracy: 2600/10000 (26%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 56.381161
Train Epoch: 6 [12736/60000 (21%)]	Loss: 60.237671
Train Epoch: 6 [19136/60000 (32%)]	Loss: 72.468483
Train Epoch: 6 [25536/60000 (43%)]	Loss: 69.431641
Train Epoch: 6 [31936/60000 (53%)]	Loss: 76.289642
Train Epoch: 6 [38336/60000 (64%)]	Loss: 71.901329
Train Epoch: 6 [44736/60000 (75%)]	Loss: 69.550674
Train Epoch: 6 [51136/60000 (85%)]	Loss: 63.566357
Train Epoch: 6 [57536/60000 (96%)]	Loss: 67.221069

Test set: Average loss: 70.4423, Accuracy: 2688/10000 (27%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 68.365112
Train Epoch: 7 [12736/60000 (21%)]	Loss: 66.816078
Train Epoch: 7 [19136/60000 (32%)]	Loss: 86.564331
Train Epoch: 7 [25536/60000 (43%)]	Loss: 75.883926
Train Epoch: 7 [31936/60000 (53%)]	Loss: 65.624748
Train Epoch: 7 [38336/60000 (64%)]	Loss: 71.684227
Train Epoch: 7 [44736/60000 (75%)]	Loss: 65.270004
Train Epoch: 7 [51136/60000 (85%)]	Loss: 61.069927
Train Epoch: 7 [57536/60000 (96%)]	Loss: 79.889648

Test set: Average loss: 71.1754, Accuracy: 2499/10000 (25%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 68.187752
Train Epoch: 8 [12736/60000 (21%)]	Loss: 53.905624
Train Epoch: 8 [19136/60000 (32%)]	Loss: 58.598877
Train Epoch: 8 [25536/60000 (43%)]	Loss: 69.846329
Train Epoch: 8 [31936/60000 (53%)]	Loss: 68.765656
Train Epoch: 8 [38336/60000 (64%)]	Loss: 63.002548
Train Epoch: 8 [44736/60000 (75%)]	Loss: 88.623314
Train Epoch: 8 [51136/60000 (85%)]	Loss: 65.117905
Train Epoch: 8 [57536/60000 (96%)]	Loss: 56.610001

Test set: Average loss: 72.2594, Accuracy: 2080/10000 (21%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 65.585152
Train Epoch: 9 [12736/60000 (21%)]	Loss: 62.591953
Train Epoch: 9 [19136/60000 (32%)]	Loss: 77.773743
Train Epoch: 9 [25536/60000 (43%)]	Loss: 61.313496
Train Epoch: 9 [31936/60000 (53%)]	Loss: 59.282730
Train Epoch: 9 [38336/60000 (64%)]	Loss: 75.866837
Train Epoch: 9 [44736/60000 (75%)]	Loss: 62.113457
Train Epoch: 9 [51136/60000 (85%)]	Loss: 66.577843
Train Epoch: 9 [57536/60000 (96%)]	Loss: 65.348633

Test set: Average loss: 68.4383, Accuracy: 2225/10000 (22%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 76.804710
Train Epoch: 10 [12736/60000 (21%)]	Loss: 73.854790
Train Epoch: 10 [19136/60000 (32%)]	Loss: 60.254467
Train Epoch: 10 [25536/60000 (43%)]	Loss: 73.266586
Train Epoch: 10 [31936/60000 (53%)]	Loss: 94.694763
Train Epoch: 10 [38336/60000 (64%)]	Loss: 66.423058
Train Epoch: 10 [44736/60000 (75%)]	Loss: 76.096214
Train Epoch: 10 [51136/60000 (85%)]	Loss: 86.249733
Train Epoch: 10 [57536/60000 (96%)]	Loss: 74.423218

Test set: Average loss: 71.4834, Accuracy: 1937/10000 (19%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 62.412998
Train Epoch: 11 [12736/60000 (21%)]	Loss: 70.697945
Train Epoch: 11 [19136/60000 (32%)]	Loss: 75.617630
Train Epoch: 11 [25536/60000 (43%)]	Loss: 60.626083
Train Epoch: 11 [31936/60000 (53%)]	Loss: 64.251816
Train Epoch: 11 [38336/60000 (64%)]	Loss: 58.006725
Train Epoch: 11 [44736/60000 (75%)]	Loss: 60.595737
Train Epoch: 11 [51136/60000 (85%)]	Loss: 72.816154
Train Epoch: 11 [57536/60000 (96%)]	Loss: 69.775948

Test set: Average loss: 70.4071, Accuracy: 1927/10000 (19%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 58.143929
Train Epoch: 12 [12736/60000 (21%)]	Loss: 69.635864
Train Epoch: 12 [19136/60000 (32%)]	Loss: 83.402992
Train Epoch: 12 [25536/60000 (43%)]	Loss: 74.820961
Train Epoch: 12 [31936/60000 (53%)]	Loss: 66.632904
Train Epoch: 12 [38336/60000 (64%)]	Loss: 60.913605
Train Epoch: 12 [44736/60000 (75%)]	Loss: 67.707062
Train Epoch: 12 [51136/60000 (85%)]	Loss: 85.548241
Train Epoch: 12 [57536/60000 (96%)]	Loss: 64.311714

Test set: Average loss: 60.4653, Accuracy: 2642/10000 (26%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=12, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.40 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4384009
