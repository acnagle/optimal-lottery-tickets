Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 11.195581
Train Epoch: 1 [12736/60000 (21%)]	Loss: 11.993148
Train Epoch: 1 [19136/60000 (32%)]	Loss: 18.973293
Train Epoch: 1 [25536/60000 (43%)]	Loss: 13.737168
Train Epoch: 1 [31936/60000 (53%)]	Loss: 21.636826
Train Epoch: 1 [38336/60000 (64%)]	Loss: 23.369637
Train Epoch: 1 [44736/60000 (75%)]	Loss: 18.424709
Train Epoch: 1 [51136/60000 (85%)]	Loss: 25.789642
Train Epoch: 1 [57536/60000 (96%)]	Loss: 34.440708

Test set: Average loss: 33.2640, Accuracy: 5096/10000 (51%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 44.837105
Train Epoch: 2 [12736/60000 (21%)]	Loss: 24.956049
Train Epoch: 2 [19136/60000 (32%)]	Loss: 39.412968
Train Epoch: 2 [25536/60000 (43%)]	Loss: 35.531605
Train Epoch: 2 [31936/60000 (53%)]	Loss: 35.745670
Train Epoch: 2 [38336/60000 (64%)]	Loss: 48.564762
Train Epoch: 2 [44736/60000 (75%)]	Loss: 37.346512
Train Epoch: 2 [51136/60000 (85%)]	Loss: 50.462215
Train Epoch: 2 [57536/60000 (96%)]	Loss: 43.916241

Test set: Average loss: 47.8103, Accuracy: 4069/10000 (41%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 45.748077
Train Epoch: 3 [12736/60000 (21%)]	Loss: 48.206673
Train Epoch: 3 [19136/60000 (32%)]	Loss: 54.788975
Train Epoch: 3 [25536/60000 (43%)]	Loss: 57.499870
Train Epoch: 3 [31936/60000 (53%)]	Loss: 47.535664
Train Epoch: 3 [38336/60000 (64%)]	Loss: 46.419975
Train Epoch: 3 [44736/60000 (75%)]	Loss: 44.348217
Train Epoch: 3 [51136/60000 (85%)]	Loss: 56.385841
Train Epoch: 3 [57536/60000 (96%)]	Loss: 61.040455

Test set: Average loss: 53.9917, Accuracy: 3355/10000 (34%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 50.229263
Train Epoch: 4 [12736/60000 (21%)]	Loss: 79.135483
Train Epoch: 4 [19136/60000 (32%)]	Loss: 49.278870
Train Epoch: 4 [25536/60000 (43%)]	Loss: 63.551384
Train Epoch: 4 [31936/60000 (53%)]	Loss: 58.932842
Train Epoch: 4 [38336/60000 (64%)]	Loss: 69.691078
Train Epoch: 4 [44736/60000 (75%)]	Loss: 57.077190
Train Epoch: 4 [51136/60000 (85%)]	Loss: 61.378120
Train Epoch: 4 [57536/60000 (96%)]	Loss: 47.412323

Test set: Average loss: 64.8411, Accuracy: 2719/10000 (27%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 66.728333
Train Epoch: 5 [12736/60000 (21%)]	Loss: 64.249847
Train Epoch: 5 [19136/60000 (32%)]	Loss: 71.537628
Train Epoch: 5 [25536/60000 (43%)]	Loss: 61.998013
Train Epoch: 5 [31936/60000 (53%)]	Loss: 66.482468
Train Epoch: 5 [38336/60000 (64%)]	Loss: 58.637474
Train Epoch: 5 [44736/60000 (75%)]	Loss: 63.602737
Train Epoch: 5 [51136/60000 (85%)]	Loss: 61.081982
Train Epoch: 5 [57536/60000 (96%)]	Loss: 73.419914

Test set: Average loss: 64.1738, Accuracy: 2353/10000 (24%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 60.244576
Train Epoch: 6 [12736/60000 (21%)]	Loss: 61.097164
Train Epoch: 6 [19136/60000 (32%)]	Loss: 70.991844
Train Epoch: 6 [25536/60000 (43%)]	Loss: 72.682480
Train Epoch: 6 [31936/60000 (53%)]	Loss: 85.217430
Train Epoch: 6 [38336/60000 (64%)]	Loss: 66.476929
Train Epoch: 6 [44736/60000 (75%)]	Loss: 69.253952
Train Epoch: 6 [51136/60000 (85%)]	Loss: 60.582695
Train Epoch: 6 [57536/60000 (96%)]	Loss: 67.408531

Test set: Average loss: 65.7236, Accuracy: 2882/10000 (29%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 69.516457
Train Epoch: 7 [12736/60000 (21%)]	Loss: 66.847565
Train Epoch: 7 [19136/60000 (32%)]	Loss: 78.846901
Train Epoch: 7 [25536/60000 (43%)]	Loss: 54.694489
Train Epoch: 7 [31936/60000 (53%)]	Loss: 64.745033
Train Epoch: 7 [38336/60000 (64%)]	Loss: 84.144646
Train Epoch: 7 [44736/60000 (75%)]	Loss: 59.829254
Train Epoch: 7 [51136/60000 (85%)]	Loss: 69.202354
Train Epoch: 7 [57536/60000 (96%)]	Loss: 80.430984

Test set: Average loss: 71.1020, Accuracy: 2556/10000 (26%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 67.753914
Train Epoch: 8 [12736/60000 (21%)]	Loss: 59.894230
Train Epoch: 8 [19136/60000 (32%)]	Loss: 67.048241
Train Epoch: 8 [25536/60000 (43%)]	Loss: 68.532303
Train Epoch: 8 [31936/60000 (53%)]	Loss: 57.331543
Train Epoch: 8 [38336/60000 (64%)]	Loss: 63.258709
Train Epoch: 8 [44736/60000 (75%)]	Loss: 75.602211
Train Epoch: 8 [51136/60000 (85%)]	Loss: 69.095078
Train Epoch: 8 [57536/60000 (96%)]	Loss: 61.779610

Test set: Average loss: 69.4260, Accuracy: 2601/10000 (26%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 70.559189
Train Epoch: 9 [12736/60000 (21%)]	Loss: 65.325562
Train Epoch: 9 [19136/60000 (32%)]	Loss: 76.745758
Train Epoch: 9 [25536/60000 (43%)]	Loss: 60.223640
Train Epoch: 9 [31936/60000 (53%)]	Loss: 64.404602
Train Epoch: 9 [38336/60000 (64%)]	Loss: 67.654129
Train Epoch: 9 [44736/60000 (75%)]	Loss: 65.523682
Train Epoch: 9 [51136/60000 (85%)]	Loss: 66.465027
Train Epoch: 9 [57536/60000 (96%)]	Loss: 74.622307

Test set: Average loss: 69.5964, Accuracy: 2421/10000 (24%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 81.080231
Train Epoch: 10 [12736/60000 (21%)]	Loss: 67.848404
Train Epoch: 10 [19136/60000 (32%)]	Loss: 64.968201
Train Epoch: 10 [25536/60000 (43%)]	Loss: 79.318764
Train Epoch: 10 [31936/60000 (53%)]	Loss: 89.868797
Train Epoch: 10 [38336/60000 (64%)]	Loss: 65.370316
Train Epoch: 10 [44736/60000 (75%)]	Loss: 68.314453
Train Epoch: 10 [51136/60000 (85%)]	Loss: 87.432327
Train Epoch: 10 [57536/60000 (96%)]	Loss: 58.745277

Test set: Average loss: 71.4299, Accuracy: 2202/10000 (22%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 59.523083
Train Epoch: 11 [12736/60000 (21%)]	Loss: 71.866882
Train Epoch: 11 [19136/60000 (32%)]	Loss: 77.471497
Train Epoch: 11 [25536/60000 (43%)]	Loss: 62.182758
Train Epoch: 11 [31936/60000 (53%)]	Loss: 60.119644
Train Epoch: 11 [38336/60000 (64%)]	Loss: 65.324013
Train Epoch: 11 [44736/60000 (75%)]	Loss: 51.327541
Train Epoch: 11 [51136/60000 (85%)]	Loss: 72.645149
Train Epoch: 11 [57536/60000 (96%)]	Loss: 70.412056

Test set: Average loss: 73.6421, Accuracy: 1810/10000 (18%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 68.598358
Train Epoch: 12 [12736/60000 (21%)]	Loss: 83.669067
Train Epoch: 12 [19136/60000 (32%)]	Loss: 89.963531
Train Epoch: 12 [25536/60000 (43%)]	Loss: 72.754456
Train Epoch: 12 [31936/60000 (53%)]	Loss: 65.488083
Train Epoch: 12 [38336/60000 (64%)]	Loss: 65.825760
Train Epoch: 12 [44736/60000 (75%)]	Loss: 73.339302
Train Epoch: 12 [51136/60000 (85%)]	Loss: 86.324211
Train Epoch: 12 [57536/60000 (96%)]	Loss: 69.517418

Test set: Average loss: 76.2887, Accuracy: 1928/10000 (19%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 76.500877
Train Epoch: 13 [12736/60000 (21%)]	Loss: 62.948479
Train Epoch: 13 [19136/60000 (32%)]	Loss: 109.222351
Train Epoch: 13 [25536/60000 (43%)]	Loss: 62.357143
Train Epoch: 13 [31936/60000 (53%)]	Loss: 77.581459
Train Epoch: 13 [38336/60000 (64%)]	Loss: 64.908081
Train Epoch: 13 [44736/60000 (75%)]	Loss: 59.381134
Train Epoch: 13 [51136/60000 (85%)]	Loss: 54.856491
Train Epoch: 13 [57536/60000 (96%)]	Loss: 71.238701

Test set: Average loss: 69.3481, Accuracy: 2455/10000 (25%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 69.315018
Train Epoch: 14 [12736/60000 (21%)]	Loss: 64.716537
Train Epoch: 14 [19136/60000 (32%)]	Loss: 88.749954
Train Epoch: 14 [25536/60000 (43%)]	Loss: 69.556534
Train Epoch: 14 [31936/60000 (53%)]	Loss: 73.407120
Train Epoch: 14 [38336/60000 (64%)]	Loss: 64.331963
Train Epoch: 14 [44736/60000 (75%)]	Loss: 64.619049
Train Epoch: 14 [51136/60000 (85%)]	Loss: 81.423347
Train Epoch: 14 [57536/60000 (96%)]	Loss: 70.084732

Test set: Average loss: 71.3213, Accuracy: 2173/10000 (22%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.80 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4384009
