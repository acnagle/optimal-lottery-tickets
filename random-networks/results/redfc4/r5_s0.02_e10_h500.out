Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 7.253252
Train Epoch: 1 [12736/60000 (21%)]	Loss: 25.429331
Train Epoch: 1 [19136/60000 (32%)]	Loss: 34.917747
Train Epoch: 1 [25536/60000 (43%)]	Loss: 38.026100
Train Epoch: 1 [31936/60000 (53%)]	Loss: 29.555229
Train Epoch: 1 [38336/60000 (64%)]	Loss: 27.226795
Train Epoch: 1 [44736/60000 (75%)]	Loss: 33.277142
Train Epoch: 1 [51136/60000 (85%)]	Loss: 45.097240
Train Epoch: 1 [57536/60000 (96%)]	Loss: 48.676105

Test set: Average loss: 47.5576, Accuracy: 3854/10000 (39%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 60.837524
Train Epoch: 2 [12736/60000 (21%)]	Loss: 46.819096
Train Epoch: 2 [19136/60000 (32%)]	Loss: 53.940880
Train Epoch: 2 [25536/60000 (43%)]	Loss: 47.741207
Train Epoch: 2 [31936/60000 (53%)]	Loss: 52.887699
Train Epoch: 2 [38336/60000 (64%)]	Loss: 63.437321
Train Epoch: 2 [44736/60000 (75%)]	Loss: 59.671200
Train Epoch: 2 [51136/60000 (85%)]	Loss: 64.121605
Train Epoch: 2 [57536/60000 (96%)]	Loss: 66.604805

Test set: Average loss: 74.6345, Accuracy: 2158/10000 (22%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 69.482040
Train Epoch: 3 [12736/60000 (21%)]	Loss: 74.676941
Train Epoch: 3 [19136/60000 (32%)]	Loss: 80.097786
Train Epoch: 3 [25536/60000 (43%)]	Loss: 88.438751
Train Epoch: 3 [31936/60000 (53%)]	Loss: 63.525475
Train Epoch: 3 [38336/60000 (64%)]	Loss: 62.562027
Train Epoch: 3 [44736/60000 (75%)]	Loss: 69.341637
Train Epoch: 3 [51136/60000 (85%)]	Loss: 81.468307
Train Epoch: 3 [57536/60000 (96%)]	Loss: 83.042374

Test set: Average loss: 78.1423, Accuracy: 2257/10000 (23%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 73.013985
Train Epoch: 4 [12736/60000 (21%)]	Loss: 112.900146
Train Epoch: 4 [19136/60000 (32%)]	Loss: 70.716484
Train Epoch: 4 [25536/60000 (43%)]	Loss: 85.713676
Train Epoch: 4 [31936/60000 (53%)]	Loss: 90.452454
Train Epoch: 4 [38336/60000 (64%)]	Loss: 93.799507
Train Epoch: 4 [44736/60000 (75%)]	Loss: 79.509102
Train Epoch: 4 [51136/60000 (85%)]	Loss: 97.794975
Train Epoch: 4 [57536/60000 (96%)]	Loss: 66.595551

Test set: Average loss: 88.4267, Accuracy: 1828/10000 (18%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 87.018829
Train Epoch: 5 [12736/60000 (21%)]	Loss: 80.444237
Train Epoch: 5 [19136/60000 (32%)]	Loss: 84.189949
Train Epoch: 5 [25536/60000 (43%)]	Loss: 82.166245
Train Epoch: 5 [31936/60000 (53%)]	Loss: 92.059517
Train Epoch: 5 [38336/60000 (64%)]	Loss: 82.753113
Train Epoch: 5 [44736/60000 (75%)]	Loss: 89.355766
Train Epoch: 5 [51136/60000 (85%)]	Loss: 83.245216
Train Epoch: 5 [57536/60000 (96%)]	Loss: 104.359062

Test set: Average loss: 85.0086, Accuracy: 1788/10000 (18%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 82.242874
Train Epoch: 6 [12736/60000 (21%)]	Loss: 84.014511
Train Epoch: 6 [19136/60000 (32%)]	Loss: 103.839874
Train Epoch: 6 [25536/60000 (43%)]	Loss: 95.643967
Train Epoch: 6 [31936/60000 (53%)]	Loss: 105.643044
Train Epoch: 6 [38336/60000 (64%)]	Loss: 88.222366
Train Epoch: 6 [44736/60000 (75%)]	Loss: 83.729118
Train Epoch: 6 [51136/60000 (85%)]	Loss: 81.721245
Train Epoch: 6 [57536/60000 (96%)]	Loss: 98.527443

Test set: Average loss: 87.6844, Accuracy: 2051/10000 (21%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 91.407303
Train Epoch: 7 [12736/60000 (21%)]	Loss: 91.209198
Train Epoch: 7 [19136/60000 (32%)]	Loss: 96.682449
Train Epoch: 7 [25536/60000 (43%)]	Loss: 81.991859
Train Epoch: 7 [31936/60000 (53%)]	Loss: 91.723244
Train Epoch: 7 [38336/60000 (64%)]	Loss: 100.519753
Train Epoch: 7 [44736/60000 (75%)]	Loss: 85.552368
Train Epoch: 7 [51136/60000 (85%)]	Loss: 82.494560
Train Epoch: 7 [57536/60000 (96%)]	Loss: 102.319153

Test set: Average loss: 90.8302, Accuracy: 1703/10000 (17%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 90.296196
Train Epoch: 8 [12736/60000 (21%)]	Loss: 76.007683
Train Epoch: 8 [19136/60000 (32%)]	Loss: 88.260612
Train Epoch: 8 [25536/60000 (43%)]	Loss: 99.045525
Train Epoch: 8 [31936/60000 (53%)]	Loss: 88.061028
Train Epoch: 8 [38336/60000 (64%)]	Loss: 87.328278
Train Epoch: 8 [44736/60000 (75%)]	Loss: 90.659607
Train Epoch: 8 [51136/60000 (85%)]	Loss: 87.364319
Train Epoch: 8 [57536/60000 (96%)]	Loss: 79.908882

Test set: Average loss: 96.2416, Accuracy: 1618/10000 (16%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 86.119743
Train Epoch: 9 [12736/60000 (21%)]	Loss: 89.250381
Train Epoch: 9 [19136/60000 (32%)]	Loss: 101.701202
Train Epoch: 9 [25536/60000 (43%)]	Loss: 82.030609
Train Epoch: 9 [31936/60000 (53%)]	Loss: 90.138634
Train Epoch: 9 [38336/60000 (64%)]	Loss: 98.314842
Train Epoch: 9 [44736/60000 (75%)]	Loss: 96.491112
Train Epoch: 9 [51136/60000 (85%)]	Loss: 95.703682
Train Epoch: 9 [57536/60000 (96%)]	Loss: 99.626564

Test set: Average loss: 95.0879, Accuracy: 1482/10000 (15%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 87.283302
Train Epoch: 10 [12736/60000 (21%)]	Loss: 86.983925
Train Epoch: 10 [19136/60000 (32%)]	Loss: 86.388596
Train Epoch: 10 [25536/60000 (43%)]	Loss: 104.860596
Train Epoch: 10 [31936/60000 (53%)]	Loss: 120.841080
Train Epoch: 10 [38336/60000 (64%)]	Loss: 84.504723
Train Epoch: 10 [44736/60000 (75%)]	Loss: 98.352509
Train Epoch: 10 [51136/60000 (85%)]	Loss: 113.608559
Train Epoch: 10 [57536/60000 (96%)]	Loss: 86.852219

Test set: Average loss: 92.4153, Accuracy: 1597/10000 (16%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.00 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
