Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.904168
Train Epoch: 1 [12736/60000 (21%)]	Loss: 3.295049
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.552685
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.573287
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.663903
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.334898
Train Epoch: 1 [44736/60000 (75%)]	Loss: 4.441846
Train Epoch: 1 [51136/60000 (85%)]	Loss: 3.991321
Train Epoch: 1 [57536/60000 (96%)]	Loss: 3.708132

Test set: Average loss: 4.7151, Accuracy: 5944/10000 (59%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 8.784354
Train Epoch: 2 [12736/60000 (21%)]	Loss: 4.352473
Train Epoch: 2 [19136/60000 (32%)]	Loss: 6.064019
Train Epoch: 2 [25536/60000 (43%)]	Loss: 6.389922
Train Epoch: 2 [31936/60000 (53%)]	Loss: 4.467470
Train Epoch: 2 [38336/60000 (64%)]	Loss: 8.255857
Train Epoch: 2 [44736/60000 (75%)]	Loss: 7.713917
Train Epoch: 2 [51136/60000 (85%)]	Loss: 6.968591
Train Epoch: 2 [57536/60000 (96%)]	Loss: 5.740610

Test set: Average loss: 7.4306, Accuracy: 4898/10000 (49%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 7.798034
Train Epoch: 3 [12736/60000 (21%)]	Loss: 8.626045
Train Epoch: 3 [19136/60000 (32%)]	Loss: 4.804486
Train Epoch: 3 [25536/60000 (43%)]	Loss: 6.699897
Train Epoch: 3 [31936/60000 (53%)]	Loss: 7.697816
Train Epoch: 3 [38336/60000 (64%)]	Loss: 10.551217
Train Epoch: 3 [44736/60000 (75%)]	Loss: 10.629818
Train Epoch: 3 [51136/60000 (85%)]	Loss: 8.704993
Train Epoch: 3 [57536/60000 (96%)]	Loss: 11.821648

Test set: Average loss: 12.6181, Accuracy: 3262/10000 (33%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 10.713306
Train Epoch: 4 [12736/60000 (21%)]	Loss: 15.301556
Train Epoch: 4 [19136/60000 (32%)]	Loss: 12.122379
Train Epoch: 4 [25536/60000 (43%)]	Loss: 16.420002
Train Epoch: 4 [31936/60000 (53%)]	Loss: 14.493128
Train Epoch: 4 [38336/60000 (64%)]	Loss: 12.285293
Train Epoch: 4 [44736/60000 (75%)]	Loss: 16.407721
Train Epoch: 4 [51136/60000 (85%)]	Loss: 14.177992
Train Epoch: 4 [57536/60000 (96%)]	Loss: 10.474739

Test set: Average loss: 14.8440, Accuracy: 2567/10000 (26%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 15.865659
Train Epoch: 5 [12736/60000 (21%)]	Loss: 12.846877
Train Epoch: 5 [19136/60000 (32%)]	Loss: 13.292955
Train Epoch: 5 [25536/60000 (43%)]	Loss: 14.758006
Train Epoch: 5 [31936/60000 (53%)]	Loss: 18.279791
Train Epoch: 5 [38336/60000 (64%)]	Loss: 16.679165
Train Epoch: 5 [44736/60000 (75%)]	Loss: 17.755016
Train Epoch: 5 [51136/60000 (85%)]	Loss: 18.633728
Train Epoch: 5 [57536/60000 (96%)]	Loss: 21.493996

Test set: Average loss: 18.1956, Accuracy: 2225/10000 (22%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 16.484848
Train Epoch: 6 [12736/60000 (21%)]	Loss: 18.138483
Train Epoch: 6 [19136/60000 (32%)]	Loss: 19.748348
Train Epoch: 6 [25536/60000 (43%)]	Loss: 17.254511
Train Epoch: 6 [31936/60000 (53%)]	Loss: 16.645691
Train Epoch: 6 [38336/60000 (64%)]	Loss: 18.054157
Train Epoch: 6 [44736/60000 (75%)]	Loss: 21.973612
Train Epoch: 6 [51136/60000 (85%)]	Loss: 20.448750
Train Epoch: 6 [57536/60000 (96%)]	Loss: 21.568577

Test set: Average loss: 19.3482, Accuracy: 2087/10000 (21%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 19.200336
Train Epoch: 7 [12736/60000 (21%)]	Loss: 15.806956
Train Epoch: 7 [19136/60000 (32%)]	Loss: 17.725195
Train Epoch: 7 [25536/60000 (43%)]	Loss: 19.430326
Train Epoch: 7 [31936/60000 (53%)]	Loss: 20.649170
Train Epoch: 7 [38336/60000 (64%)]	Loss: 18.643238
Train Epoch: 7 [44736/60000 (75%)]	Loss: 18.425556
Train Epoch: 7 [51136/60000 (85%)]	Loss: 17.769915
Train Epoch: 7 [57536/60000 (96%)]	Loss: 19.991735

Test set: Average loss: 19.3516, Accuracy: 2051/10000 (21%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 19.318800
Train Epoch: 8 [12736/60000 (21%)]	Loss: 12.485666
Train Epoch: 8 [19136/60000 (32%)]	Loss: 21.239801
Train Epoch: 8 [25536/60000 (43%)]	Loss: 18.093750
Train Epoch: 8 [31936/60000 (53%)]	Loss: 17.818914
Train Epoch: 8 [38336/60000 (64%)]	Loss: 20.545191
Train Epoch: 8 [44736/60000 (75%)]	Loss: 17.982050
Train Epoch: 8 [51136/60000 (85%)]	Loss: 20.133070
Train Epoch: 8 [57536/60000 (96%)]	Loss: 20.389252

Test set: Average loss: 18.8557, Accuracy: 2000/10000 (20%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 20.054016
Train Epoch: 9 [12736/60000 (21%)]	Loss: 17.577608
Train Epoch: 9 [19136/60000 (32%)]	Loss: 20.086130
Train Epoch: 9 [25536/60000 (43%)]	Loss: 15.259933
Train Epoch: 9 [31936/60000 (53%)]	Loss: 17.004253
Train Epoch: 9 [38336/60000 (64%)]	Loss: 19.576651
Train Epoch: 9 [44736/60000 (75%)]	Loss: 19.473574
Train Epoch: 9 [51136/60000 (85%)]	Loss: 15.970205
Train Epoch: 9 [57536/60000 (96%)]	Loss: 20.704119

Test set: Average loss: 20.4477, Accuracy: 1867/10000 (19%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 18.880955
Train Epoch: 10 [12736/60000 (21%)]	Loss: 17.623713
Train Epoch: 10 [19136/60000 (32%)]	Loss: 19.440880
Train Epoch: 10 [25536/60000 (43%)]	Loss: 15.983119
Train Epoch: 10 [31936/60000 (53%)]	Loss: 20.467094
Train Epoch: 10 [38336/60000 (64%)]	Loss: 18.160992
Train Epoch: 10 [44736/60000 (75%)]	Loss: 20.276701
Train Epoch: 10 [51136/60000 (85%)]	Loss: 17.488520
Train Epoch: 10 [57536/60000 (96%)]	Loss: 17.196222

Test set: Average loss: 19.1270, Accuracy: 2083/10000 (21%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005)


Total time spent pruning/training: 2.03 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
