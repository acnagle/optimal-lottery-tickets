Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 7.253252
Train Epoch: 1 [12736/60000 (21%)]	Loss: 25.429331
Train Epoch: 1 [19136/60000 (32%)]	Loss: 34.917747
Train Epoch: 1 [25536/60000 (43%)]	Loss: 38.026100
Train Epoch: 1 [31936/60000 (53%)]	Loss: 29.555229
Train Epoch: 1 [38336/60000 (64%)]	Loss: 27.226795
Train Epoch: 1 [44736/60000 (75%)]	Loss: 33.277142
Train Epoch: 1 [51136/60000 (85%)]	Loss: 45.097240
Train Epoch: 1 [57536/60000 (96%)]	Loss: 48.676105

Test set: Average loss: 47.5576, Accuracy: 3854/10000 (39%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 55.598759
Train Epoch: 2 [12736/60000 (21%)]	Loss: 43.471508
Train Epoch: 2 [19136/60000 (32%)]	Loss: 64.476051
Train Epoch: 2 [25536/60000 (43%)]	Loss: 61.328350
Train Epoch: 2 [31936/60000 (53%)]	Loss: 50.860725
Train Epoch: 2 [38336/60000 (64%)]	Loss: 64.591248
Train Epoch: 2 [44736/60000 (75%)]	Loss: 64.990234
Train Epoch: 2 [51136/60000 (85%)]	Loss: 61.482059
Train Epoch: 2 [57536/60000 (96%)]	Loss: 60.432404

Test set: Average loss: 71.2254, Accuracy: 2205/10000 (22%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 61.776978
Train Epoch: 3 [12736/60000 (21%)]	Loss: 78.355804
Train Epoch: 3 [19136/60000 (32%)]	Loss: 86.669937
Train Epoch: 3 [25536/60000 (43%)]	Loss: 85.706779
Train Epoch: 3 [31936/60000 (53%)]	Loss: 67.357063
Train Epoch: 3 [38336/60000 (64%)]	Loss: 76.374390
Train Epoch: 3 [44736/60000 (75%)]	Loss: 71.808678
Train Epoch: 3 [51136/60000 (85%)]	Loss: 81.435371
Train Epoch: 3 [57536/60000 (96%)]	Loss: 79.754677

Test set: Average loss: 78.1700, Accuracy: 2362/10000 (24%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 68.580917
Train Epoch: 4 [12736/60000 (21%)]	Loss: 113.540497
Train Epoch: 4 [19136/60000 (32%)]	Loss: 70.424492
Train Epoch: 4 [25536/60000 (43%)]	Loss: 94.914566
Train Epoch: 4 [31936/60000 (53%)]	Loss: 91.778175
Train Epoch: 4 [38336/60000 (64%)]	Loss: 95.069977
Train Epoch: 4 [44736/60000 (75%)]	Loss: 84.279152
Train Epoch: 4 [51136/60000 (85%)]	Loss: 81.553452
Train Epoch: 4 [57536/60000 (96%)]	Loss: 64.304924

Test set: Average loss: 84.7167, Accuracy: 1997/10000 (20%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 93.750282
Train Epoch: 5 [12736/60000 (21%)]	Loss: 84.659454
Train Epoch: 5 [19136/60000 (32%)]	Loss: 92.085793
Train Epoch: 5 [25536/60000 (43%)]	Loss: 87.033691
Train Epoch: 5 [31936/60000 (53%)]	Loss: 90.424110
Train Epoch: 5 [38336/60000 (64%)]	Loss: 81.244522
Train Epoch: 5 [44736/60000 (75%)]	Loss: 90.871468
Train Epoch: 5 [51136/60000 (85%)]	Loss: 85.582512
Train Epoch: 5 [57536/60000 (96%)]	Loss: 100.362030

Test set: Average loss: 91.2280, Accuracy: 1600/10000 (16%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 77.184258
Train Epoch: 6 [12736/60000 (21%)]	Loss: 78.522118
Train Epoch: 6 [19136/60000 (32%)]	Loss: 102.549881
Train Epoch: 6 [25536/60000 (43%)]	Loss: 92.757698
Train Epoch: 6 [31936/60000 (53%)]	Loss: 108.566383
Train Epoch: 6 [38336/60000 (64%)]	Loss: 93.873077
Train Epoch: 6 [44736/60000 (75%)]	Loss: 90.641533
Train Epoch: 6 [51136/60000 (85%)]	Loss: 89.026512
Train Epoch: 6 [57536/60000 (96%)]	Loss: 92.190247

Test set: Average loss: 90.9060, Accuracy: 1848/10000 (18%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 89.358047
Train Epoch: 7 [12736/60000 (21%)]	Loss: 91.743507
Train Epoch: 7 [19136/60000 (32%)]	Loss: 104.480003
Train Epoch: 7 [25536/60000 (43%)]	Loss: 83.137482
Train Epoch: 7 [31936/60000 (53%)]	Loss: 97.731987
Train Epoch: 7 [38336/60000 (64%)]	Loss: 101.572563
Train Epoch: 7 [44736/60000 (75%)]	Loss: 94.133072
Train Epoch: 7 [51136/60000 (85%)]	Loss: 85.527000
Train Epoch: 7 [57536/60000 (96%)]	Loss: 100.490067

Test set: Average loss: 90.9479, Accuracy: 1686/10000 (17%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 95.028603
Train Epoch: 8 [12736/60000 (21%)]	Loss: 77.775925
Train Epoch: 8 [19136/60000 (32%)]	Loss: 91.387581
Train Epoch: 8 [25536/60000 (43%)]	Loss: 93.908600
Train Epoch: 8 [31936/60000 (53%)]	Loss: 90.642799
Train Epoch: 8 [38336/60000 (64%)]	Loss: 85.776039
Train Epoch: 8 [44736/60000 (75%)]	Loss: 98.012634
Train Epoch: 8 [51136/60000 (85%)]	Loss: 90.965279
Train Epoch: 8 [57536/60000 (96%)]	Loss: 77.732269

Test set: Average loss: 95.4878, Accuracy: 1412/10000 (14%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 89.696571
Train Epoch: 9 [12736/60000 (21%)]	Loss: 89.034988
Train Epoch: 9 [19136/60000 (32%)]	Loss: 106.029366
Train Epoch: 9 [25536/60000 (43%)]	Loss: 82.169151
Train Epoch: 9 [31936/60000 (53%)]	Loss: 85.496857
Train Epoch: 9 [38336/60000 (64%)]	Loss: 90.370750
Train Epoch: 9 [44736/60000 (75%)]	Loss: 101.624237
Train Epoch: 9 [51136/60000 (85%)]	Loss: 100.180809
Train Epoch: 9 [57536/60000 (96%)]	Loss: 103.764740

Test set: Average loss: 96.0350, Accuracy: 1637/10000 (16%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 103.022339
Train Epoch: 10 [12736/60000 (21%)]	Loss: 89.838440
Train Epoch: 10 [19136/60000 (32%)]	Loss: 85.403442
Train Epoch: 10 [25536/60000 (43%)]	Loss: 103.343605
Train Epoch: 10 [31936/60000 (53%)]	Loss: 125.009583
Train Epoch: 10 [38336/60000 (64%)]	Loss: 90.963226
Train Epoch: 10 [44736/60000 (75%)]	Loss: 88.499695
Train Epoch: 10 [51136/60000 (85%)]	Loss: 105.545654
Train Epoch: 10 [57536/60000 (96%)]	Loss: 84.861977

Test set: Average loss: 91.5048, Accuracy: 1519/10000 (15%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 81.395844
Train Epoch: 11 [12736/60000 (21%)]	Loss: 89.358864
Train Epoch: 11 [19136/60000 (32%)]	Loss: 101.307724
Train Epoch: 11 [25536/60000 (43%)]	Loss: 79.513382
Train Epoch: 11 [31936/60000 (53%)]	Loss: 88.886330
Train Epoch: 11 [38336/60000 (64%)]	Loss: 81.564835
Train Epoch: 11 [44736/60000 (75%)]	Loss: 83.598946
Train Epoch: 11 [51136/60000 (85%)]	Loss: 92.350304
Train Epoch: 11 [57536/60000 (96%)]	Loss: 100.513458

Test set: Average loss: 92.3141, Accuracy: 1498/10000 (15%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.20 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
