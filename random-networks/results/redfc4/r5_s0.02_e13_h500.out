Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 7.253252
Train Epoch: 1 [12736/60000 (21%)]	Loss: 25.429331
Train Epoch: 1 [19136/60000 (32%)]	Loss: 34.917747
Train Epoch: 1 [25536/60000 (43%)]	Loss: 38.026100
Train Epoch: 1 [31936/60000 (53%)]	Loss: 29.555229
Train Epoch: 1 [38336/60000 (64%)]	Loss: 27.226795
Train Epoch: 1 [44736/60000 (75%)]	Loss: 33.277142
Train Epoch: 1 [51136/60000 (85%)]	Loss: 45.097240
Train Epoch: 1 [57536/60000 (96%)]	Loss: 48.676105

Test set: Average loss: 47.5576, Accuracy: 3854/10000 (39%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 53.903706
Train Epoch: 2 [12736/60000 (21%)]	Loss: 44.157307
Train Epoch: 2 [19136/60000 (32%)]	Loss: 65.512024
Train Epoch: 2 [25536/60000 (43%)]	Loss: 50.455231
Train Epoch: 2 [31936/60000 (53%)]	Loss: 51.789501
Train Epoch: 2 [38336/60000 (64%)]	Loss: 72.548828
Train Epoch: 2 [44736/60000 (75%)]	Loss: 66.538017
Train Epoch: 2 [51136/60000 (85%)]	Loss: 65.114838
Train Epoch: 2 [57536/60000 (96%)]	Loss: 62.421787

Test set: Average loss: 71.6176, Accuracy: 2353/10000 (24%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 77.051811
Train Epoch: 3 [12736/60000 (21%)]	Loss: 75.298462
Train Epoch: 3 [19136/60000 (32%)]	Loss: 77.627899
Train Epoch: 3 [25536/60000 (43%)]	Loss: 88.763138
Train Epoch: 3 [31936/60000 (53%)]	Loss: 71.808319
Train Epoch: 3 [38336/60000 (64%)]	Loss: 82.725105
Train Epoch: 3 [44736/60000 (75%)]	Loss: 68.217064
Train Epoch: 3 [51136/60000 (85%)]	Loss: 79.709366
Train Epoch: 3 [57536/60000 (96%)]	Loss: 86.616760

Test set: Average loss: 72.2187, Accuracy: 2402/10000 (24%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 73.993378
Train Epoch: 4 [12736/60000 (21%)]	Loss: 104.983894
Train Epoch: 4 [19136/60000 (32%)]	Loss: 70.782784
Train Epoch: 4 [25536/60000 (43%)]	Loss: 90.502678
Train Epoch: 4 [31936/60000 (53%)]	Loss: 91.898483
Train Epoch: 4 [38336/60000 (64%)]	Loss: 96.507812
Train Epoch: 4 [44736/60000 (75%)]	Loss: 80.950317
Train Epoch: 4 [51136/60000 (85%)]	Loss: 87.989990
Train Epoch: 4 [57536/60000 (96%)]	Loss: 70.773338

Test set: Average loss: 85.6091, Accuracy: 2043/10000 (20%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 93.752449
Train Epoch: 5 [12736/60000 (21%)]	Loss: 94.254128
Train Epoch: 5 [19136/60000 (32%)]	Loss: 95.060135
Train Epoch: 5 [25536/60000 (43%)]	Loss: 84.333244
Train Epoch: 5 [31936/60000 (53%)]	Loss: 90.194496
Train Epoch: 5 [38336/60000 (64%)]	Loss: 81.423126
Train Epoch: 5 [44736/60000 (75%)]	Loss: 91.453285
Train Epoch: 5 [51136/60000 (85%)]	Loss: 87.934319
Train Epoch: 5 [57536/60000 (96%)]	Loss: 98.155609

Test set: Average loss: 92.4396, Accuracy: 1511/10000 (15%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 80.057198
Train Epoch: 6 [12736/60000 (21%)]	Loss: 87.783257
Train Epoch: 6 [19136/60000 (32%)]	Loss: 106.114136
Train Epoch: 6 [25536/60000 (43%)]	Loss: 90.613602
Train Epoch: 6 [31936/60000 (53%)]	Loss: 109.757095
Train Epoch: 6 [38336/60000 (64%)]	Loss: 96.239265
Train Epoch: 6 [44736/60000 (75%)]	Loss: 101.422279
Train Epoch: 6 [51136/60000 (85%)]	Loss: 87.012917
Train Epoch: 6 [57536/60000 (96%)]	Loss: 103.669373

Test set: Average loss: 95.9195, Accuracy: 1580/10000 (16%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 100.179512
Train Epoch: 7 [12736/60000 (21%)]	Loss: 95.326355
Train Epoch: 7 [19136/60000 (32%)]	Loss: 95.402718
Train Epoch: 7 [25536/60000 (43%)]	Loss: 87.300072
Train Epoch: 7 [31936/60000 (53%)]	Loss: 93.024841
Train Epoch: 7 [38336/60000 (64%)]	Loss: 99.089790
Train Epoch: 7 [44736/60000 (75%)]	Loss: 93.730354
Train Epoch: 7 [51136/60000 (85%)]	Loss: 89.111717
Train Epoch: 7 [57536/60000 (96%)]	Loss: 104.168373

Test set: Average loss: 89.4121, Accuracy: 1705/10000 (17%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 89.833969
Train Epoch: 8 [12736/60000 (21%)]	Loss: 76.645668
Train Epoch: 8 [19136/60000 (32%)]	Loss: 94.182007
Train Epoch: 8 [25536/60000 (43%)]	Loss: 98.536964
Train Epoch: 8 [31936/60000 (53%)]	Loss: 95.669518
Train Epoch: 8 [38336/60000 (64%)]	Loss: 86.274216
Train Epoch: 8 [44736/60000 (75%)]	Loss: 98.721695
Train Epoch: 8 [51136/60000 (85%)]	Loss: 92.662804
Train Epoch: 8 [57536/60000 (96%)]	Loss: 85.118385

Test set: Average loss: 95.0412, Accuracy: 1617/10000 (16%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 92.402168
Train Epoch: 9 [12736/60000 (21%)]	Loss: 87.335503
Train Epoch: 9 [19136/60000 (32%)]	Loss: 99.640541
Train Epoch: 9 [25536/60000 (43%)]	Loss: 84.167641
Train Epoch: 9 [31936/60000 (53%)]	Loss: 85.765450
Train Epoch: 9 [38336/60000 (64%)]	Loss: 93.904480
Train Epoch: 9 [44736/60000 (75%)]	Loss: 96.142006
Train Epoch: 9 [51136/60000 (85%)]	Loss: 100.789185
Train Epoch: 9 [57536/60000 (96%)]	Loss: 98.432747

Test set: Average loss: 96.6994, Accuracy: 1409/10000 (14%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 104.413132
Train Epoch: 10 [12736/60000 (21%)]	Loss: 93.939362
Train Epoch: 10 [19136/60000 (32%)]	Loss: 87.480904
Train Epoch: 10 [25536/60000 (43%)]	Loss: 106.068420
Train Epoch: 10 [31936/60000 (53%)]	Loss: 119.896645
Train Epoch: 10 [38336/60000 (64%)]	Loss: 90.823120
Train Epoch: 10 [44736/60000 (75%)]	Loss: 93.911942
Train Epoch: 10 [51136/60000 (85%)]	Loss: 108.808578
Train Epoch: 10 [57536/60000 (96%)]	Loss: 87.337273

Test set: Average loss: 92.5111, Accuracy: 1476/10000 (15%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 89.623566
Train Epoch: 11 [12736/60000 (21%)]	Loss: 91.766380
Train Epoch: 11 [19136/60000 (32%)]	Loss: 104.532455
Train Epoch: 11 [25536/60000 (43%)]	Loss: 77.060562
Train Epoch: 11 [31936/60000 (53%)]	Loss: 89.572243
Train Epoch: 11 [38336/60000 (64%)]	Loss: 86.743423
Train Epoch: 11 [44736/60000 (75%)]	Loss: 80.011162
Train Epoch: 11 [51136/60000 (85%)]	Loss: 96.950722
Train Epoch: 11 [57536/60000 (96%)]	Loss: 92.462250

Test set: Average loss: 99.5021, Accuracy: 1485/10000 (15%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 91.850853
Train Epoch: 12 [12736/60000 (21%)]	Loss: 102.711021
Train Epoch: 12 [19136/60000 (32%)]	Loss: 110.778725
Train Epoch: 12 [25536/60000 (43%)]	Loss: 103.043816
Train Epoch: 12 [31936/60000 (53%)]	Loss: 99.353653
Train Epoch: 12 [38336/60000 (64%)]	Loss: 93.867455
Train Epoch: 12 [44736/60000 (75%)]	Loss: 95.527039
Train Epoch: 12 [51136/60000 (85%)]	Loss: 106.885086
Train Epoch: 12 [57536/60000 (96%)]	Loss: 93.112328

Test set: Average loss: 94.8860, Accuracy: 1487/10000 (15%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 96.378792
Train Epoch: 13 [12736/60000 (21%)]	Loss: 84.541435
Train Epoch: 13 [19136/60000 (32%)]	Loss: 130.697449
Train Epoch: 13 [25536/60000 (43%)]	Loss: 89.176064
Train Epoch: 13 [31936/60000 (53%)]	Loss: 100.984085
Train Epoch: 13 [38336/60000 (64%)]	Loss: 92.902405
Train Epoch: 13 [44736/60000 (75%)]	Loss: 97.385223
Train Epoch: 13 [51136/60000 (85%)]	Loss: 81.111038
Train Epoch: 13 [57536/60000 (96%)]	Loss: 94.965698

Test set: Average loss: 91.9385, Accuracy: 1761/10000 (18%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.60 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
