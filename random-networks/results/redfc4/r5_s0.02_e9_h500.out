Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 7.253252
Train Epoch: 1 [12736/60000 (21%)]	Loss: 25.429331
Train Epoch: 1 [19136/60000 (32%)]	Loss: 34.917747
Train Epoch: 1 [25536/60000 (43%)]	Loss: 38.026100
Train Epoch: 1 [31936/60000 (53%)]	Loss: 29.555229
Train Epoch: 1 [38336/60000 (64%)]	Loss: 27.226795
Train Epoch: 1 [44736/60000 (75%)]	Loss: 33.277142
Train Epoch: 1 [51136/60000 (85%)]	Loss: 45.097240
Train Epoch: 1 [57536/60000 (96%)]	Loss: 48.676105

Test set: Average loss: 47.5576, Accuracy: 3854/10000 (39%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 55.023663
Train Epoch: 2 [12736/60000 (21%)]	Loss: 44.951374
Train Epoch: 2 [19136/60000 (32%)]	Loss: 58.303192
Train Epoch: 2 [25536/60000 (43%)]	Loss: 54.362408
Train Epoch: 2 [31936/60000 (53%)]	Loss: 47.838860
Train Epoch: 2 [38336/60000 (64%)]	Loss: 67.863800
Train Epoch: 2 [44736/60000 (75%)]	Loss: 58.379814
Train Epoch: 2 [51136/60000 (85%)]	Loss: 64.612640
Train Epoch: 2 [57536/60000 (96%)]	Loss: 62.117252

Test set: Average loss: 72.2485, Accuracy: 2370/10000 (24%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 71.160065
Train Epoch: 3 [12736/60000 (21%)]	Loss: 68.375359
Train Epoch: 3 [19136/60000 (32%)]	Loss: 74.429153
Train Epoch: 3 [25536/60000 (43%)]	Loss: 80.644241
Train Epoch: 3 [31936/60000 (53%)]	Loss: 63.148918
Train Epoch: 3 [38336/60000 (64%)]	Loss: 78.841110
Train Epoch: 3 [44736/60000 (75%)]	Loss: 73.167671
Train Epoch: 3 [51136/60000 (85%)]	Loss: 82.540268
Train Epoch: 3 [57536/60000 (96%)]	Loss: 83.645973

Test set: Average loss: 80.7113, Accuracy: 2276/10000 (23%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 75.947197
Train Epoch: 4 [12736/60000 (21%)]	Loss: 102.895157
Train Epoch: 4 [19136/60000 (32%)]	Loss: 72.325401
Train Epoch: 4 [25536/60000 (43%)]	Loss: 93.827271
Train Epoch: 4 [31936/60000 (53%)]	Loss: 91.621422
Train Epoch: 4 [38336/60000 (64%)]	Loss: 93.753479
Train Epoch: 4 [44736/60000 (75%)]	Loss: 80.113350
Train Epoch: 4 [51136/60000 (85%)]	Loss: 92.442123
Train Epoch: 4 [57536/60000 (96%)]	Loss: 71.223328

Test set: Average loss: 85.9088, Accuracy: 1930/10000 (19%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 87.706139
Train Epoch: 5 [12736/60000 (21%)]	Loss: 92.164642
Train Epoch: 5 [19136/60000 (32%)]	Loss: 86.746529
Train Epoch: 5 [25536/60000 (43%)]	Loss: 86.871544
Train Epoch: 5 [31936/60000 (53%)]	Loss: 95.803230
Train Epoch: 5 [38336/60000 (64%)]	Loss: 80.237282
Train Epoch: 5 [44736/60000 (75%)]	Loss: 89.898331
Train Epoch: 5 [51136/60000 (85%)]	Loss: 82.242355
Train Epoch: 5 [57536/60000 (96%)]	Loss: 106.489403

Test set: Average loss: 87.8649, Accuracy: 1817/10000 (18%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 83.030716
Train Epoch: 6 [12736/60000 (21%)]	Loss: 80.876846
Train Epoch: 6 [19136/60000 (32%)]	Loss: 110.004860
Train Epoch: 6 [25536/60000 (43%)]	Loss: 88.424194
Train Epoch: 6 [31936/60000 (53%)]	Loss: 107.772369
Train Epoch: 6 [38336/60000 (64%)]	Loss: 90.196274
Train Epoch: 6 [44736/60000 (75%)]	Loss: 83.583900
Train Epoch: 6 [51136/60000 (85%)]	Loss: 93.765938
Train Epoch: 6 [57536/60000 (96%)]	Loss: 88.423401

Test set: Average loss: 92.0107, Accuracy: 1671/10000 (17%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 90.725418
Train Epoch: 7 [12736/60000 (21%)]	Loss: 91.483292
Train Epoch: 7 [19136/60000 (32%)]	Loss: 99.948166
Train Epoch: 7 [25536/60000 (43%)]	Loss: 83.731552
Train Epoch: 7 [31936/60000 (53%)]	Loss: 91.696404
Train Epoch: 7 [38336/60000 (64%)]	Loss: 93.509682
Train Epoch: 7 [44736/60000 (75%)]	Loss: 83.864075
Train Epoch: 7 [51136/60000 (85%)]	Loss: 87.075836
Train Epoch: 7 [57536/60000 (96%)]	Loss: 102.855873

Test set: Average loss: 87.9702, Accuracy: 1734/10000 (17%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 89.896820
Train Epoch: 8 [12736/60000 (21%)]	Loss: 77.720032
Train Epoch: 8 [19136/60000 (32%)]	Loss: 89.284714
Train Epoch: 8 [25536/60000 (43%)]	Loss: 91.910652
Train Epoch: 8 [31936/60000 (53%)]	Loss: 77.611237
Train Epoch: 8 [38336/60000 (64%)]	Loss: 85.967033
Train Epoch: 8 [44736/60000 (75%)]	Loss: 91.364929
Train Epoch: 8 [51136/60000 (85%)]	Loss: 93.958412
Train Epoch: 8 [57536/60000 (96%)]	Loss: 73.625656

Test set: Average loss: 93.4580, Accuracy: 1656/10000 (17%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 81.570099
Train Epoch: 9 [12736/60000 (21%)]	Loss: 85.512566
Train Epoch: 9 [19136/60000 (32%)]	Loss: 97.702606
Train Epoch: 9 [25536/60000 (43%)]	Loss: 85.445541
Train Epoch: 9 [31936/60000 (53%)]	Loss: 79.677483
Train Epoch: 9 [38336/60000 (64%)]	Loss: 88.285736
Train Epoch: 9 [44736/60000 (75%)]	Loss: 100.173233
Train Epoch: 9 [51136/60000 (85%)]	Loss: 99.788322
Train Epoch: 9 [57536/60000 (96%)]	Loss: 98.296898

Test set: Average loss: 90.7756, Accuracy: 1706/10000 (17%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.80 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4406491
