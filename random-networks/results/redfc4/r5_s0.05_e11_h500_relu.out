Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=True, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.882387
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.633579
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.388350
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.669397
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.435292
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.469264
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.316151
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.631530
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.426363

Test set: Average loss: 0.3549, Accuracy: 8888/10000 (89%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.432224
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.285405
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.351288
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.509811
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.429024
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.633805
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.431675
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.539706
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.408101

Test set: Average loss: 0.4330, Accuracy: 8682/10000 (87%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.602577
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.141909
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.179355
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.542411
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.394423
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.286113
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.674519
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.325650
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.301014

Test set: Average loss: 0.5377, Accuracy: 8397/10000 (84%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.428650
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.690143
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.877889
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.550039
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.627777
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.446417
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.649810
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.498456
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.519862

Test set: Average loss: 0.8628, Accuracy: 7898/10000 (79%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.297689
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.476188
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.529090
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.685952
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.572931
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.147147
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.902788
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.293395
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.072498

Test set: Average loss: 2.1164, Accuracy: 6404/10000 (64%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.502497
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.154561
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.129491
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.051797
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.524930
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.958395
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.873815
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.993926
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.557111

Test set: Average loss: 1.6832, Accuracy: 7137/10000 (71%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.257271
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.442326
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.154788
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.014507
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.180241
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.149915
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.966167
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.378973
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.295711

Test set: Average loss: 2.0840, Accuracy: 6898/10000 (69%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.682138
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.444033
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.119021
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.936560
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.291756
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.369778
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.806842
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.910873
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.723776

Test set: Average loss: 1.3947, Accuracy: 7632/10000 (76%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.528986
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.222485
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.442385
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.859755
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.828296
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.803845
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.255053
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.006999
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.395190

Test set: Average loss: 1.2297, Accuracy: 7936/10000 (79%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.959796
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.808186
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.733845
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.278574
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.656759
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.963196
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.784984
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.096069
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.271758

Test set: Average loss: 1.0704, Accuracy: 7781/10000 (78%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.491878
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.400177
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.352293
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.436293
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.947711
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.463846
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.443263
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.569663
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.266015

Test set: Average loss: 0.4881, Accuracy: 8656/10000 (87%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=True, wd=0.0005)


Total time spent pruning/training: 2.23 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4271599
