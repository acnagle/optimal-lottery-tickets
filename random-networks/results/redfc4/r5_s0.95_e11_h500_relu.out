Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=True, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.302583
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.302449
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.302224
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.302287
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.301998
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.299843
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.300602
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.295126
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.287957

Test set: Average loss: 2.2887, Accuracy: 1160/10000 (12%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.275890
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.284786
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.293829
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.247950
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.237935
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.259013
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.243638
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.197606
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.149826

Test set: Average loss: 2.1832, Accuracy: 1744/10000 (17%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.124852
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.039825
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.093827
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.034154
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.041092
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.057669
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.047680
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.028839
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.023353

Test set: Average loss: 2.0124, Accuracy: 2695/10000 (27%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.999326
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.036506
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.965684
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.972807
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.996131
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.934413
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.955945
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.941793
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.909122

Test set: Average loss: 1.9435, Accuracy: 3355/10000 (34%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.965001
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.000486
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.893721
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.893638
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.898919
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.987292
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.901292
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.921106
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.974391

Test set: Average loss: 1.9262, Accuracy: 3629/10000 (36%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.936037
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.905628
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.912138
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.920042
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.906602
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.926646
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.915629
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.927456
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.958070

Test set: Average loss: 1.9042, Accuracy: 3356/10000 (34%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.898738
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.907835
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.936794
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.882873
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.938672
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.957527
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.884461
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.940600
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.927728

Test set: Average loss: 1.9010, Accuracy: 3849/10000 (38%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.872240
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.932912
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.866410
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.922954
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.876079
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.913858
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.873443
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.927612
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.870186

Test set: Average loss: 1.9016, Accuracy: 3432/10000 (34%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.941104
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.889020
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.893840
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.942289
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.965734
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.910212
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.936207
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.848698
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.876275

Test set: Average loss: 1.8846, Accuracy: 3953/10000 (40%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.855624
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.897103
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.885959
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.939297
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.899553
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.882558
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.948527
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.882197
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.836991

Test set: Average loss: 1.8791, Accuracy: 3931/10000 (39%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.883598
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.822564
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.866218
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.943169
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.919695
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.896687
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.881027
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.810013
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.938635

Test set: Average loss: 1.8787, Accuracy: 3828/10000 (38%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=True, wd=0.0005)


Total time spent pruning/training: 2.21 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 224821
