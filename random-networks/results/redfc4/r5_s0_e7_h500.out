Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=7, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.0, use_relu=False, wd=0.0005) 

Pruning a Four-Layer Fully Connected Redundant Network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 157.418182
Train Epoch: 1 [12736/60000 (21%)]	Loss: 230.465469
Train Epoch: 1 [19136/60000 (32%)]	Loss: 225.913086
Train Epoch: 1 [25536/60000 (43%)]	Loss: 201.067261
Train Epoch: 1 [31936/60000 (53%)]	Loss: 181.535690
Train Epoch: 1 [38336/60000 (64%)]	Loss: 215.293457
Train Epoch: 1 [44736/60000 (75%)]	Loss: 217.300156
Train Epoch: 1 [51136/60000 (85%)]	Loss: 192.630768
Train Epoch: 1 [57536/60000 (96%)]	Loss: 210.543350

Test set: Average loss: 205.6305, Accuracy: 1047/10000 (10%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 180.367004
Train Epoch: 2 [12736/60000 (21%)]	Loss: 213.071793
Train Epoch: 2 [19136/60000 (32%)]	Loss: 200.291168
Train Epoch: 2 [25536/60000 (43%)]	Loss: 211.101120
Train Epoch: 2 [31936/60000 (53%)]	Loss: 194.288696
Train Epoch: 2 [38336/60000 (64%)]	Loss: 219.276886
Train Epoch: 2 [44736/60000 (75%)]	Loss: 207.023071
Train Epoch: 2 [51136/60000 (85%)]	Loss: 230.074081
Train Epoch: 2 [57536/60000 (96%)]	Loss: 224.027695

Test set: Average loss: 205.6195, Accuracy: 1047/10000 (10%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 174.036102
Train Epoch: 3 [12736/60000 (21%)]	Loss: 213.826889
Train Epoch: 3 [19136/60000 (32%)]	Loss: 219.364822
Train Epoch: 3 [25536/60000 (43%)]	Loss: 223.442078
Train Epoch: 3 [31936/60000 (53%)]	Loss: 209.358154
Train Epoch: 3 [38336/60000 (64%)]	Loss: 201.746155
Train Epoch: 3 [44736/60000 (75%)]	Loss: 199.699829
Train Epoch: 3 [51136/60000 (85%)]	Loss: 190.701996
Train Epoch: 3 [57536/60000 (96%)]	Loss: 205.713120

Test set: Average loss: 205.7529, Accuracy: 1047/10000 (10%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 184.803558
Train Epoch: 4 [12736/60000 (21%)]	Loss: 238.878876
Train Epoch: 4 [19136/60000 (32%)]	Loss: 208.699936
Train Epoch: 4 [25536/60000 (43%)]	Loss: 219.471375
Train Epoch: 4 [31936/60000 (53%)]	Loss: 203.553497
Train Epoch: 4 [38336/60000 (64%)]	Loss: 218.683136
Train Epoch: 4 [44736/60000 (75%)]	Loss: 183.872879
Train Epoch: 4 [51136/60000 (85%)]	Loss: 207.569504
Train Epoch: 4 [57536/60000 (96%)]	Loss: 167.328613

Test set: Average loss: 205.3506, Accuracy: 1047/10000 (10%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 212.958664
Train Epoch: 5 [12736/60000 (21%)]	Loss: 200.323425
Train Epoch: 5 [19136/60000 (32%)]	Loss: 189.882965
Train Epoch: 5 [25536/60000 (43%)]	Loss: 202.514557
Train Epoch: 5 [31936/60000 (53%)]	Loss: 218.446594
Train Epoch: 5 [38336/60000 (64%)]	Loss: 192.091980
Train Epoch: 5 [44736/60000 (75%)]	Loss: 212.992462
Train Epoch: 5 [51136/60000 (85%)]	Loss: 193.908920
Train Epoch: 5 [57536/60000 (96%)]	Loss: 213.269791

Test set: Average loss: 205.5177, Accuracy: 1047/10000 (10%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 193.586884
Train Epoch: 6 [12736/60000 (21%)]	Loss: 186.157120
Train Epoch: 6 [19136/60000 (32%)]	Loss: 224.166183
Train Epoch: 6 [25536/60000 (43%)]	Loss: 199.816681
Train Epoch: 6 [31936/60000 (53%)]	Loss: 226.185654
Train Epoch: 6 [38336/60000 (64%)]	Loss: 207.062424
Train Epoch: 6 [44736/60000 (75%)]	Loss: 200.587830
Train Epoch: 6 [51136/60000 (85%)]	Loss: 216.682343
Train Epoch: 6 [57536/60000 (96%)]	Loss: 219.214325

Test set: Average loss: 205.5156, Accuracy: 1047/10000 (10%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 205.254974
Train Epoch: 7 [12736/60000 (21%)]	Loss: 210.182693
Train Epoch: 7 [19136/60000 (32%)]	Loss: 216.626648
Train Epoch: 7 [25536/60000 (43%)]	Loss: 197.557800
Train Epoch: 7 [31936/60000 (53%)]	Loss: 223.480637
Train Epoch: 7 [38336/60000 (64%)]	Loss: 217.818390
Train Epoch: 7 [44736/60000 (75%)]	Loss: 203.119781
Train Epoch: 7 [51136/60000 (85%)]	Loss: 191.747147
Train Epoch: 7 [57536/60000 (96%)]	Loss: 223.144241

Test set: Average loss: 205.6564, Accuracy: 1047/10000 (10%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=7, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='redfc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.0, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.39 minutes
Total number of parameters in model: 4496420
Number of parameters in pruned model: 4496420
