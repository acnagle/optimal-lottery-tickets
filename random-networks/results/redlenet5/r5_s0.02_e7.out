Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=7, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='redlenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Training a RedLeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.589047
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.203286
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.767450
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.981925
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.849517
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.697098
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.336214
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.568365
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.212293

Test set: Average loss: 1.8664, Accuracy: 5650/10000 (56%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.259139
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.368333
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.833094
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.306093
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.776852
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.907768
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.301005
Train Epoch: 2 [51136/60000 (85%)]	Loss: 3.694600
Train Epoch: 2 [57536/60000 (96%)]	Loss: 3.546901

Test set: Average loss: 3.5929, Accuracy: 4404/10000 (44%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 4.384635
Train Epoch: 3 [12736/60000 (21%)]	Loss: 3.353939
Train Epoch: 3 [19136/60000 (32%)]	Loss: 3.170849
Train Epoch: 3 [25536/60000 (43%)]	Loss: 5.248959
Train Epoch: 3 [31936/60000 (53%)]	Loss: 3.597277
Train Epoch: 3 [38336/60000 (64%)]	Loss: 5.342831
Train Epoch: 3 [44736/60000 (75%)]	Loss: 4.461957
Train Epoch: 3 [51136/60000 (85%)]	Loss: 3.926071
Train Epoch: 3 [57536/60000 (96%)]	Loss: 3.717531

Test set: Average loss: 5.1227, Accuracy: 3098/10000 (31%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 5.385238
Train Epoch: 4 [12736/60000 (21%)]	Loss: 5.713498
Train Epoch: 4 [19136/60000 (32%)]	Loss: 4.954782
Train Epoch: 4 [25536/60000 (43%)]	Loss: 4.317196
Train Epoch: 4 [31936/60000 (53%)]	Loss: 5.439633
Train Epoch: 4 [38336/60000 (64%)]	Loss: 4.008689
Train Epoch: 4 [44736/60000 (75%)]	Loss: 5.703409
Train Epoch: 4 [51136/60000 (85%)]	Loss: 4.989791
Train Epoch: 4 [57536/60000 (96%)]	Loss: 5.482138

Test set: Average loss: 6.1677, Accuracy: 2560/10000 (26%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 5.226231
Train Epoch: 5 [12736/60000 (21%)]	Loss: 4.895940
Train Epoch: 5 [19136/60000 (32%)]	Loss: 5.071301
Train Epoch: 5 [25536/60000 (43%)]	Loss: 7.016449
Train Epoch: 5 [31936/60000 (53%)]	Loss: 6.645984
Train Epoch: 5 [38336/60000 (64%)]	Loss: 6.111326
Train Epoch: 5 [44736/60000 (75%)]	Loss: 6.651206
Train Epoch: 5 [51136/60000 (85%)]	Loss: 6.513968
Train Epoch: 5 [57536/60000 (96%)]	Loss: 6.024120

Test set: Average loss: 6.8467, Accuracy: 2678/10000 (27%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 6.367024
Train Epoch: 6 [12736/60000 (21%)]	Loss: 5.747828
Train Epoch: 6 [19136/60000 (32%)]	Loss: 6.941104
Train Epoch: 6 [25536/60000 (43%)]	Loss: 7.605994
Train Epoch: 6 [31936/60000 (53%)]	Loss: 5.360917
Train Epoch: 6 [38336/60000 (64%)]	Loss: 7.142220
Train Epoch: 6 [44736/60000 (75%)]	Loss: 7.060678
Train Epoch: 6 [51136/60000 (85%)]	Loss: 7.596293
Train Epoch: 6 [57536/60000 (96%)]	Loss: 7.517439

Test set: Average loss: 7.4819, Accuracy: 2155/10000 (22%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 5.859423
Train Epoch: 7 [12736/60000 (21%)]	Loss: 7.207111
Train Epoch: 7 [19136/60000 (32%)]	Loss: 6.429076
Train Epoch: 7 [25536/60000 (43%)]	Loss: 6.761275
Train Epoch: 7 [31936/60000 (53%)]	Loss: 5.665210
Train Epoch: 7 [38336/60000 (64%)]	Loss: 6.419661
Train Epoch: 7 [44736/60000 (75%)]	Loss: 8.569287
Train Epoch: 7 [51136/60000 (85%)]	Loss: 8.447475
Train Epoch: 7 [57536/60000 (96%)]	Loss: 6.978171

Test set: Average loss: 6.5724, Accuracy: 2108/10000 (21%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=7, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='redlenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 0.75 minutes
Total number of parameters in model: 300170
Number of parameters in pruned model: 294218
