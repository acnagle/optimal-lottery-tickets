Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=9, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='redlenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005) 

Training a RedLeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 1.321551
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.169610
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.637318
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.712404
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.864735
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.819310
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.776016
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.740030
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.868175

Test set: Average loss: 0.6299, Accuracy: 7892/10000 (79%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.616196
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.571544
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.555345
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.633411
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.699996
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.896899
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.426136
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.675361
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.709621

Test set: Average loss: 0.6286, Accuracy: 8006/10000 (80%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.643699
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.640016
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.579175
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.460054
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.522580
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.546668
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.733870
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.728090
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.660339

Test set: Average loss: 0.6836, Accuracy: 7789/10000 (78%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.794761
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.619525
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.792954
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.669069
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.629695
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.492308
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.636582
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.564587
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.646025

Test set: Average loss: 0.7307, Accuracy: 7494/10000 (75%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.652168
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.749524
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.463889
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.759797
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.648503
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.604167
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.794599
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.741116
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.302779

Test set: Average loss: 0.5585, Accuracy: 8287/10000 (83%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.771159
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.392189
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.811095
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.953820
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.142530
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.661399
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.453271
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.644519
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.621031

Test set: Average loss: 0.6931, Accuracy: 7812/10000 (78%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.820498
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.056338
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.879123
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.674677
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.795248
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.710519
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.848222
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.799820
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.762018

Test set: Average loss: 0.7885, Accuracy: 7563/10000 (76%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.801504
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.739772
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.640430
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.622413
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.682715
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.533436
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.568507
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.783778
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.607882

Test set: Average loss: 1.0734, Accuracy: 6604/10000 (66%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.553697
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.803797
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.692493
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.507182
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.456232
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.685645
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.835473
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.827153
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.899887

Test set: Average loss: 0.6914, Accuracy: 7787/10000 (78%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=9, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='redlenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=True, wd=0.0005)


Total time spent pruning/training: 0.96 minutes
Total number of parameters in model: 300170
Number of parameters in pruned model: 294218
