Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.794131
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.097545
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.174879
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.264423
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.279152
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.400858
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.501635
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.769526
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.732135

Test set: Average loss: 1.6869, Accuracy: 4362/10000 (44%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.663583
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.789868
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.696390
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.970978
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.880596
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.825030
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.874796
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.225941
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.903176

Test set: Average loss: 2.0249, Accuracy: 3263/10000 (33%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.042891
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.216885
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.946088
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.155501
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.109516
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.951032
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.009708
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.122045
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.136280

Test set: Average loss: 2.1601, Accuracy: 2635/10000 (26%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.250890
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.010292
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.078466
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.011373
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.164699
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.200660
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.348160
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.231565
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.435228

Test set: Average loss: 2.2347, Accuracy: 2293/10000 (23%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.234724
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.293998
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.283259
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.294748
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.227811
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.356709
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.352453
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.042728
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.318193

Test set: Average loss: 2.2691, Accuracy: 2276/10000 (23%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.317003
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.236890
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.106330
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.291364
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.235048
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.259788
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.221682
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.243618
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.289900

Test set: Average loss: 2.2822, Accuracy: 2330/10000 (23%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.277254
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.332560
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.422456
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.216568
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.248755
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.342433
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.428346
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.149826
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.267925

Test set: Average loss: 2.3019, Accuracy: 2307/10000 (23%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.366045
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.392743
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.527346
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.366494
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.480120
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.338442
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.151093
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.329817
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.281870

Test set: Average loss: 2.3213, Accuracy: 2108/10000 (21%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.047235
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.129018
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.517756
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.493088
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.515854
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.470670
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.456301
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.560007
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.399602

Test set: Average loss: 2.3100, Accuracy: 2208/10000 (22%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.360249
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.248597
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.533407
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.228850
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.319613
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.518231
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.385417
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.331024
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.361092

Test set: Average loss: 2.3196, Accuracy: 2189/10000 (22%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.06 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1971438
