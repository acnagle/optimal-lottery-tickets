Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.794131
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.097545
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.174879
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.264423
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.279152
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.400858
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.501635
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.769526
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.732135

Test set: Average loss: 1.6869, Accuracy: 4362/10000 (44%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.661438
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.840516
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.725071
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.956290
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.890668
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.861801
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.867966
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.223451
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.914412

Test set: Average loss: 2.0388, Accuracy: 3198/10000 (32%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.045396
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.221912
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.939975
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.154948
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.112381
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.958925
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.042622
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.141990
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.150061

Test set: Average loss: 2.1689, Accuracy: 2610/10000 (26%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.250635
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.046503
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.056771
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.024617
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.176212
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.179060
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.349839
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.213839
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.461445

Test set: Average loss: 2.2369, Accuracy: 2312/10000 (23%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.232275
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.283278
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.295142
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.308198
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.245374
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.344461
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.334771
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.034982
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.318865

Test set: Average loss: 2.2741, Accuracy: 2253/10000 (23%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.307620
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.264102
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.114755
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.273150
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.225786
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.261433
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.230868
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.245818
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.300800

Test set: Average loss: 2.3013, Accuracy: 2243/10000 (22%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.275342
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.347102
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.433295
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.216148
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.238901
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.368135
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.433537
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.162737
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.261344

Test set: Average loss: 2.3290, Accuracy: 2249/10000 (22%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.381975
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.377372
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.542295
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.368941
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.479854
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.345414
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.149639
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.358520
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.273879

Test set: Average loss: 2.3228, Accuracy: 2114/10000 (21%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.058728
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.156771
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.516358
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.491214
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.533714
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.475212
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.469470
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.590605
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.412664

Test set: Average loss: 2.3307, Accuracy: 2108/10000 (21%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.371967
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.299434
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.544872
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.224109
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.343962
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.515228
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.406675
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.372716
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.372317

Test set: Average loss: 2.3352, Accuracy: 2104/10000 (21%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.147906
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.433562
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.394451
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.503136
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.439043
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.400239
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.439358
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.261873
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.333976

Test set: Average loss: 2.3503, Accuracy: 2042/10000 (20%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=11, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.15 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1971438
