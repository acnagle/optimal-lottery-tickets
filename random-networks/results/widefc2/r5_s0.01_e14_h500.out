Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.794131
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.097545
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.174879
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.264423
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.279152
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.400858
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.501635
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.769526
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.732135

Test set: Average loss: 1.6869, Accuracy: 4362/10000 (44%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.672589
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.823879
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.719607
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.983157
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.894323
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.865963
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.875331
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.221385
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.916650

Test set: Average loss: 2.0300, Accuracy: 3266/10000 (33%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.019575
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.246556
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.946057
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.157217
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.113600
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.977754
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.033364
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.150537
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.151184

Test set: Average loss: 2.1577, Accuracy: 2617/10000 (26%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.257979
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.042650
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.080246
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.008033
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.186308
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.175368
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.359419
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.221712
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.459671

Test set: Average loss: 2.2325, Accuracy: 2355/10000 (24%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.238974
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.289302
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.308977
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.305383
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.237671
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.362645
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.339976
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.071882
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.364037

Test set: Average loss: 2.2806, Accuracy: 2223/10000 (22%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.342112
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.252094
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.132573
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.295659
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.231140
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.273357
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.237916
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.253753
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.314046

Test set: Average loss: 2.3151, Accuracy: 2144/10000 (21%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.329237
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.352338
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.454180
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.236426
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.257612
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.388818
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.442590
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.161756
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.303884

Test set: Average loss: 2.3316, Accuracy: 2179/10000 (22%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.415531
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.408734
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.568880
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.395948
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.520379
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.359262
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.188620
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.356527
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.281896

Test set: Average loss: 2.3418, Accuracy: 1968/10000 (20%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.062169
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.161197
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.562323
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.542239
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.537815
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.480062
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.499148
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.603466
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.450752

Test set: Average loss: 2.3558, Accuracy: 2061/10000 (21%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.423173
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.296201
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.568380
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.245321
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.393381
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.557734
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.441769
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.389393
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.390988

Test set: Average loss: 2.3656, Accuracy: 1996/10000 (20%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.211216
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.494346
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.445780
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.542498
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.456368
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.436472
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.466905
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.254600
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.364439

Test set: Average loss: 2.3806, Accuracy: 2003/10000 (20%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.449447
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.383901
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.433901
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.033555
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.305708
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.288270
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.348866
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.557203
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.511461

Test set: Average loss: 2.3661, Accuracy: 2038/10000 (20%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.394943
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.376953
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.556629
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.569089
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.502227
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.312574
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.571110
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.424304
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.388572

Test set: Average loss: 2.3762, Accuracy: 1936/10000 (19%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 2.183356
Train Epoch: 14 [12736/60000 (21%)]	Loss: 2.569813
Train Epoch: 14 [19136/60000 (32%)]	Loss: 2.492287
Train Epoch: 14 [25536/60000 (43%)]	Loss: 2.107322
Train Epoch: 14 [31936/60000 (53%)]	Loss: 2.425451
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.719825
Train Epoch: 14 [44736/60000 (75%)]	Loss: 2.122311
Train Epoch: 14 [51136/60000 (85%)]	Loss: 2.267691
Train Epoch: 14 [57536/60000 (96%)]	Loss: 2.532209

Test set: Average loss: 2.3750, Accuracy: 1925/10000 (19%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.47 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1971438
