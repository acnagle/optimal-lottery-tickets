Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.794131
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.097545
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.174879
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.264423
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.279152
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.400858
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.501635
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.769526
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.732135

Test set: Average loss: 1.6869, Accuracy: 4362/10000 (44%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 1.685143
Train Epoch: 2 [12736/60000 (21%)]	Loss: 1.834086
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.711442
Train Epoch: 2 [25536/60000 (43%)]	Loss: 1.956182
Train Epoch: 2 [31936/60000 (53%)]	Loss: 1.886359
Train Epoch: 2 [38336/60000 (64%)]	Loss: 1.854740
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.871776
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.238465
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.906596

Test set: Average loss: 2.0232, Accuracy: 3260/10000 (33%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.023901
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.214188
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.956224
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.146769
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.098581
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.977219
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.043127
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.135070
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.153163

Test set: Average loss: 2.1656, Accuracy: 2641/10000 (26%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.248264
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.047791
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.071918
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.017166
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.175172
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.199915
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.356506
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.229211
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.433920

Test set: Average loss: 2.2462, Accuracy: 2294/10000 (23%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.251325
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.319873
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.322052
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.301823
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.239662
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.349755
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.338334
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.039051
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.339554

Test set: Average loss: 2.2828, Accuracy: 2225/10000 (22%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.330041
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.254343
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.134330
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.308984
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.254573
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.275688
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.252784
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.275328
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.318633

Test set: Average loss: 2.3207, Accuracy: 2151/10000 (22%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.288595
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.376384
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.458318
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.252574
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.269179
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.381842
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.455271
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.189452
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.311341

Test set: Average loss: 2.3348, Accuracy: 2162/10000 (22%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.415562
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.414632
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.571299
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.390900
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.504570
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.363255
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.189027
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.361749
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.302552

Test set: Average loss: 2.3584, Accuracy: 1980/10000 (20%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.086942
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.177050
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.564449
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.545900
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.552472
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.510597
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.511461
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.615966
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.454515

Test set: Average loss: 2.3738, Accuracy: 1976/10000 (20%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.415444
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.306775
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.566719
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.258341
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.372012
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.556605
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.434979
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.379426
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.404952

Test set: Average loss: 2.3721, Accuracy: 2019/10000 (20%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.185503
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.483399
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.422950
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.531368
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.461706
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.437197
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.452353
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.286457
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.354235

Test set: Average loss: 2.3911, Accuracy: 1908/10000 (19%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.469774
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.397405
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.472510
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.031685
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.321872
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.278786
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.373399
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.546040
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.517344

Test set: Average loss: 2.3766, Accuracy: 2018/10000 (20%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.382056
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.363729
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.585470
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.601843
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.530347
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.308299
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.600596
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.449378
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.396411

Test set: Average loss: 2.3963, Accuracy: 1868/10000 (19%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 2.201487
Train Epoch: 14 [12736/60000 (21%)]	Loss: 2.569299
Train Epoch: 14 [19136/60000 (32%)]	Loss: 2.500339
Train Epoch: 14 [25536/60000 (43%)]	Loss: 2.123232
Train Epoch: 14 [31936/60000 (53%)]	Loss: 2.427886
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.697687
Train Epoch: 14 [44736/60000 (75%)]	Loss: 2.120332
Train Epoch: 14 [51136/60000 (85%)]	Loss: 2.281502
Train Epoch: 14 [57536/60000 (96%)]	Loss: 2.539767

Test set: Average loss: 2.3818, Accuracy: 1864/10000 (19%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 2.472629
Train Epoch: 15 [12736/60000 (21%)]	Loss: 2.410882
Train Epoch: 15 [19136/60000 (32%)]	Loss: 2.434720
Train Epoch: 15 [25536/60000 (43%)]	Loss: 2.195971
Train Epoch: 15 [31936/60000 (53%)]	Loss: 2.429110
Train Epoch: 15 [38336/60000 (64%)]	Loss: 2.238308
Train Epoch: 15 [44736/60000 (75%)]	Loss: 2.131800
Train Epoch: 15 [51136/60000 (85%)]	Loss: 2.103439
Train Epoch: 15 [57536/60000 (96%)]	Loss: 2.572067

Test set: Average loss: 2.3877, Accuracy: 1944/10000 (19%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.58 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1971438
