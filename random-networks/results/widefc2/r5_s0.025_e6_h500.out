Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.449582
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.404629
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.292543
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.344189
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.334094
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.284673
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.401256
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.457978
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.266256

Test set: Average loss: 0.3108, Accuracy: 9183/10000 (92%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.349832
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.371697
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.394648
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.656598
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.562895
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.444903
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.441082
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.450237
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.428799

Test set: Average loss: 0.5444, Accuracy: 8378/10000 (84%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.484465
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.475039
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.411703
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.692111
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.602126
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.615246
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.682939
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.938745
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.831964

Test set: Average loss: 0.8992, Accuracy: 7158/10000 (72%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.055636
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.912509
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.848979
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.939072
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.004850
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.978294
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.080016
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.025101
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.220229

Test set: Average loss: 1.2000, Accuracy: 6069/10000 (61%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.129455
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.362428
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.096314
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.337432
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.125298
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.350804
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.209190
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.102216
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.358660

Test set: Average loss: 1.2347, Accuracy: 6203/10000 (62%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.270709
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.323121
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.177801
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.175839
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.319087
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.214847
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.138344
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.245287
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.199348

Test set: Average loss: 1.3271, Accuracy: 5620/10000 (56%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 0.63 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 1941568
