Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=12, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.5, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.407745
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.351249
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.274962
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.288066
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.233345
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.135031
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.314829
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.300489
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.148695

Test set: Average loss: 0.1637, Accuracy: 9531/10000 (95%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.181213
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.112919
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.121943
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.199836
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.194273
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.106172
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.154353
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.098975
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.060810

Test set: Average loss: 0.1265, Accuracy: 9642/10000 (96%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.101728
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.053012
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.049483
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.080249
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.189638
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.065841
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.093035
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.322562
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.065099

Test set: Average loss: 0.1096, Accuracy: 9670/10000 (97%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.082645
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.169767
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.052525
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.087292
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.102634
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.101743
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.122866
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.067833
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.057074

Test set: Average loss: 0.1103, Accuracy: 9680/10000 (97%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.025418
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.023261
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.035844
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.103041
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.124166
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.109054
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.057248
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.028206
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.038233

Test set: Average loss: 0.0990, Accuracy: 9704/10000 (97%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.107746
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.037872
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.051051
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.126946
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.031586
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.089939
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.061427
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.093186
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.034708

Test set: Average loss: 0.0923, Accuracy: 9732/10000 (97%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.019366
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.039130
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.065764
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.056183
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.076037
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.116243
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.116000
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.017391
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.035989

Test set: Average loss: 0.0842, Accuracy: 9749/10000 (97%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.061844
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.040877
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.032072
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.063605
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.137673
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.045370
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.049052
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.022539
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.094572

Test set: Average loss: 0.0815, Accuracy: 9757/10000 (98%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.047779
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.024005
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.019802
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.031361
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.026598
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.019470
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.033439
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.041333
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.089315

Test set: Average loss: 0.0717, Accuracy: 9786/10000 (98%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.054146
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.023702
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.042200
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.028284
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.062584
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.015247
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.034286
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.016384
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.033345

Test set: Average loss: 0.0689, Accuracy: 9806/10000 (98%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.025074
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.010622
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.017232
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.017535
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.034453
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.038961
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.024546
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.011956
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.018152

Test set: Average loss: 0.0664, Accuracy: 9801/10000 (98%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.014280
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.030395
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.026000
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.012895
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.017954
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.047793
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.016398
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.036692
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.036347

Test set: Average loss: 0.0660, Accuracy: 9807/10000 (98%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=12, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.5, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.28 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 995676
