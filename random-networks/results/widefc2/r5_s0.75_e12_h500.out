Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=12, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.75, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.459534
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.404926
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.302269
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.323539
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.278083
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.173084
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.316158
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.315274
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.173689

Test set: Average loss: 0.1899, Accuracy: 9467/10000 (95%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.229902
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.116036
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.151248
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.226794
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.225756
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.109035
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.159493
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.116434
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.088070

Test set: Average loss: 0.1483, Accuracy: 9578/10000 (96%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.136551
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.055282
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.070550
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.093673
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.202393
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.078650
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.125651
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.301219
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.083088

Test set: Average loss: 0.1217, Accuracy: 9641/10000 (96%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.111142
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.200622
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.088033
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.119822
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.121707
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.130566
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.135903
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.106119
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.075055

Test set: Average loss: 0.1247, Accuracy: 9634/10000 (96%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.037413
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.032009
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.042959
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.124974
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.137326
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.140490
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.065885
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.031893
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.051696

Test set: Average loss: 0.1087, Accuracy: 9703/10000 (97%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.140842
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.064281
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.075483
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.128526
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.049043
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.099479
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.056983
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.117288
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.044172

Test set: Average loss: 0.0988, Accuracy: 9719/10000 (97%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.033247
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.060995
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.081927
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.058133
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.114365
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.114688
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.120057
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.024252
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.048256

Test set: Average loss: 0.0928, Accuracy: 9731/10000 (97%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.101124
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.068596
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.043544
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.088979
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.166180
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.075040
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.072893
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.039026
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.142551

Test set: Average loss: 0.0898, Accuracy: 9740/10000 (97%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.063586
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.031412
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.029059
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.039378
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.041578
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.025108
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.051466
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.057997
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.158539

Test set: Average loss: 0.0837, Accuracy: 9762/10000 (98%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.079562
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.034000
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.048238
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.043946
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.093756
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.028049
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.057733
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.026863
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.053234

Test set: Average loss: 0.0816, Accuracy: 9768/10000 (98%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.042562
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.017383
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.028498
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.027280
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.051298
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.059732
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.039376
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.018078
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.031725

Test set: Average loss: 0.0789, Accuracy: 9776/10000 (98%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.034125
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.045656
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.053249
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.020282
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.030779
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.078349
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.027416
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.069383
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.062349

Test set: Average loss: 0.0785, Accuracy: 9773/10000 (98%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=12, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.75, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.26 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 497838
