Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.675023
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.556639
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.438839
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.459859
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.406084
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.311373
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.412441
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.399826
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.326061

Test set: Average loss: 0.3124, Accuracy: 9192/10000 (92%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.356283
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.222578
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.300234
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.388644
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.403426
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.187783
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.219351
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.203436
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.230960

Test set: Average loss: 0.2681, Accuracy: 9261/10000 (93%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.216689
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.146056
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.193002
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.235694
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.288286
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.163390
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.265156
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.435831
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.153525

Test set: Average loss: 0.2394, Accuracy: 9353/10000 (94%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.281237
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.304014
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.293974
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.254658
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.275498
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.269553
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.214125
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.215802
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.215750

Test set: Average loss: 0.2232, Accuracy: 9390/10000 (94%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.119410
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.126716
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.150929
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.226691
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.255290
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.418641
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.203154
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.152690
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.176821

Test set: Average loss: 0.2126, Accuracy: 9424/10000 (94%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.246474
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.198550
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.209935
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.255403
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.212573
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.189433
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.144450
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.220661
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.158256

Test set: Average loss: 0.2084, Accuracy: 9427/10000 (94%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.125481
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.147514
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.248075
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.138671
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.274277
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.194924
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.213827
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.143016
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.206581

Test set: Average loss: 0.1989, Accuracy: 9450/10000 (94%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.413783
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.194094
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.130266
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.227126
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.274667
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.123017
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.159690
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.157249
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.275116

Test set: Average loss: 0.1946, Accuracy: 9466/10000 (95%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.154755
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.127797
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.137347
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.126233
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.196862
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.151044
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.186536
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.247498
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.332699

Test set: Average loss: 0.1940, Accuracy: 9457/10000 (95%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.164015
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.175206
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.152519
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.159492
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.260296
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.118605
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.245589
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.112917
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.190589

Test set: Average loss: 0.1925, Accuracy: 9480/10000 (95%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.04 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 99567
