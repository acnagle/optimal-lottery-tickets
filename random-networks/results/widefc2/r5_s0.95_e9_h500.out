Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.675023
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.556639
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.438839
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.459859
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.406084
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.311373
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.412441
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.399826
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.326061

Test set: Average loss: 0.3124, Accuracy: 9192/10000 (92%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.357092
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.221818
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.301870
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.389568
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.401353
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.190015
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.217996
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.202675
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.232417

Test set: Average loss: 0.2684, Accuracy: 9256/10000 (93%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.215191
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.145729
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.194851
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.235969
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.286330
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.162559
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.260023
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.442595
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.156245

Test set: Average loss: 0.2410, Accuracy: 9342/10000 (93%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.284377
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.299879
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.299688
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.254189
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.272578
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.270452
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.215971
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.218708
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.216363

Test set: Average loss: 0.2255, Accuracy: 9379/10000 (94%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.120641
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.128758
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.153190
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.231059
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.251336
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.425782
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.204831
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.151240
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.175583

Test set: Average loss: 0.2154, Accuracy: 9420/10000 (94%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.248538
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.199527
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.206909
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.260279
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.218915
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.191431
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.149270
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.216609
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.158845

Test set: Average loss: 0.2116, Accuracy: 9415/10000 (94%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.126641
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.150021
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.244178
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.133116
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.277477
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.197690
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.214821
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.143520
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.206750

Test set: Average loss: 0.2023, Accuracy: 9441/10000 (94%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.426784
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.191155
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.129038
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.225644
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.273705
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.118962
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.160361
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.160043
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.283841

Test set: Average loss: 0.1985, Accuracy: 9455/10000 (95%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.156254
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.128990
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.139140
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.130792
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.205186
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.160150
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.192756
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.250886
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.332635

Test set: Average loss: 0.1979, Accuracy: 9456/10000 (95%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=9, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.95, use_relu=False, wd=0.0005)


Total time spent pruning/training: 0.94 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 99567
