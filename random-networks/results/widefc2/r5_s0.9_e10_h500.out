Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.9, use_relu=False, wd=0.0005) 

Pruning a Wide Two-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.531287
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.484917
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.378034
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.393386
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.356634
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.259089
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.381510
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.368591
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.254703

Test set: Average loss: 0.2576, Accuracy: 9310/10000 (93%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.302360
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.164266
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.236809
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.318525
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.345499
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.149137
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.172061
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.167688
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.150620

Test set: Average loss: 0.2090, Accuracy: 9423/10000 (94%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.187713
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.109908
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.137527
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.154131
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.280400
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.111729
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.203568
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.370104
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.105707

Test set: Average loss: 0.1775, Accuracy: 9507/10000 (95%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.202798
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.239515
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.172093
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.190090
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.199190
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.197827
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.160549
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.156288
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.139588

Test set: Average loss: 0.1658, Accuracy: 9540/10000 (95%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.074962
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.068805
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.080604
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.172177
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.174996
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.254699
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.122967
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.071700
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.112456

Test set: Average loss: 0.1503, Accuracy: 9597/10000 (96%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.200948
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.134554
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.135239
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.187078
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.109705
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.146157
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.091483
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.154976
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.095144

Test set: Average loss: 0.1446, Accuracy: 9591/10000 (96%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.076885
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.102565
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.148064
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.087533
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.193179
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.148073
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.156906
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.066873
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.124619

Test set: Average loss: 0.1355, Accuracy: 9621/10000 (96%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.281637
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.118575
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.086637
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.156070
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.208127
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.096653
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.105915
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.087435
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.209347

Test set: Average loss: 0.1321, Accuracy: 9633/10000 (96%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.098173
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.078819
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.071236
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.077589
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.110042
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.068327
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.110764
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.123984
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.230932

Test set: Average loss: 0.1294, Accuracy: 9641/10000 (96%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.106494
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.096007
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.107595
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.091799
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.177647
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.075162
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.140181
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.056288
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.111251

Test set: Average loss: 0.1276, Accuracy: 9649/10000 (96%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=10, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc2', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.9, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.06 minutes
Total number of parameters in model: 1991352
Number of parameters in pruned model: 199135
