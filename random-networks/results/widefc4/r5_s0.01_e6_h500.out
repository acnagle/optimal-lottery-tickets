Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.460646
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.325525
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.622307
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.544986
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.545340
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.554977
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.576526
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.543963
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.877355

Test set: Average loss: 0.6750, Accuracy: 8073/10000 (81%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.514198
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.897160
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.817885
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.810969
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.907447
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.864102
Train Epoch: 2 [44736/60000 (75%)]	Loss: 1.047763
Train Epoch: 2 [51136/60000 (85%)]	Loss: 1.059493
Train Epoch: 2 [57536/60000 (96%)]	Loss: 1.073109

Test set: Average loss: 1.1392, Accuracy: 6437/10000 (64%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 1.125239
Train Epoch: 3 [12736/60000 (21%)]	Loss: 1.180444
Train Epoch: 3 [19136/60000 (32%)]	Loss: 1.009734
Train Epoch: 3 [25536/60000 (43%)]	Loss: 1.323715
Train Epoch: 3 [31936/60000 (53%)]	Loss: 1.243122
Train Epoch: 3 [38336/60000 (64%)]	Loss: 1.553133
Train Epoch: 3 [44736/60000 (75%)]	Loss: 1.233099
Train Epoch: 3 [51136/60000 (85%)]	Loss: 1.493341
Train Epoch: 3 [57536/60000 (96%)]	Loss: 1.526686

Test set: Average loss: 1.3825, Accuracy: 5760/10000 (58%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.642232
Train Epoch: 4 [12736/60000 (21%)]	Loss: 1.427196
Train Epoch: 4 [19136/60000 (32%)]	Loss: 1.536299
Train Epoch: 4 [25536/60000 (43%)]	Loss: 1.542636
Train Epoch: 4 [31936/60000 (53%)]	Loss: 1.455632
Train Epoch: 4 [38336/60000 (64%)]	Loss: 1.533525
Train Epoch: 4 [44736/60000 (75%)]	Loss: 1.568613
Train Epoch: 4 [51136/60000 (85%)]	Loss: 1.507223
Train Epoch: 4 [57536/60000 (96%)]	Loss: 1.623785

Test set: Average loss: 1.5797, Accuracy: 5024/10000 (50%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 1.663754
Train Epoch: 5 [12736/60000 (21%)]	Loss: 1.487954
Train Epoch: 5 [19136/60000 (32%)]	Loss: 1.658375
Train Epoch: 5 [25536/60000 (43%)]	Loss: 1.431922
Train Epoch: 5 [31936/60000 (53%)]	Loss: 1.510824
Train Epoch: 5 [38336/60000 (64%)]	Loss: 1.495418
Train Epoch: 5 [44736/60000 (75%)]	Loss: 1.612704
Train Epoch: 5 [51136/60000 (85%)]	Loss: 1.645172
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.835870

Test set: Average loss: 1.6577, Accuracy: 4592/10000 (46%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 1.534654
Train Epoch: 6 [12736/60000 (21%)]	Loss: 1.728295
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.852500
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.744679
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.743725
Train Epoch: 6 [38336/60000 (64%)]	Loss: 1.731898
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.728509
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.588961
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.573943

Test set: Average loss: 1.6531, Accuracy: 4704/10000 (47%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=6, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.07 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 4451542
