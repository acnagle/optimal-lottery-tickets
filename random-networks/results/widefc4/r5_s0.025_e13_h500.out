Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.438271
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.193287
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.502275
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.234330
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.281237
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.501562
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.129462
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.174747
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.240347

Test set: Average loss: 0.3205, Accuracy: 8956/10000 (90%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.156413
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.249559
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.314515
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.137162
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.303874
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.216648
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.455873
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.305515
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.222359

Test set: Average loss: 0.2878, Accuracy: 9108/10000 (91%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.404143
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.168934
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.248972
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.349571
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.197288
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.478384
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.301768
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.487496
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.356324

Test set: Average loss: 0.3886, Accuracy: 8853/10000 (89%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.778229
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.257814
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.499628
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.756921
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.349298
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.365335
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.734582
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.465786
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.402282

Test set: Average loss: 0.6800, Accuracy: 7564/10000 (76%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.505899
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.465934
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.493620
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.369786
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.511117
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.502364
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.511736
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.738601
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.175448

Test set: Average loss: 0.5693, Accuracy: 8254/10000 (83%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.550709
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.730244
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.875847
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.666483
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.605848
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.840359
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.852466
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.639917
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.694471

Test set: Average loss: 0.7996, Accuracy: 7497/10000 (75%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.010824
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.876851
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.778810
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.696543
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.081003
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.755452
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.814059
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.958133
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.777473

Test set: Average loss: 0.8616, Accuracy: 7290/10000 (73%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.630146
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.996599
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.952670
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.780765
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.827742
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.857974
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.804846
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.929025
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.724576

Test set: Average loss: 0.9177, Accuracy: 7191/10000 (72%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.950197
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.842307
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.868294
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.812643
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.975557
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.857657
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.961699
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.001941
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.074458

Test set: Average loss: 1.0116, Accuracy: 6896/10000 (69%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.040187
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.934532
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.860578
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.016728
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.042431
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.895499
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.958211
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.995058
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.112596

Test set: Average loss: 0.9476, Accuracy: 7243/10000 (72%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.880151
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.941300
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.920001
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.994965
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.842795
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.949688
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.923762
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.995651
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.203084

Test set: Average loss: 0.9905, Accuracy: 7126/10000 (71%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.022987
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.867269
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.816422
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.962440
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.096339
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.979912
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.109728
Train Epoch: 12 [51136/60000 (85%)]	Loss: 1.062399
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.933134

Test set: Average loss: 1.0230, Accuracy: 6839/10000 (68%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.073056
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.754800
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.024336
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.936538
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.919857
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.164569
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.911881
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.949471
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.193613

Test set: Average loss: 0.9680, Accuracy: 7222/10000 (72%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=13, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.025, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.31 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 4384095
