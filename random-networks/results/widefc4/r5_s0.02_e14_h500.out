Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.434348
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.213677
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.455440
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.205159
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.281525
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.444896
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.154133
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.210653
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.346843

Test set: Average loss: 0.2798, Accuracy: 9135/10000 (91%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.094752
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.318447
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.375441
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.196999
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.321026
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.450263
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.495569
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.357150
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.218525

Test set: Average loss: 0.3701, Accuracy: 8802/10000 (88%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.352110
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.308496
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.321120
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.428132
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.442743
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.812347
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.448135
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.869271
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.518668

Test set: Average loss: 0.4902, Accuracy: 8502/10000 (85%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.965061
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.444288
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.708382
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.733293
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.606108
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.542909
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.750330
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.646531
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.801158

Test set: Average loss: 0.7915, Accuracy: 7196/10000 (72%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.764414
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.755774
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.816916
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.526728
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.735789
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.795541
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.880744
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.834499
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.209848

Test set: Average loss: 0.9073, Accuracy: 7092/10000 (71%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.771187
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.947955
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.034796
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.996049
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.985647
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.937424
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.048686
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.900964
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.032185

Test set: Average loss: 1.0373, Accuracy: 6981/10000 (70%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.174787
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.141274
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.163940
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.122002
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.352202
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.219954
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.234731
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.285629
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.097919

Test set: Average loss: 1.2533, Accuracy: 6052/10000 (61%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.967696
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.252429
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.306400
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.201074
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.228380
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.198345
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.134398
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.307490
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.154831

Test set: Average loss: 1.2356, Accuracy: 6366/10000 (64%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.275648
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.258485
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.204322
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.117004
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.311135
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.192429
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.280646
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.386772
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.438902

Test set: Average loss: 1.3488, Accuracy: 5780/10000 (58%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.444932
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.306740
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.239164
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.397412
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.349901
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.290237
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.385929
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.384678
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.488860

Test set: Average loss: 1.3091, Accuracy: 6066/10000 (61%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.193962
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.311721
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.248288
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.339720
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.256854
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.224920
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.308478
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.387082
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.502839

Test set: Average loss: 1.3616, Accuracy: 5925/10000 (59%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.400187
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.303255
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.228507
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.399743
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.417132
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.365197
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.395380
Train Epoch: 12 [51136/60000 (85%)]	Loss: 1.398040
Train Epoch: 12 [57536/60000 (96%)]	Loss: 1.341621

Test set: Average loss: 1.3845, Accuracy: 5576/10000 (56%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.366664
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.185933
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.434942
Train Epoch: 13 [25536/60000 (43%)]	Loss: 1.373837
Train Epoch: 13 [31936/60000 (53%)]	Loss: 1.357210
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.596291
Train Epoch: 13 [44736/60000 (75%)]	Loss: 1.298883
Train Epoch: 13 [51136/60000 (85%)]	Loss: 1.277589
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.501847

Test set: Average loss: 1.3727, Accuracy: 5806/10000 (58%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 1.439962
Train Epoch: 14 [12736/60000 (21%)]	Loss: 1.378877
Train Epoch: 14 [19136/60000 (32%)]	Loss: 1.470408
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.381798
Train Epoch: 14 [31936/60000 (53%)]	Loss: 1.376309
Train Epoch: 14 [38336/60000 (64%)]	Loss: 1.390617
Train Epoch: 14 [44736/60000 (75%)]	Loss: 1.336742
Train Epoch: 14 [51136/60000 (85%)]	Loss: 1.297831
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.186822

Test set: Average loss: 1.3802, Accuracy: 5796/10000 (58%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.49 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 4406577
