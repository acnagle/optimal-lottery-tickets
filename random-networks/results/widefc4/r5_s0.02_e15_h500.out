Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.434348
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.213677
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.455440
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.205159
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.281525
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.444896
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.154133
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.210653
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.346843

Test set: Average loss: 0.2798, Accuracy: 9135/10000 (91%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.096622
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.335601
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.462458
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.191745
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.322243
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.386371
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.482914
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.324788
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.255203

Test set: Average loss: 0.4343, Accuracy: 8558/10000 (86%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.385977
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.321310
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.298628
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.455780
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.312678
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.684857
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.446979
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.745866
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.496476

Test set: Average loss: 0.5301, Accuracy: 8383/10000 (84%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 1.240568
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.469287
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.795258
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.750873
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.597135
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.606304
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.760970
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.636081
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.737329

Test set: Average loss: 0.7549, Accuracy: 7409/10000 (74%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.764369
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.664804
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.819945
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.559709
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.712134
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.720315
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.878383
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.866928
Train Epoch: 5 [57536/60000 (96%)]	Loss: 1.184603

Test set: Average loss: 0.8871, Accuracy: 7332/10000 (73%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.754688
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.928069
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.059112
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.035673
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.992347
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.934089
Train Epoch: 6 [44736/60000 (75%)]	Loss: 1.059398
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.945147
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.043682

Test set: Average loss: 1.0465, Accuracy: 6927/10000 (69%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.195685
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.144262
Train Epoch: 7 [19136/60000 (32%)]	Loss: 1.178864
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.107541
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.333935
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.306695
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.249186
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.361449
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.109981

Test set: Average loss: 1.2842, Accuracy: 5864/10000 (59%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.016314
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.290255
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.289920
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.203036
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.245921
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.236192
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.171826
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.367394
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.176107

Test set: Average loss: 1.2458, Accuracy: 6344/10000 (63%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.308448
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.286090
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.278662
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.152517
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.380851
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.226162
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.290098
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.395237
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.452957

Test set: Average loss: 1.3538, Accuracy: 5758/10000 (58%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.462085
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.364889
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.267493
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.455118
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.378083
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.338163
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.454989
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.415540
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.565760

Test set: Average loss: 1.3483, Accuracy: 5953/10000 (60%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.202272
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.356292
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.301366
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.402136
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.350678
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.255266
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.327828
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.438377
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.529724

Test set: Average loss: 1.4000, Accuracy: 5735/10000 (57%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.410359
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.344380
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.273126
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.466105
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.440173
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.380836
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.439476
Train Epoch: 12 [51136/60000 (85%)]	Loss: 1.448670
Train Epoch: 12 [57536/60000 (96%)]	Loss: 1.346101

Test set: Average loss: 1.4148, Accuracy: 5515/10000 (55%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.433999
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.231675
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.467444
Train Epoch: 13 [25536/60000 (43%)]	Loss: 1.439755
Train Epoch: 13 [31936/60000 (53%)]	Loss: 1.426124
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.621322
Train Epoch: 13 [44736/60000 (75%)]	Loss: 1.359544
Train Epoch: 13 [51136/60000 (85%)]	Loss: 1.344594
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.552228

Test set: Average loss: 1.4171, Accuracy: 5638/10000 (56%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 1.495073
Train Epoch: 14 [12736/60000 (21%)]	Loss: 1.454596
Train Epoch: 14 [19136/60000 (32%)]	Loss: 1.509941
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.475500
Train Epoch: 14 [31936/60000 (53%)]	Loss: 1.417074
Train Epoch: 14 [38336/60000 (64%)]	Loss: 1.441900
Train Epoch: 14 [44736/60000 (75%)]	Loss: 1.357082
Train Epoch: 14 [51136/60000 (85%)]	Loss: 1.358319
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.214658

Test set: Average loss: 1.4209, Accuracy: 5668/10000 (57%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 1.385746
Train Epoch: 15 [12736/60000 (21%)]	Loss: 1.452766
Train Epoch: 15 [19136/60000 (32%)]	Loss: 1.142375
Train Epoch: 15 [25536/60000 (43%)]	Loss: 1.388636
Train Epoch: 15 [31936/60000 (53%)]	Loss: 1.466130
Train Epoch: 15 [38336/60000 (64%)]	Loss: 1.366433
Train Epoch: 15 [44736/60000 (75%)]	Loss: 1.314011
Train Epoch: 15 [51136/60000 (85%)]	Loss: 1.423725
Train Epoch: 15 [57536/60000 (96%)]	Loss: 1.416436

Test set: Average loss: 1.4003, Accuracy: 5686/10000 (57%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=15, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.02, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.66 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 4406577
