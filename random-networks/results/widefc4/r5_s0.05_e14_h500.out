Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=False, wd=0.0005) 

Pruning a Wide Four-Layer Fully Connected network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 0.383277
Train Epoch: 1 [12736/60000 (21%)]	Loss: 0.147681
Train Epoch: 1 [19136/60000 (32%)]	Loss: 0.411475
Train Epoch: 1 [25536/60000 (43%)]	Loss: 0.174934
Train Epoch: 1 [31936/60000 (53%)]	Loss: 0.252086
Train Epoch: 1 [38336/60000 (64%)]	Loss: 0.377365
Train Epoch: 1 [44736/60000 (75%)]	Loss: 0.078369
Train Epoch: 1 [51136/60000 (85%)]	Loss: 0.150619
Train Epoch: 1 [57536/60000 (96%)]	Loss: 0.109401

Test set: Average loss: 0.2038, Accuracy: 9383/10000 (94%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 0.070492
Train Epoch: 2 [12736/60000 (21%)]	Loss: 0.156347
Train Epoch: 2 [19136/60000 (32%)]	Loss: 0.148116
Train Epoch: 2 [25536/60000 (43%)]	Loss: 0.132441
Train Epoch: 2 [31936/60000 (53%)]	Loss: 0.247654
Train Epoch: 2 [38336/60000 (64%)]	Loss: 0.165914
Train Epoch: 2 [44736/60000 (75%)]	Loss: 0.266396
Train Epoch: 2 [51136/60000 (85%)]	Loss: 0.175653
Train Epoch: 2 [57536/60000 (96%)]	Loss: 0.164538

Test set: Average loss: 0.1375, Accuracy: 9560/10000 (96%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 0.246650
Train Epoch: 3 [12736/60000 (21%)]	Loss: 0.093298
Train Epoch: 3 [19136/60000 (32%)]	Loss: 0.178387
Train Epoch: 3 [25536/60000 (43%)]	Loss: 0.176121
Train Epoch: 3 [31936/60000 (53%)]	Loss: 0.133814
Train Epoch: 3 [38336/60000 (64%)]	Loss: 0.254769
Train Epoch: 3 [44736/60000 (75%)]	Loss: 0.151484
Train Epoch: 3 [51136/60000 (85%)]	Loss: 0.121932
Train Epoch: 3 [57536/60000 (96%)]	Loss: 0.107257

Test set: Average loss: 0.1704, Accuracy: 9464/10000 (95%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 0.086055
Train Epoch: 4 [12736/60000 (21%)]	Loss: 0.150851
Train Epoch: 4 [19136/60000 (32%)]	Loss: 0.129021
Train Epoch: 4 [25536/60000 (43%)]	Loss: 0.347500
Train Epoch: 4 [31936/60000 (53%)]	Loss: 0.237985
Train Epoch: 4 [38336/60000 (64%)]	Loss: 0.313985
Train Epoch: 4 [44736/60000 (75%)]	Loss: 0.306467
Train Epoch: 4 [51136/60000 (85%)]	Loss: 0.192158
Train Epoch: 4 [57536/60000 (96%)]	Loss: 0.150410

Test set: Average loss: 0.1967, Accuracy: 9385/10000 (94%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 0.250644
Train Epoch: 5 [12736/60000 (21%)]	Loss: 0.139553
Train Epoch: 5 [19136/60000 (32%)]	Loss: 0.159906
Train Epoch: 5 [25536/60000 (43%)]	Loss: 0.155763
Train Epoch: 5 [31936/60000 (53%)]	Loss: 0.210528
Train Epoch: 5 [38336/60000 (64%)]	Loss: 0.166629
Train Epoch: 5 [44736/60000 (75%)]	Loss: 0.263562
Train Epoch: 5 [51136/60000 (85%)]	Loss: 0.223424
Train Epoch: 5 [57536/60000 (96%)]	Loss: 0.410640

Test set: Average loss: 0.3705, Accuracy: 8761/10000 (88%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 0.236252
Train Epoch: 6 [12736/60000 (21%)]	Loss: 0.270263
Train Epoch: 6 [19136/60000 (32%)]	Loss: 0.286374
Train Epoch: 6 [25536/60000 (43%)]	Loss: 0.230990
Train Epoch: 6 [31936/60000 (53%)]	Loss: 0.338858
Train Epoch: 6 [38336/60000 (64%)]	Loss: 0.132705
Train Epoch: 6 [44736/60000 (75%)]	Loss: 0.396651
Train Epoch: 6 [51136/60000 (85%)]	Loss: 0.187810
Train Epoch: 6 [57536/60000 (96%)]	Loss: 0.347770

Test set: Average loss: 0.2708, Accuracy: 9188/10000 (92%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 0.273517
Train Epoch: 7 [12736/60000 (21%)]	Loss: 0.360730
Train Epoch: 7 [19136/60000 (32%)]	Loss: 0.414527
Train Epoch: 7 [25536/60000 (43%)]	Loss: 0.304865
Train Epoch: 7 [31936/60000 (53%)]	Loss: 0.398047
Train Epoch: 7 [38336/60000 (64%)]	Loss: 0.246738
Train Epoch: 7 [44736/60000 (75%)]	Loss: 0.318353
Train Epoch: 7 [51136/60000 (85%)]	Loss: 0.408218
Train Epoch: 7 [57536/60000 (96%)]	Loss: 0.260537

Test set: Average loss: 0.3158, Accuracy: 9047/10000 (90%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 0.295043
Train Epoch: 8 [12736/60000 (21%)]	Loss: 0.261092
Train Epoch: 8 [19136/60000 (32%)]	Loss: 0.518307
Train Epoch: 8 [25536/60000 (43%)]	Loss: 0.216344
Train Epoch: 8 [31936/60000 (53%)]	Loss: 0.371442
Train Epoch: 8 [38336/60000 (64%)]	Loss: 0.592711
Train Epoch: 8 [44736/60000 (75%)]	Loss: 0.289649
Train Epoch: 8 [51136/60000 (85%)]	Loss: 0.385907
Train Epoch: 8 [57536/60000 (96%)]	Loss: 0.399673

Test set: Average loss: 0.3101, Accuracy: 8998/10000 (90%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 0.459287
Train Epoch: 9 [12736/60000 (21%)]	Loss: 0.222656
Train Epoch: 9 [19136/60000 (32%)]	Loss: 0.218351
Train Epoch: 9 [25536/60000 (43%)]	Loss: 0.264925
Train Epoch: 9 [31936/60000 (53%)]	Loss: 0.489193
Train Epoch: 9 [38336/60000 (64%)]	Loss: 0.361424
Train Epoch: 9 [44736/60000 (75%)]	Loss: 0.405973
Train Epoch: 9 [51136/60000 (85%)]	Loss: 0.421545
Train Epoch: 9 [57536/60000 (96%)]	Loss: 0.376435

Test set: Average loss: 0.7664, Accuracy: 7235/10000 (72%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 0.323350
Train Epoch: 10 [12736/60000 (21%)]	Loss: 0.476539
Train Epoch: 10 [19136/60000 (32%)]	Loss: 0.404393
Train Epoch: 10 [25536/60000 (43%)]	Loss: 0.447752
Train Epoch: 10 [31936/60000 (53%)]	Loss: 0.550148
Train Epoch: 10 [38336/60000 (64%)]	Loss: 0.319365
Train Epoch: 10 [44736/60000 (75%)]	Loss: 0.232011
Train Epoch: 10 [51136/60000 (85%)]	Loss: 0.941523
Train Epoch: 10 [57536/60000 (96%)]	Loss: 0.373127

Test set: Average loss: 0.3649, Accuracy: 8846/10000 (88%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 0.360942
Train Epoch: 11 [12736/60000 (21%)]	Loss: 0.419084
Train Epoch: 11 [19136/60000 (32%)]	Loss: 0.361630
Train Epoch: 11 [25536/60000 (43%)]	Loss: 0.369886
Train Epoch: 11 [31936/60000 (53%)]	Loss: 0.220245
Train Epoch: 11 [38336/60000 (64%)]	Loss: 0.377391
Train Epoch: 11 [44736/60000 (75%)]	Loss: 0.262907
Train Epoch: 11 [51136/60000 (85%)]	Loss: 0.359576
Train Epoch: 11 [57536/60000 (96%)]	Loss: 0.524714

Test set: Average loss: 0.5125, Accuracy: 8283/10000 (83%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 0.441189
Train Epoch: 12 [12736/60000 (21%)]	Loss: 0.332506
Train Epoch: 12 [19136/60000 (32%)]	Loss: 0.242083
Train Epoch: 12 [25536/60000 (43%)]	Loss: 0.333541
Train Epoch: 12 [31936/60000 (53%)]	Loss: 0.462561
Train Epoch: 12 [38336/60000 (64%)]	Loss: 0.628717
Train Epoch: 12 [44736/60000 (75%)]	Loss: 0.524747
Train Epoch: 12 [51136/60000 (85%)]	Loss: 0.426167
Train Epoch: 12 [57536/60000 (96%)]	Loss: 0.353345

Test set: Average loss: 0.3887, Accuracy: 8780/10000 (88%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 0.563327
Train Epoch: 13 [12736/60000 (21%)]	Loss: 0.212742
Train Epoch: 13 [19136/60000 (32%)]	Loss: 0.359046
Train Epoch: 13 [25536/60000 (43%)]	Loss: 0.427826
Train Epoch: 13 [31936/60000 (53%)]	Loss: 0.320222
Train Epoch: 13 [38336/60000 (64%)]	Loss: 0.410683
Train Epoch: 13 [44736/60000 (75%)]	Loss: 0.308660
Train Epoch: 13 [51136/60000 (85%)]	Loss: 0.515853
Train Epoch: 13 [57536/60000 (96%)]	Loss: 0.300950

Test set: Average loss: 0.3608, Accuracy: 8876/10000 (89%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 0.388066
Train Epoch: 14 [12736/60000 (21%)]	Loss: 0.370705
Train Epoch: 14 [19136/60000 (32%)]	Loss: 0.273185
Train Epoch: 14 [25536/60000 (43%)]	Loss: 0.364521
Train Epoch: 14 [31936/60000 (53%)]	Loss: 0.123739
Train Epoch: 14 [38336/60000 (64%)]	Loss: 0.313315
Train Epoch: 14 [44736/60000 (75%)]	Loss: 0.312439
Train Epoch: 14 [51136/60000 (85%)]	Loss: 0.499113
Train Epoch: 14 [57536/60000 (96%)]	Loss: 0.347697

Test set: Average loss: 0.3072, Accuracy: 9074/10000 (91%)

Namespace(batch_size=64, bias=None, data='../data', device=1, epochs=14, hidden_size=500, load_weights=None, log_interval=100, lr=0.1, model='widefc4', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.05, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.49 minutes
Total number of parameters in model: 4496508
Number of parameters in pruned model: 4271682
