Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=19, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.010781
Train Epoch: 1 [12736/60000 (21%)]	Loss: 1.832156
Train Epoch: 1 [19136/60000 (32%)]	Loss: 1.755229
Train Epoch: 1 [25536/60000 (43%)]	Loss: 1.777992
Train Epoch: 1 [31936/60000 (53%)]	Loss: 1.848336
Train Epoch: 1 [38336/60000 (64%)]	Loss: 1.913699
Train Epoch: 1 [44736/60000 (75%)]	Loss: 1.866158
Train Epoch: 1 [51136/60000 (85%)]	Loss: 1.922926
Train Epoch: 1 [57536/60000 (96%)]	Loss: 1.941242

Test set: Average loss: 1.9726, Accuracy: 4485/10000 (45%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.037647
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.008722
Train Epoch: 2 [19136/60000 (32%)]	Loss: 1.966105
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.056934
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.090762
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.016060
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.071282
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.036987
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.017482

Test set: Average loss: 2.1102, Accuracy: 3459/10000 (35%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.106765
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.212891
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.127119
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.104044
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.135118
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.072211
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.210300
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.170494
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.221300

Test set: Average loss: 2.1695, Accuracy: 2313/10000 (23%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.106465
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.224208
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.270304
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.110665
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.274344
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.088630
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.234546
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.295489
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.169428

Test set: Average loss: 2.2083, Accuracy: 2482/10000 (25%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.185036
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.144714
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.154089
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.130519
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.149174
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.288592
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.334120
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.326845
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.288769

Test set: Average loss: 2.2557, Accuracy: 1797/10000 (18%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.224261
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.228725
Train Epoch: 6 [19136/60000 (32%)]	Loss: 2.301142
Train Epoch: 6 [25536/60000 (43%)]	Loss: 2.290440
Train Epoch: 6 [31936/60000 (53%)]	Loss: 2.234324
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.229076
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.190295
Train Epoch: 6 [51136/60000 (85%)]	Loss: 2.253166
Train Epoch: 6 [57536/60000 (96%)]	Loss: 2.165335

Test set: Average loss: 2.2755, Accuracy: 1711/10000 (17%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 2.320467
Train Epoch: 7 [12736/60000 (21%)]	Loss: 2.269742
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.288671
Train Epoch: 7 [25536/60000 (43%)]	Loss: 2.268742
Train Epoch: 7 [31936/60000 (53%)]	Loss: 2.320795
Train Epoch: 7 [38336/60000 (64%)]	Loss: 2.263715
Train Epoch: 7 [44736/60000 (75%)]	Loss: 2.347591
Train Epoch: 7 [51136/60000 (85%)]	Loss: 2.307100
Train Epoch: 7 [57536/60000 (96%)]	Loss: 2.211461

Test set: Average loss: 2.2754, Accuracy: 1329/10000 (13%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 2.166956
Train Epoch: 8 [12736/60000 (21%)]	Loss: 2.278168
Train Epoch: 8 [19136/60000 (32%)]	Loss: 2.212477
Train Epoch: 8 [25536/60000 (43%)]	Loss: 2.314862
Train Epoch: 8 [31936/60000 (53%)]	Loss: 2.262160
Train Epoch: 8 [38336/60000 (64%)]	Loss: 2.290662
Train Epoch: 8 [44736/60000 (75%)]	Loss: 2.358027
Train Epoch: 8 [51136/60000 (85%)]	Loss: 2.294318
Train Epoch: 8 [57536/60000 (96%)]	Loss: 2.212598

Test set: Average loss: 2.2975, Accuracy: 1442/10000 (14%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 2.313215
Train Epoch: 9 [12736/60000 (21%)]	Loss: 2.250533
Train Epoch: 9 [19136/60000 (32%)]	Loss: 2.205082
Train Epoch: 9 [25536/60000 (43%)]	Loss: 2.387856
Train Epoch: 9 [31936/60000 (53%)]	Loss: 2.371559
Train Epoch: 9 [38336/60000 (64%)]	Loss: 2.275416
Train Epoch: 9 [44736/60000 (75%)]	Loss: 2.361403
Train Epoch: 9 [51136/60000 (85%)]	Loss: 2.357703
Train Epoch: 9 [57536/60000 (96%)]	Loss: 2.380614

Test set: Average loss: 2.3106, Accuracy: 1193/10000 (12%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 2.325152
Train Epoch: 10 [12736/60000 (21%)]	Loss: 2.290978
Train Epoch: 10 [19136/60000 (32%)]	Loss: 2.255515
Train Epoch: 10 [25536/60000 (43%)]	Loss: 2.323469
Train Epoch: 10 [31936/60000 (53%)]	Loss: 2.351486
Train Epoch: 10 [38336/60000 (64%)]	Loss: 2.335750
Train Epoch: 10 [44736/60000 (75%)]	Loss: 2.317793
Train Epoch: 10 [51136/60000 (85%)]	Loss: 2.351644
Train Epoch: 10 [57536/60000 (96%)]	Loss: 2.349730

Test set: Average loss: 2.3307, Accuracy: 1301/10000 (13%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 2.246527
Train Epoch: 11 [12736/60000 (21%)]	Loss: 2.427126
Train Epoch: 11 [19136/60000 (32%)]	Loss: 2.341186
Train Epoch: 11 [25536/60000 (43%)]	Loss: 2.364828
Train Epoch: 11 [31936/60000 (53%)]	Loss: 2.297906
Train Epoch: 11 [38336/60000 (64%)]	Loss: 2.374036
Train Epoch: 11 [44736/60000 (75%)]	Loss: 2.275247
Train Epoch: 11 [51136/60000 (85%)]	Loss: 2.334022
Train Epoch: 11 [57536/60000 (96%)]	Loss: 2.300141

Test set: Average loss: 2.3237, Accuracy: 1238/10000 (12%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 2.261097
Train Epoch: 12 [12736/60000 (21%)]	Loss: 2.355366
Train Epoch: 12 [19136/60000 (32%)]	Loss: 2.323248
Train Epoch: 12 [25536/60000 (43%)]	Loss: 2.346121
Train Epoch: 12 [31936/60000 (53%)]	Loss: 2.430819
Train Epoch: 12 [38336/60000 (64%)]	Loss: 2.326788
Train Epoch: 12 [44736/60000 (75%)]	Loss: 2.365967
Train Epoch: 12 [51136/60000 (85%)]	Loss: 2.357916
Train Epoch: 12 [57536/60000 (96%)]	Loss: 2.336501

Test set: Average loss: 2.3356, Accuracy: 1104/10000 (11%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 2.376558
Train Epoch: 13 [12736/60000 (21%)]	Loss: 2.310035
Train Epoch: 13 [19136/60000 (32%)]	Loss: 2.210307
Train Epoch: 13 [25536/60000 (43%)]	Loss: 2.261635
Train Epoch: 13 [31936/60000 (53%)]	Loss: 2.407606
Train Epoch: 13 [38336/60000 (64%)]	Loss: 2.249334
Train Epoch: 13 [44736/60000 (75%)]	Loss: 2.337599
Train Epoch: 13 [51136/60000 (85%)]	Loss: 2.345372
Train Epoch: 13 [57536/60000 (96%)]	Loss: 2.418664

Test set: Average loss: 2.3373, Accuracy: 1071/10000 (11%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 2.331680
Train Epoch: 14 [12736/60000 (21%)]	Loss: 2.273035
Train Epoch: 14 [19136/60000 (32%)]	Loss: 2.270932
Train Epoch: 14 [25536/60000 (43%)]	Loss: 2.246342
Train Epoch: 14 [31936/60000 (53%)]	Loss: 2.273997
Train Epoch: 14 [38336/60000 (64%)]	Loss: 2.330703
Train Epoch: 14 [44736/60000 (75%)]	Loss: 2.313632
Train Epoch: 14 [51136/60000 (85%)]	Loss: 2.277013
Train Epoch: 14 [57536/60000 (96%)]	Loss: 2.232169

Test set: Average loss: 2.3413, Accuracy: 1344/10000 (13%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 2.237772
Train Epoch: 15 [12736/60000 (21%)]	Loss: 2.441797
Train Epoch: 15 [19136/60000 (32%)]	Loss: 2.360056
Train Epoch: 15 [25536/60000 (43%)]	Loss: 2.279967
Train Epoch: 15 [31936/60000 (53%)]	Loss: 2.417697
Train Epoch: 15 [38336/60000 (64%)]	Loss: 2.275942
Train Epoch: 15 [44736/60000 (75%)]	Loss: 2.404037
Train Epoch: 15 [51136/60000 (85%)]	Loss: 2.398560
Train Epoch: 15 [57536/60000 (96%)]	Loss: 2.266159

Test set: Average loss: 2.3499, Accuracy: 1106/10000 (11%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 2.460340
Train Epoch: 16 [12736/60000 (21%)]	Loss: 2.396693
Train Epoch: 16 [19136/60000 (32%)]	Loss: 2.388645
Train Epoch: 16 [25536/60000 (43%)]	Loss: 2.356225
Train Epoch: 16 [31936/60000 (53%)]	Loss: 2.397459
Train Epoch: 16 [38336/60000 (64%)]	Loss: 2.338759
Train Epoch: 16 [44736/60000 (75%)]	Loss: 2.348943
Train Epoch: 16 [51136/60000 (85%)]	Loss: 2.365137
Train Epoch: 16 [57536/60000 (96%)]	Loss: 2.385309

Test set: Average loss: 2.3434, Accuracy: 1194/10000 (12%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 2.427372
Train Epoch: 17 [12736/60000 (21%)]	Loss: 2.324235
Train Epoch: 17 [19136/60000 (32%)]	Loss: 2.334702
Train Epoch: 17 [25536/60000 (43%)]	Loss: 2.193978
Train Epoch: 17 [31936/60000 (53%)]	Loss: 2.396359
Train Epoch: 17 [38336/60000 (64%)]	Loss: 2.380893
Train Epoch: 17 [44736/60000 (75%)]	Loss: 2.375084
Train Epoch: 17 [51136/60000 (85%)]	Loss: 2.310047
Train Epoch: 17 [57536/60000 (96%)]	Loss: 2.305928

Test set: Average loss: 2.3540, Accuracy: 1033/10000 (10%)

Train Epoch: 18 [6336/60000 (11%)]	Loss: 2.279164
Train Epoch: 18 [12736/60000 (21%)]	Loss: 2.433432
Train Epoch: 18 [19136/60000 (32%)]	Loss: 2.368470
Train Epoch: 18 [25536/60000 (43%)]	Loss: 2.384450
Train Epoch: 18 [31936/60000 (53%)]	Loss: 2.266338
Train Epoch: 18 [38336/60000 (64%)]	Loss: 2.415331
Train Epoch: 18 [44736/60000 (75%)]	Loss: 2.368731
Train Epoch: 18 [51136/60000 (85%)]	Loss: 2.315322
Train Epoch: 18 [57536/60000 (96%)]	Loss: 2.382887

Test set: Average loss: 2.3421, Accuracy: 1129/10000 (11%)

Train Epoch: 19 [6336/60000 (11%)]	Loss: 2.405894
Train Epoch: 19 [12736/60000 (21%)]	Loss: 2.485890
Train Epoch: 19 [19136/60000 (32%)]	Loss: 2.256519
Train Epoch: 19 [25536/60000 (43%)]	Loss: 2.292966
Train Epoch: 19 [31936/60000 (53%)]	Loss: 2.412156
Train Epoch: 19 [38336/60000 (64%)]	Loss: 2.407303
Train Epoch: 19 [44736/60000 (75%)]	Loss: 2.292992
Train Epoch: 19 [51136/60000 (85%)]	Loss: 2.355122
Train Epoch: 19 [57536/60000 (96%)]	Loss: 2.361080

Test set: Average loss: 2.3461, Accuracy: 950/10000 (10%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=19, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.01, use_relu=False, wd=0.0005)


Total time spent pruning/training: 2.06 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 297436
