Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.98, use_relu=False, wd=0.0005) 

Pruning a Wide LeNet5 network ...
Train Epoch: 1 [6336/60000 (11%)]	Loss: 2.302230
Train Epoch: 1 [12736/60000 (21%)]	Loss: 2.302355
Train Epoch: 1 [19136/60000 (32%)]	Loss: 2.301976
Train Epoch: 1 [25536/60000 (43%)]	Loss: 2.300707
Train Epoch: 1 [31936/60000 (53%)]	Loss: 2.300787
Train Epoch: 1 [38336/60000 (64%)]	Loss: 2.301117
Train Epoch: 1 [44736/60000 (75%)]	Loss: 2.299415
Train Epoch: 1 [51136/60000 (85%)]	Loss: 2.297118
Train Epoch: 1 [57536/60000 (96%)]	Loss: 2.296203

Test set: Average loss: 2.2982, Accuracy: 1971/10000 (20%)

Train Epoch: 2 [6336/60000 (11%)]	Loss: 2.297491
Train Epoch: 2 [12736/60000 (21%)]	Loss: 2.293693
Train Epoch: 2 [19136/60000 (32%)]	Loss: 2.296343
Train Epoch: 2 [25536/60000 (43%)]	Loss: 2.293142
Train Epoch: 2 [31936/60000 (53%)]	Loss: 2.295527
Train Epoch: 2 [38336/60000 (64%)]	Loss: 2.284431
Train Epoch: 2 [44736/60000 (75%)]	Loss: 2.287289
Train Epoch: 2 [51136/60000 (85%)]	Loss: 2.280111
Train Epoch: 2 [57536/60000 (96%)]	Loss: 2.275188

Test set: Average loss: 2.2845, Accuracy: 1999/10000 (20%)

Train Epoch: 3 [6336/60000 (11%)]	Loss: 2.274456
Train Epoch: 3 [12736/60000 (21%)]	Loss: 2.272419
Train Epoch: 3 [19136/60000 (32%)]	Loss: 2.271436
Train Epoch: 3 [25536/60000 (43%)]	Loss: 2.277709
Train Epoch: 3 [31936/60000 (53%)]	Loss: 2.255571
Train Epoch: 3 [38336/60000 (64%)]	Loss: 2.271575
Train Epoch: 3 [44736/60000 (75%)]	Loss: 2.255400
Train Epoch: 3 [51136/60000 (85%)]	Loss: 2.243593
Train Epoch: 3 [57536/60000 (96%)]	Loss: 2.231900

Test set: Average loss: 2.2308, Accuracy: 2208/10000 (22%)

Train Epoch: 4 [6336/60000 (11%)]	Loss: 2.203458
Train Epoch: 4 [12736/60000 (21%)]	Loss: 2.168740
Train Epoch: 4 [19136/60000 (32%)]	Loss: 2.218468
Train Epoch: 4 [25536/60000 (43%)]	Loss: 2.168277
Train Epoch: 4 [31936/60000 (53%)]	Loss: 2.188916
Train Epoch: 4 [38336/60000 (64%)]	Loss: 2.188749
Train Epoch: 4 [44736/60000 (75%)]	Loss: 2.179417
Train Epoch: 4 [51136/60000 (85%)]	Loss: 2.142071
Train Epoch: 4 [57536/60000 (96%)]	Loss: 2.092623

Test set: Average loss: 2.1328, Accuracy: 3035/10000 (30%)

Train Epoch: 5 [6336/60000 (11%)]	Loss: 2.101153
Train Epoch: 5 [12736/60000 (21%)]	Loss: 2.131204
Train Epoch: 5 [19136/60000 (32%)]	Loss: 2.147364
Train Epoch: 5 [25536/60000 (43%)]	Loss: 2.032234
Train Epoch: 5 [31936/60000 (53%)]	Loss: 2.077143
Train Epoch: 5 [38336/60000 (64%)]	Loss: 2.145223
Train Epoch: 5 [44736/60000 (75%)]	Loss: 2.103331
Train Epoch: 5 [51136/60000 (85%)]	Loss: 2.085246
Train Epoch: 5 [57536/60000 (96%)]	Loss: 2.136066

Test set: Average loss: 2.0241, Accuracy: 3730/10000 (37%)

Train Epoch: 6 [6336/60000 (11%)]	Loss: 2.009018
Train Epoch: 6 [12736/60000 (21%)]	Loss: 2.043454
Train Epoch: 6 [19136/60000 (32%)]	Loss: 1.993596
Train Epoch: 6 [25536/60000 (43%)]	Loss: 1.981806
Train Epoch: 6 [31936/60000 (53%)]	Loss: 1.952587
Train Epoch: 6 [38336/60000 (64%)]	Loss: 2.032424
Train Epoch: 6 [44736/60000 (75%)]	Loss: 2.011791
Train Epoch: 6 [51136/60000 (85%)]	Loss: 1.857585
Train Epoch: 6 [57536/60000 (96%)]	Loss: 1.931515

Test set: Average loss: 1.9497, Accuracy: 3845/10000 (38%)

Train Epoch: 7 [6336/60000 (11%)]	Loss: 1.945462
Train Epoch: 7 [12736/60000 (21%)]	Loss: 1.966143
Train Epoch: 7 [19136/60000 (32%)]	Loss: 2.034840
Train Epoch: 7 [25536/60000 (43%)]	Loss: 1.899191
Train Epoch: 7 [31936/60000 (53%)]	Loss: 1.897580
Train Epoch: 7 [38336/60000 (64%)]	Loss: 1.977378
Train Epoch: 7 [44736/60000 (75%)]	Loss: 1.997132
Train Epoch: 7 [51136/60000 (85%)]	Loss: 1.935601
Train Epoch: 7 [57536/60000 (96%)]	Loss: 1.956210

Test set: Average loss: 1.9041, Accuracy: 3975/10000 (40%)

Train Epoch: 8 [6336/60000 (11%)]	Loss: 1.878942
Train Epoch: 8 [12736/60000 (21%)]	Loss: 1.942001
Train Epoch: 8 [19136/60000 (32%)]	Loss: 1.833197
Train Epoch: 8 [25536/60000 (43%)]	Loss: 1.974896
Train Epoch: 8 [31936/60000 (53%)]	Loss: 1.774626
Train Epoch: 8 [38336/60000 (64%)]	Loss: 1.944188
Train Epoch: 8 [44736/60000 (75%)]	Loss: 1.943543
Train Epoch: 8 [51136/60000 (85%)]	Loss: 1.834388
Train Epoch: 8 [57536/60000 (96%)]	Loss: 1.779244

Test set: Average loss: 1.8726, Accuracy: 4123/10000 (41%)

Train Epoch: 9 [6336/60000 (11%)]	Loss: 1.893898
Train Epoch: 9 [12736/60000 (21%)]	Loss: 1.819547
Train Epoch: 9 [19136/60000 (32%)]	Loss: 1.933059
Train Epoch: 9 [25536/60000 (43%)]	Loss: 1.863757
Train Epoch: 9 [31936/60000 (53%)]	Loss: 1.877693
Train Epoch: 9 [38336/60000 (64%)]	Loss: 1.972606
Train Epoch: 9 [44736/60000 (75%)]	Loss: 1.907033
Train Epoch: 9 [51136/60000 (85%)]	Loss: 1.904695
Train Epoch: 9 [57536/60000 (96%)]	Loss: 1.938340

Test set: Average loss: 1.8547, Accuracy: 4344/10000 (43%)

Train Epoch: 10 [6336/60000 (11%)]	Loss: 1.868821
Train Epoch: 10 [12736/60000 (21%)]	Loss: 1.923391
Train Epoch: 10 [19136/60000 (32%)]	Loss: 1.830483
Train Epoch: 10 [25536/60000 (43%)]	Loss: 1.864100
Train Epoch: 10 [31936/60000 (53%)]	Loss: 1.826604
Train Epoch: 10 [38336/60000 (64%)]	Loss: 1.761700
Train Epoch: 10 [44736/60000 (75%)]	Loss: 1.793995
Train Epoch: 10 [51136/60000 (85%)]	Loss: 1.851206
Train Epoch: 10 [57536/60000 (96%)]	Loss: 1.888151

Test set: Average loss: 1.8411, Accuracy: 4162/10000 (42%)

Train Epoch: 11 [6336/60000 (11%)]	Loss: 1.749635
Train Epoch: 11 [12736/60000 (21%)]	Loss: 1.900158
Train Epoch: 11 [19136/60000 (32%)]	Loss: 1.784303
Train Epoch: 11 [25536/60000 (43%)]	Loss: 1.794598
Train Epoch: 11 [31936/60000 (53%)]	Loss: 1.909995
Train Epoch: 11 [38336/60000 (64%)]	Loss: 1.852564
Train Epoch: 11 [44736/60000 (75%)]	Loss: 1.937233
Train Epoch: 11 [51136/60000 (85%)]	Loss: 1.858168
Train Epoch: 11 [57536/60000 (96%)]	Loss: 1.866190

Test set: Average loss: 1.8333, Accuracy: 4182/10000 (42%)

Train Epoch: 12 [6336/60000 (11%)]	Loss: 1.914560
Train Epoch: 12 [12736/60000 (21%)]	Loss: 1.812988
Train Epoch: 12 [19136/60000 (32%)]	Loss: 1.949714
Train Epoch: 12 [25536/60000 (43%)]	Loss: 1.856494
Train Epoch: 12 [31936/60000 (53%)]	Loss: 1.778388
Train Epoch: 12 [38336/60000 (64%)]	Loss: 1.964851
Train Epoch: 12 [44736/60000 (75%)]	Loss: 1.812250
Train Epoch: 12 [51136/60000 (85%)]	Loss: 1.855195
Train Epoch: 12 [57536/60000 (96%)]	Loss: 1.930542

Test set: Average loss: 1.8263, Accuracy: 4133/10000 (41%)

Train Epoch: 13 [6336/60000 (11%)]	Loss: 1.849933
Train Epoch: 13 [12736/60000 (21%)]	Loss: 1.835905
Train Epoch: 13 [19136/60000 (32%)]	Loss: 1.832510
Train Epoch: 13 [25536/60000 (43%)]	Loss: 1.761491
Train Epoch: 13 [31936/60000 (53%)]	Loss: 1.833101
Train Epoch: 13 [38336/60000 (64%)]	Loss: 1.867794
Train Epoch: 13 [44736/60000 (75%)]	Loss: 1.906850
Train Epoch: 13 [51136/60000 (85%)]	Loss: 1.841235
Train Epoch: 13 [57536/60000 (96%)]	Loss: 1.894202

Test set: Average loss: 1.8212, Accuracy: 4447/10000 (44%)

Train Epoch: 14 [6336/60000 (11%)]	Loss: 1.817918
Train Epoch: 14 [12736/60000 (21%)]	Loss: 1.803670
Train Epoch: 14 [19136/60000 (32%)]	Loss: 1.752383
Train Epoch: 14 [25536/60000 (43%)]	Loss: 1.700185
Train Epoch: 14 [31936/60000 (53%)]	Loss: 1.814838
Train Epoch: 14 [38336/60000 (64%)]	Loss: 1.879278
Train Epoch: 14 [44736/60000 (75%)]	Loss: 1.710497
Train Epoch: 14 [51136/60000 (85%)]	Loss: 1.799513
Train Epoch: 14 [57536/60000 (96%)]	Loss: 1.784851

Test set: Average loss: 1.8194, Accuracy: 4444/10000 (44%)

Train Epoch: 15 [6336/60000 (11%)]	Loss: 1.884031
Train Epoch: 15 [12736/60000 (21%)]	Loss: 1.899564
Train Epoch: 15 [19136/60000 (32%)]	Loss: 1.922438
Train Epoch: 15 [25536/60000 (43%)]	Loss: 1.852723
Train Epoch: 15 [31936/60000 (53%)]	Loss: 1.769901
Train Epoch: 15 [38336/60000 (64%)]	Loss: 1.787747
Train Epoch: 15 [44736/60000 (75%)]	Loss: 1.741311
Train Epoch: 15 [51136/60000 (85%)]	Loss: 1.820135
Train Epoch: 15 [57536/60000 (96%)]	Loss: 1.808087

Test set: Average loss: 1.8162, Accuracy: 4226/10000 (42%)

Train Epoch: 16 [6336/60000 (11%)]	Loss: 1.884238
Train Epoch: 16 [12736/60000 (21%)]	Loss: 1.832980
Train Epoch: 16 [19136/60000 (32%)]	Loss: 1.845662
Train Epoch: 16 [25536/60000 (43%)]	Loss: 1.687906
Train Epoch: 16 [31936/60000 (53%)]	Loss: 1.857690
Train Epoch: 16 [38336/60000 (64%)]	Loss: 1.743396
Train Epoch: 16 [44736/60000 (75%)]	Loss: 1.814619
Train Epoch: 16 [51136/60000 (85%)]	Loss: 1.851007
Train Epoch: 16 [57536/60000 (96%)]	Loss: 1.832096

Test set: Average loss: 1.8159, Accuracy: 4246/10000 (42%)

Train Epoch: 17 [6336/60000 (11%)]	Loss: 1.872557
Train Epoch: 17 [12736/60000 (21%)]	Loss: 1.836755
Train Epoch: 17 [19136/60000 (32%)]	Loss: 1.749446
Train Epoch: 17 [25536/60000 (43%)]	Loss: 1.789617
Train Epoch: 17 [31936/60000 (53%)]	Loss: 1.917129
Train Epoch: 17 [38336/60000 (64%)]	Loss: 1.804680
Train Epoch: 17 [44736/60000 (75%)]	Loss: 1.877944
Train Epoch: 17 [51136/60000 (85%)]	Loss: 1.865369
Train Epoch: 17 [57536/60000 (96%)]	Loss: 1.829936

Test set: Average loss: 1.8155, Accuracy: 4260/10000 (43%)

Namespace(batch_size=64, bias=None, data='../data', device=0, epochs=17, hidden_size=500, load_weights='./paper/lenet5/lenet5_e50_h500.pt', log_interval=100, lr=0.01, model='widelenet5', momentum=0.9, no_cuda=False, r=5, save_model=None, save_results=True, seed=1, sparsity=0.98, use_relu=False, wd=0.0005)


Total time spent pruning/training: 1.84 minutes
Total number of parameters in model: 300414
Number of parameters in pruned model: 8508
